{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n"
     ]
    }
   ],
   "source": [
    "# Respect & reference https://github.com/aidiary/pytorch-examples/blob/master/180306-cyclegan-horse2zebra.ipynb\n",
    "# Respect & reference https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available!\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "load_size = 286  # オリジナルの画像はこのサイズにリサイズ\n",
    "fine_size = 256  # 286x286の画像からランダムに256x256をcrop\n",
    "batch_size = 1\n",
    "num_epoch = 2000\n",
    "\n",
    "lr = 0.001  # initial learning rate for adam\n",
    "beta1 = 0.5  # momentum term of adam\n",
    "\n",
    "save_epoch_freq = 5\n",
    "log_dir = 'logs'\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('cuda available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnalignedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, is_train):\n",
    "        super(torch.utils.data.Dataset, self).__init__()\n",
    "\n",
    "        root_dir = os.path.join('data', '0209')\n",
    "        \n",
    "        if is_train:\n",
    "            dir_A = os.path.join(root_dir, 'trainMS')\n",
    "            dir_B = os.path.join(root_dir, 'trainMA')\n",
    "        else:\n",
    "            dir_A = os.path.join(root_dir, 'testMS')\n",
    "            dir_B = os.path.join(root_dir, 'testMA')\n",
    "\n",
    "        self.image_paths_A = self._make_dataset(dir_A)\n",
    "        self.image_paths_B = self._make_dataset(dir_B)\n",
    "\n",
    "        self.size_A = len(self.image_paths_A)\n",
    "        self.size_B = len(self.image_paths_B)\n",
    "\n",
    "        #!!!\n",
    "        self.transform = self._make_transform(is_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_A = index % self.size_A\n",
    "        path_A = self.image_paths_A[index_A]\n",
    "        \n",
    "        # クラスBの画像はランダムに選択\n",
    "        index_B = random.randint(0, self.size_B - 1)\n",
    "        path_B = self.image_paths_B[index_B]\n",
    "\n",
    "        #img_A = Image.open(path_A).convert('RGB')\n",
    "        #img_B = Image.open(path_B).convert('RGB')\n",
    "        nimg_A = np.load(path_A)\n",
    "        nimg_B = np.load(path_B)\n",
    "        #nimg_A = np.resize(nimg_A, [64,24])\n",
    "        #nimg_B = np.resize(nimg_B, [64,24])\n",
    "        #nimg_A = nimg_A.T\n",
    "        #nimg_B = nimg_B.T\n",
    "        img_A = torch.from_numpy(nimg_A)\n",
    "        img_B = torch.from_numpy(nimg_B)\n",
    "        #img_A = img_A.unsqueeze(0)\n",
    "        #img_B = img_B.unsqueeze(0)\n",
    "        \n",
    "        # データ拡張\n",
    "        # label data no kotodeha ?\n",
    "        #A = self.transform(img_A)\n",
    "        #B = self.transform(img_B)\n",
    "        #A = A.type(torch.cuda.FloatTensor)\n",
    "        #B = B.type(torch.cuda.FloatTensor)\n",
    "        A = img_A.type(torch.cuda.FloatTensor)\n",
    "        B = img_B.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "        return {'A': A, 'B': B, 'path_A': path_A, 'path_B': path_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.size_A, self.size_B)\n",
    "\n",
    "    def _make_dataset(self, dir):\n",
    "        images = []\n",
    "        for fname in os.listdir(dir):\n",
    "            #if fname.endswith('.jpg'):\n",
    "            if fname.endswith('.npy'):\n",
    "                path = os.path.join(dir, fname)\n",
    "                images.append(path)\n",
    "        sorted(images)\n",
    "        return images\n",
    "\n",
    "    def _make_transform(self, is_train):\n",
    "        transforms_list = []\n",
    "        #!!!\n",
    "        #transforms_list.append(transforms.Resize((load_size, load_size), Image.BICUBIC))\n",
    "        #transforms_list.append(transforms.RandomCrop(fine_size))\n",
    "        #if is_train:\n",
    "            #transforms_list.append(transforms.RandomHorizontalFlip())\n",
    "        #transforms_list.append(transforms.ToTensor())\n",
    "        #transforms_list.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))  # [0, 1] => [-1, 1]\n",
    "        return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainJ = np.load(\"data/sad2joy/OGVC_JOY/trainJOY.npy\")\n",
    "#file_list1 = sorted(glob.glob(\"data/Voiceactor/tmpAA/*.npy\"))\n",
    "#x1_train = []\n",
    "#for i in tqdm(file_list1[:30]):\n",
    "#    x1_train.append(np.load(i))\n",
    "#x1_train = np.vstack(x1_train)\n",
    "#x1_train = np.log10(x1_train)/10.\n",
    "#trainJ = np.load(\"data/Voiceactor/tmpAA/\")\n",
    "#trainS = np.load(\"data/sad2joy/OGVC_SAD/trainSAD.npy\")\n",
    "#testJ = np.load(\"data/sad2joy/OGVC_JOY/testJOY.npy\")\n",
    "#testS = np.load(\"data/sad2joy/OGVC_SAD/testSAD.npy\")\n",
    "\n",
    "train_dataset = UnalignedDataset(is_train=True)\n",
    "#train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x1_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 128, 24])\n",
      "torch.Size([1, 7, 128, 24])\n",
      "['data/0209/trainMS/MTY0603SAD3_0.npy']\n",
      "['data/0209/trainMA/MOY0903ANG1_2.npy']\n"
     ]
    }
   ],
   "source": [
    "# SAMPLE TEST\n",
    "batch = iter(train_loader).next()\n",
    "#print(batch)\n",
    "print(batch['A'].shape)\n",
    "print(batch['B'].shape)\n",
    "print(batch['path_A'])\n",
    "print(batch['path_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの描画\n",
    "def imshow(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    npimg = 0.5 * (npimg + 1)  # [-1,1] => [0, 1]\n",
    "    # [c, h, w] => [h, w, c]\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid dimensions for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-6c041902deb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-171-1b7342c332d6>\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [-1,1] => [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# [c, h, w] => [h, w, c]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3204\u001b[0m                         \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m                         **kwargs)\n\u001b[0m\u001b[1;32m   3206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5485\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    651\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    652\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 653\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid dimensions for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "batch = iter(train_loader).next()\n",
    "images_A = batch['A']  # horses\n",
    "images_B = batch['B']  # zebras\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(make_grid(images_A, nrow=4))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(make_grid(images_B, nrow=4))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        conv_block = []\n",
    "        conv_block += [nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim, dim*4, kernel_size=3),\n",
    "                       nn.InstanceNorm2d(dim*4),\n",
    "                       nn.GLU(1),\n",
    "                       #nn.Sigmoid(),\n",
    "                       nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim*2, dim, kernel_size=3),\n",
    "                       nn.InstanceNorm2d(dim)]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "\n",
    "            nn.Conv2d(7, 64, kernel_size=(7,4)),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "\n",
    "            nn.Conv2d(32, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.model2 = nn.Sequential(\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            #ResNetBlock(256),\n",
    "            #ResNetBlock(256),\n",
    "            #ResNetBlock(256),\n",
    "        )\n",
    "        \n",
    "        self.model3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=(3,2), stride=2, padding=1, output_padding=(1,0)),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=(3,2), stride=2, padding=1, output_padding=(1,0)),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            \n",
    "            #nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.InstanceNorm2d(4),\n",
    "            #nn.GLU(1),\n",
    "\n",
    "            #nn.ConvTranspose2d(64, 2, kernel_size=7, stride=1, padding=0, output_padding=0),\n",
    "            #nn.InstanceNorm2d(8),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(8, 7, kernel_size=(7,5), stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # initialize weights\n",
    "        self.model1.apply(self._init_weights)\n",
    "        self.model2.apply(self._init_weights)\n",
    "        self.model3.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.model1(input.cuda())\n",
    "        y = self.model2(x)\n",
    "        return self.model3(y)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, kernel_size=(4,2), stride=1, padding=1),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(32, 128, kernel_size=(4,2), stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 256, kernel_size=(4,2), stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(128, 512, kernel_size=(4,2), stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(256, 1, kernel_size=(4,2), stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # initialize weights\n",
    "        self.model.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input.cuda())\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model1): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(7, 64, kernel_size=(7, 4), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): GLU(dim=1)\n",
      "    (4): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): GLU(dim=1)\n",
      "    (7): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): GLU(dim=1)\n",
      "  )\n",
      "  (model2): Sequential(\n",
      "    (0): ResNetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): GLU(dim=1)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (1): ResNetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): GLU(dim=1)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (2): ResNetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): GLU(dim=1)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (3): ResNetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): GLU(dim=1)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (4): ResNetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): GLU(dim=1)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (5): ResNetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): GLU(dim=1)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (model3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(3, 2), stride=(2, 2), padding=(1, 1), output_padding=(1, 0))\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): GLU(dim=1)\n",
      "    (3): ConvTranspose2d(32, 16, kernel_size=(3, 2), stride=(2, 2), padding=(1, 1), output_padding=(1, 0))\n",
      "    (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (5): GLU(dim=1)\n",
      "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (7): Conv2d(8, 7, kernel_size=(7, 5), stride=(1, 1))\n",
      "    (8): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 5563839\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(7, 64, kernel_size=(4, 2), stride=(1, 1), padding=(1, 1))\n",
      "    (1): GLU(dim=1)\n",
      "    (2): Conv2d(32, 128, kernel_size=(4, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (4): GLU(dim=1)\n",
      "    (5): Conv2d(64, 256, kernel_size=(4, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (7): GLU(dim=1)\n",
      "    (8): Conv2d(128, 512, kernel_size=(4, 2), stride=(2, 2), padding=(1, 1))\n",
      "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (10): GLU(dim=1)\n",
      "    (11): Conv2d(256, 1, kernel_size=(4, 2), stride=(1, 1), padding=(1, 1))\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 694721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:73: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "\n",
    "gen = Generator()\n",
    "print_network(gen)\n",
    "\n",
    "disc = Discriminator()\n",
    "print_network(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        # プールを使わないときはそのまま返す\n",
    "        if self.pool_size == 0:\n",
    "            return Variable(images)\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            # バッチの次元を削除して3Dテンソルに\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size - 1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            # 高速化のため？\n",
    "            # varがNoneのままか形状が違うときに作り直す\n",
    "            create_label = ((self.real_label_var is None) or (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = torch.ones(input.size())\n",
    "                if cuda:\n",
    "                    real_tensor = real_tensor.cuda()\n",
    "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = torch.zeros(input.size())\n",
    "                if cuda:\n",
    "                    fake_tensor = fake_tensor.cuda()\n",
    "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(object):\n",
    "    \n",
    "    def __init__(self, log_dir='logs'):\n",
    "        self.netG_A = Generator()\n",
    "        self.netG_B = Generator()\n",
    "        self.netD_A = Discriminator()\n",
    "        self.netD_B = Discriminator()\n",
    "\n",
    "        if cuda:\n",
    "            self.netG_A.cuda()\n",
    "            self.netG_B.cuda()\n",
    "            self.netD_A.cuda()\n",
    "            self.netD_B.cuda()\n",
    "\n",
    "        self.fake_A_pool = ImagePool(50)\n",
    "        self.fake_B_pool = ImagePool(50)\n",
    "\n",
    "        # targetが本物か偽物かで代わるのでオリジナルのGANLossクラスを作成\n",
    "        self.criterionGAN = GANLoss()\n",
    "        self.criterionCycle = torch.nn.L1Loss()\n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "\n",
    "        # Generatorは2つのパラメータを同時に更新\n",
    "        self.optimizer_G = torch.optim.Adam(\n",
    "            itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n",
    "            lr=lr,\n",
    "            betas=(beta1, 0.999))\n",
    "        self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=lr*0.00001, betas=(beta1, 0.999))\n",
    "        self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=lr*0.00001, betas=(beta1, 0.999))\n",
    "        self.optimizers = []\n",
    "        self.optimizers.append(self.optimizer_G)\n",
    "        self.optimizers.append(self.optimizer_D_A)\n",
    "        self.optimizers.append(self.optimizer_D_B)\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "    \n",
    "    def set_input(self, input):\n",
    "        input_A = input['A']\n",
    "        input_B = input['B']\n",
    "        if cuda:\n",
    "            input_A = input_A.cuda()\n",
    "            input_B = input_B.cuda()\n",
    "        self.input_A = input_A\n",
    "        self.input_B = input_B\n",
    "        self.image_paths = input['path_A']\n",
    "\n",
    "    def backward_G(self, real_A, real_B):\n",
    "        # Generatorに関連するlossと勾配計算処理\n",
    "        lambda_idt = 15.0\n",
    "        lambda_A = 20.0\n",
    "        lambda_B = 20.0\n",
    "\n",
    "        # G_A, G_Bは変換先ドメインの本物画像を入力したときはそのまま出力するべき\n",
    "        # netG_AはドメインAの画像からドメインBの画像を生成するGeneratorだが\n",
    "        # ドメインBの画像も入れることができる\n",
    "        # その場合は何も変換してほしくないという制約\n",
    "        # TODO: idt_Aの命名はよくない気がする idt_Bの方が適切では？\n",
    "        idt_A = self.netG_A(real_B)\n",
    "        loss_idt_A = self.criterionIdt(idt_A, real_B) * lambda_B * lambda_idt\n",
    "\n",
    "        idt_B = self.netG_B(real_A)\n",
    "        loss_idt_B = self.criterionIdt(idt_B, real_A) * lambda_A * lambda_idt\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        # G_Aとしては生成した偽物画像が本物（True）とみなしてほしい\n",
    "        fake_B = self.netG_A(real_A)\n",
    "        pred_fake = self.netD_A(fake_B)\n",
    "        loss_G_A = self.criterionGAN(pred_fake, True)\n",
    "\n",
    "        # GAN loss D_B(G_B(B))\n",
    "        # G_Bとしては生成した偽物画像が本物（True）とみなしてほしい\n",
    "        fake_A = self.netG_B(real_B)\n",
    "        pred_fake = self.netD_B(fake_A)\n",
    "        loss_G_B = self.criterionGAN(pred_fake, True)\n",
    "        \n",
    "        # forward cycle loss\n",
    "        # real_A => fake_B => rec_Aが元のreal_Aに近いほどよい\n",
    "        rec_A = self.netG_B(fake_B)\n",
    "        loss_cycle_A = self.criterionCycle(rec_A, real_A) * lambda_A\n",
    "        \n",
    "        # backward cycle loss\n",
    "        # real_B => fake_A => rec_Bが元のreal_Bに近いほどよい\n",
    "        rec_B = self.netG_A(fake_A)\n",
    "        loss_cycle_B = self.criterionCycle(rec_B, real_B) * lambda_B\n",
    "        \n",
    "        # combined loss\n",
    "        loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B\n",
    "        loss_G.backward()\n",
    "\n",
    "        # 次のDiscriminatorの更新でfake画像が必要なので一緒に返す\n",
    "        return loss_G_A.data[0], loss_G_B.data[0], loss_cycle_A.data[0], loss_cycle_B.data[0], loss_idt_A.data[0], loss_idt_B.data[0], fake_A.data, fake_B.data\n",
    "\n",
    "    def backward_D_A(self, real_B, fake_B):\n",
    "        # ドメインAから生成したfake_Bが本物か偽物か見分ける\n",
    "\n",
    "        # TODO: これは何をしている？\n",
    "        # fake_Bを直接使わずに過去に生成した偽画像から新しくランダムサンプリングしている？\n",
    "        fake_B = self.fake_B_pool.query(fake_B)\n",
    "\n",
    "        # 本物画像を入れたときは本物と認識するほうがよい\n",
    "        pred_real = self.netD_A(real_B)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "        # ドメインAから生成した偽物画像を入れたときは偽物と認識するほうがよい\n",
    "        # fake_Bを生成したGeneratorまで勾配が伝搬しないようにdetach()する\n",
    "        pred_fake = self.netD_A(fake_B.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "\n",
    "        # combined loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        \n",
    "        return loss_D_A.data[0]\n",
    "\n",
    "    def backward_D_B(self, real_A, fake_A):\n",
    "        # ドメインBから生成したfake_Aが本物か偽物か見分ける\n",
    "\n",
    "        fake_A = self.fake_A_pool.query(fake_A)\n",
    "        \n",
    "        # 本物画像を入れたときは本物と認識するほうがよい\n",
    "        pred_real = self.netD_B(real_A)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "        # 偽物画像を入れたときは偽物と認識するほうがよい\n",
    "        pred_fake = self.netD_B(fake_A.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "        \n",
    "        # combined loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        \n",
    "        return loss_D_B.data[0]\n",
    "\n",
    "    def optimize(self):\n",
    "        real_A = Variable(self.input_A)\n",
    "        real_B = Variable(self.input_B)\n",
    "        \n",
    "        # update Generator (G_A and G_B)\n",
    "        self.optimizer_G.zero_grad()\n",
    "        loss_G_A, loss_G_B, loss_cycle_A, loss_cycle_B, loss_idt_A, loss_idt_B, fake_A, fake_B = self.backward_G(real_A, real_B)\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # update D_A\n",
    "        self.optimizer_D_A.zero_grad()\n",
    "        loss_D_A = self.backward_D_A(real_B, fake_B)\n",
    "        self.optimizer_D_A.step()\n",
    "        \n",
    "        # update D_B\n",
    "        self.optimizer_D_B.zero_grad()\n",
    "        loss_D_B = self.backward_D_B(real_A, fake_A)\n",
    "        self.optimizer_D_B.step()\n",
    "\n",
    "        ret_loss = [loss_G_A, loss_D_A,\n",
    "                    loss_G_B, loss_D_B,\n",
    "                    loss_cycle_A, loss_cycle_B,\n",
    "                    loss_idt_A, loss_idt_B]\n",
    "\n",
    "        return np.array(ret_loss)\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        running_loss = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            self.set_input(data)\n",
    "            losses = self.optimize()\n",
    "            running_loss += losses\n",
    "        running_loss /= len(data_loader)\n",
    "        return running_loss\n",
    "    \n",
    "    def save_network(self, network, network_label, epoch_label):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.log_dir, save_filename)\n",
    "        # GPUで動いている場合はCPUに移してから保存\n",
    "        # これやっておけばCPUでモデルをロードしやすくなる?\n",
    "        #print(\"Test\")\n",
    "        torch.save(network.cpu().state_dict(), save_path)\n",
    "        # GPUに戻す\n",
    "        if torch.cuda.is_available():\n",
    "            #print(\"check\")\n",
    "            network.cuda()\n",
    "\n",
    "    def load_network(self, network, network_label, epoch_label):\n",
    "        load_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        load_path = os.path.join(self.log_dir, load_filename)\n",
    "        network.load_state_dict(torch.load(load_path))\n",
    "\n",
    "    def save(self, label):\n",
    "        self.save_network(self.netG_A, 'G_A', label)\n",
    "        self.save_network(self.netD_A, 'D_A', label)\n",
    "        self.save_network(self.netG_B, 'G_B', label)\n",
    "        self.save_network(self.netD_B, 'D_B', label)\n",
    "    \n",
    "    def load(self, label):\n",
    "        self.load_network(self.netG_A, 'G_A', label)\n",
    "        self.load_network(self.netD_A, 'D_A', label)\n",
    "        self.load_network(self.netG_B, 'G_B', label)\n",
    "        self.load_network(self.netD_B, 'D_B', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:73: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real_A: torch.Size([1, 7, 128, 24])\n",
      "fake_B: torch.Size([1, 7, 128, 24])\n",
      "recon_A: torch.Size([1, 7, 128, 24])\n",
      "real_B: torch.Size([1, 7, 128, 24])\n",
      "fake_A: torch.Size([1, 7, 128, 24])\n",
      "recon_B: torch.Size([1, 7, 128, 24])\n",
      "torch.Size([1, 1, 14, 5])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "# netG_AもnetG_Bも同じサイズの画像を生成することが確認できる\n",
    "model = CycleGAN()\n",
    "data = iter(train_loader).next()\n",
    "\n",
    "real_A = Variable(data['A'], volatile=True)\n",
    "print('real_A:', real_A.size())\n",
    "fake_B = model.netG_A(real_A)\n",
    "print('fake_B:', fake_B.size())\n",
    "recon_A = model.netG_B(fake_B)\n",
    "print('recon_A:', recon_A.size())\n",
    "\n",
    "real_B = Variable(data['B'], volatile=True)\n",
    "print('real_B:', real_B.size())\n",
    "fake_A = model.netG_B(real_B)\n",
    "print('fake_A:', fake_A.size())\n",
    "recon_B = model.netG_A(fake_A)\n",
    "print('recon_B:', recon_B.size())\n",
    "\n",
    "# Discriminatorが0 or 1のスカラーを返すのではなく、\n",
    "# 30x30のfeature mapを返す\n",
    "# 本物画像のときは30x30がすべて1、偽物画像のときは30x30がすべて0がターゲットとなる\n",
    "out = model.netD_A(real_A)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.],\n",
      "          [ 1.,  1.,  1.,  1.,  1.]]]], device='cuda:0')\n",
      "tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "criterionGAN = GANLoss()\n",
    "loss = criterionGAN(out, True)\n",
    "print(criterionGAN.real_label_var)\n",
    "loss = criterionGAN(out, False)\n",
    "print(criterionGAN.fake_label_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:73: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:93: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:115: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:134: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, losses: [ 0.2474733   0.26044234  0.24606918  0.26709664  6.20721724  6.19347874\n",
      " 92.49858287 92.66622981]\n",
      "epoch 2, losses: [ 0.24103687  0.26496872  0.25176067  0.26247656  5.80884815  6.07892564\n",
      " 90.61766582 86.50623732]\n",
      "epoch 3, losses: [ 0.24685029  0.25934022  0.25377121  0.26233094  5.32287735  5.66901448\n",
      " 83.67070378 75.18591737]\n",
      "epoch 4, losses: [ 0.24718972  0.25765956  0.2469383   0.26671529  4.64254418  5.01482322\n",
      " 70.89411057 63.18847738]\n",
      "epoch 5, losses: [ 0.245482    0.25878361  0.24850714  0.26642983  4.34049658  4.57428235\n",
      " 63.1519428  56.20187762]\n",
      "epoch 6, losses: [ 0.24955392  0.25776343  0.25004022  0.26549658  4.07856377  4.13481169\n",
      " 59.29033878 55.09061895]\n",
      "epoch 7, losses: [ 0.24807289  0.25814363  0.25075433  0.26438813  3.89971735  3.95514353\n",
      " 57.67635843 53.18335561]\n",
      "epoch 8, losses: [ 0.25026635  0.25623007  0.25208299  0.26421963  3.68365144  4.47537129\n",
      " 53.58497389 53.04169407]\n",
      "epoch 9, losses: [ 0.25110122  0.25596518  0.25398111  0.26274506  3.76363781  4.44216588\n",
      " 53.69878761 53.80877596]\n",
      "epoch 10, losses: [ 0.2520188   0.25497039  0.25359704  0.26174541  3.77837027  4.00590633\n",
      " 55.74778883 51.14332744]\n",
      "epoch 11, losses: [ 0.25284524  0.25424021  0.25418403  0.26147451  3.59565106  3.84432262\n",
      " 51.63176815 52.66558754]\n",
      "epoch 12, losses: [ 0.2551012   0.25303232  0.2556491   0.26020138  3.65148545  3.94426724\n",
      " 52.46491324 51.17618354]\n",
      "epoch 13, losses: [ 0.25523212  0.25251566  0.25729921  0.25950843  3.74803354  3.91596137\n",
      " 52.55342187 50.81898145]\n",
      "epoch 14, losses: [ 0.25466548  0.25298312  0.25799245  0.25899826  3.73703393  3.72461738\n",
      " 52.9546791  49.99556181]\n",
      "epoch 15, losses: [ 0.257043    0.25073863  0.25808482  0.25860306  3.45594455  3.49338392\n",
      " 49.38864356 49.74119902]\n",
      "epoch 16, losses: [ 0.25832393  0.25030726  0.26000326  0.25709416  3.27835387  3.49866461\n",
      " 48.89964024 49.73360414]\n",
      "epoch 17, losses: [ 0.26032138  0.24910281  0.26096693  0.25618588  3.48417939  4.1835299\n",
      " 49.15794264 48.66151174]\n",
      "epoch 18, losses: [ 0.26198167  0.24803078  0.26216511  0.25554434  3.44799788  3.38524927\n",
      " 49.06801524 49.54178301]\n",
      "epoch 19, losses: [ 0.26234943  0.24663031  0.2629864   0.2548501   3.44628316  3.53963022\n",
      " 51.27100412 47.44562489]\n",
      "epoch 20, losses: [ 0.26494184  0.2451947   0.26448617  0.25414282  3.53546871  3.70941185\n",
      " 50.32308359 47.7488534 ]\n",
      "epoch 21, losses: [ 0.26764207  0.24369491  0.26673978  0.2530176   3.55590502  3.75353692\n",
      " 50.6213562  49.59656294]\n",
      "epoch 22, losses: [ 0.26795563  0.24326468  0.266873    0.25244964  3.45822206  3.50862451\n",
      " 50.56888633 50.09426394]\n",
      "epoch 23, losses: [ 0.26791999  0.24360529  0.26845775  0.2515744   3.56104228  3.66121929\n",
      " 49.04989047 48.78093051]\n",
      "epoch 24, losses: [ 0.26964615  0.24309798  0.26893732  0.25096893  3.37251399  3.62327731\n",
      " 48.48626294 48.55780433]\n",
      "epoch 25, losses: [ 0.2700894   0.242263    0.27082748  0.24980664  3.60509628  3.61582903\n",
      " 49.14457187 48.35862518]\n",
      "epoch 26, losses: [ 0.27206296  0.24121034  0.27023616  0.24994474  3.48540003  3.59936476\n",
      " 48.5450904  48.84398225]\n",
      "epoch 27, losses: [ 0.27323968  0.24015669  0.27150313  0.24894304  3.7799282   3.63102149\n",
      " 51.42030317 46.91281311]\n",
      "epoch 28, losses: [ 0.27540764  0.23897756  0.27419511  0.24778778  3.38983154  3.53076688\n",
      " 48.76702561 45.84752404]\n",
      "epoch 29, losses: [ 0.27782742  0.23763808  0.27454539  0.24743667  3.73845551  3.63775653\n",
      " 47.40153037 47.05344955]\n",
      "epoch 30, losses: [ 0.27700129  0.23712382  0.27566941  0.24689891  3.30248901  3.57491489\n",
      " 46.64096438 48.94106253]\n",
      "epoch 31, losses: [ 0.27856729  0.23656671  0.27660307  0.24619855  3.27752374  3.74842037\n",
      " 46.17514828 50.58298367]\n",
      "epoch 32, losses: [ 0.27997442  0.23540679  0.27683142  0.24573436  3.19959745  3.62295945\n",
      " 47.96031256 47.5633481 ]\n",
      "epoch 33, losses: [ 0.28128565  0.23432227  0.27777222  0.24537216  3.89548695  3.53232433\n",
      " 55.89147471 47.65170952]\n",
      "epoch 34, losses: [ 0.28298878  0.23317507  0.28081317  0.24306721  3.38908181  3.36899401\n",
      " 48.3336063  47.71865545]\n",
      "epoch 35, losses: [ 0.28466841  0.23297255  0.28092016  0.24317681  3.26069922  3.78151296\n",
      " 50.79974938 47.68246787]\n",
      "epoch 36, losses: [ 0.28522055  0.23250903  0.28132632  0.2429789   3.53354399  3.49713814\n",
      " 50.31472995 48.00785444]\n",
      "epoch 37, losses: [ 0.28761247  0.23203553  0.28454978  0.24108163  3.34932277  3.37476867\n",
      " 46.98730617 49.45833859]\n",
      "epoch 38, losses: [ 0.28931583  0.22980077  0.28508916  0.24110074  3.21130521  3.35883374\n",
      " 46.09153268 47.93336665]\n",
      "epoch 39, losses: [ 0.29008131  0.22874874  0.28597986  0.24021421  3.30732264  3.45157889\n",
      " 48.25657443 46.67313925]\n",
      "epoch 40, losses: [ 0.2908905   0.22804365  0.2860359   0.24008996  3.15169704  3.29782303\n",
      " 48.24152509 46.77394878]\n",
      "epoch 41, losses: [ 0.29191288  0.22843624  0.28663575  0.23991388  3.37346832  3.42166031\n",
      " 48.03593955 47.04806012]\n",
      "epoch 42, losses: [ 0.29253312  0.22708382  0.28616785  0.239933    3.53846734  3.31143153\n",
      " 48.08420833 46.9329581 ]\n",
      "epoch 43, losses: [ 0.29396106  0.22602994  0.28825232  0.23865566  3.37795905  3.19919171\n",
      " 46.73473446 46.33820601]\n",
      "epoch 44, losses: [ 0.29612303  0.22550183  0.28817364  0.23809118  3.36738236  3.55585535\n",
      " 46.77774514 47.75705583]\n",
      "epoch 45, losses: [ 0.29708389  0.22411242  0.28966085  0.23724233  3.24908652  3.29147415\n",
      " 45.65370207 48.31414688]\n",
      "epoch 46, losses: [ 0.29935401  0.22390719  0.29108145  0.23671316  3.17203207  3.11738421\n",
      " 45.64884391 45.79979847]\n",
      "epoch 47, losses: [ 0.30061631  0.22267491  0.28989144  0.23661075  3.19624577  3.44132371\n",
      " 45.86126457 46.33015338]\n",
      "epoch 48, losses: [ 0.3003403   0.22289891  0.29104988  0.23624977  3.28505231  3.31539386\n",
      " 45.65748956 45.93422851]\n",
      "epoch 49, losses: [ 0.30386668  0.22164301  0.29327243  0.23490387  3.32835646  3.30620697\n",
      " 48.10611236 46.45786249]\n",
      "epoch 50, losses: [ 0.30424531  0.22169548  0.29120564  0.23615831  3.28631309  3.36009405\n",
      " 48.02876652 45.53145481]\n",
      "epoch 51, losses: [ 0.30639797  0.22078497  0.28789152  0.23709593  3.64897649  3.31128106\n",
      " 48.18930732 45.59662973]\n",
      "epoch 52, losses: [ 0.30820714  0.21815998  0.28980864  0.23641188  3.12201662  3.36056444\n",
      " 46.52737961 46.26468134]\n",
      "epoch 53, losses: [ 0.30890078  0.21837007  0.28879354  0.2366501   3.22825813  3.57072077\n",
      " 46.02376633 47.37426514]\n",
      "epoch 54, losses: [ 0.31063054  0.2174196   0.28769875  0.23676177  3.14539152  3.77072214\n",
      " 47.58384653 47.23534443]\n",
      "epoch 55, losses: [ 0.31114456  0.21705083  0.28855321  0.23674775  3.17823762  3.19435046\n",
      " 47.17685039 46.07680251]\n",
      "epoch 56, losses: [ 0.31191398  0.21697094  0.28526978  0.23742383  3.10653194  3.50768206\n",
      " 46.3795717  46.98367888]\n",
      "epoch 57, losses: [ 0.31216348  0.21556284  0.28768571  0.23644287  3.0930288   3.27817387\n",
      " 45.34248604 45.80309843]\n",
      "epoch 58, losses: [ 0.31168528  0.21633307  0.28972734  0.23579194  3.31365089  3.2715674\n",
      " 46.82200026 45.94551496]\n",
      "epoch 59, losses: [ 0.31199713  0.21643895  0.28454758  0.23730079  3.17637343  3.26999175\n",
      " 45.65614368 45.83208102]\n",
      "epoch 60, losses: [ 0.31321896  0.21522485  0.28552517  0.23668899  3.1883127   3.40281765\n",
      " 45.73109825 45.68433176]\n",
      "epoch 61, losses: [ 0.3150017   0.21427231  0.28684018  0.23629555  3.18764766  3.25367848\n",
      " 45.75907824 46.0015623 ]\n",
      "epoch 62, losses: [ 0.31209618  0.21528957  0.28503319  0.23624038  3.34936302  3.34213668\n",
      " 45.40313722 46.24479333]\n",
      "epoch 63, losses: [ 0.30889226  0.21663269  0.28612991  0.23586395  3.4349136   3.37010599\n",
      " 48.21484107 45.52191248]\n",
      "epoch 64, losses: [ 0.31044583  0.21552984  0.28651956  0.2352911   3.27702489  3.37266512\n",
      " 45.83464422 45.76424781]\n",
      "epoch 65, losses: [ 0.31033828  0.21607836  0.28598354  0.23545286  3.23107869  3.16256667\n",
      " 45.15696564 46.32311597]\n",
      "epoch 66, losses: [ 0.30954768  0.21589713  0.28321322  0.23653096  3.08023171  3.18916287\n",
      " 44.17826514 45.14911724]\n",
      "epoch 67, losses: [ 0.30966125  0.21547105  0.28408831  0.23635709  3.25085867  3.23546796\n",
      " 48.19130358 45.22107859]\n",
      "epoch 68, losses: [ 0.30961593  0.21457084  0.28720133  0.23438683  3.26114831  3.24394073\n",
      " 46.58525884 46.50197056]\n",
      "epoch 69, losses: [ 0.31429458  0.21366945  0.28461826  0.23499134  3.20625914  3.28572804\n",
      " 46.19715531 44.61882439]\n",
      "epoch 70, losses: [ 0.31329086  0.21392917  0.284051    0.235321    3.14161038  3.11909439\n",
      " 44.9126311  43.89546788]\n",
      "epoch 71, losses: [ 0.3113395   0.21523184  0.28364222  0.23539273  3.15025475  3.34245956\n",
      " 42.9777252  43.91024705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72, losses: [ 0.30970096  0.21425583  0.28495494  0.23409324  3.03048677  3.06737476\n",
      " 43.17536698 43.52823415]\n",
      "epoch 73, losses: [ 0.31306534  0.21413129  0.28485356  0.23418577  3.14176776  3.10772683\n",
      " 43.82006872 43.46046828]\n",
      "epoch 74, losses: [ 0.3131392   0.2124958   0.28379017  0.23388034  2.96631384  3.00465447\n",
      " 40.62126559 42.55081527]\n",
      "epoch 75, losses: [ 0.31361453  0.21347165  0.27859005  0.23663237  3.07049055  3.18507424\n",
      " 40.70916828 41.54434998]\n",
      "epoch 76, losses: [ 0.30888666  0.21398298  0.27664247  0.23690781  3.15271483  3.00980948\n",
      " 39.51929592 42.60902947]\n",
      "epoch 77, losses: [ 0.30502521  0.21531366  0.27322162  0.24010453  3.09725085  3.08531493\n",
      " 41.18519121 40.14822191]\n",
      "epoch 78, losses: [ 0.30990125  0.21380613  0.27628405  0.23742043  2.9170387   3.04198514\n",
      " 38.8307671  40.78593149]\n",
      "epoch 79, losses: [ 0.31673083  0.21143985  0.27391413  0.23883051  3.02283078  3.19673738\n",
      " 39.55407426 40.17730065]\n",
      "epoch 80, losses: [ 0.31003652  0.21323286  0.27303273  0.23900656  2.78253612  2.84692076\n",
      " 39.35502444 38.6576028 ]\n",
      "epoch 81, losses: [ 0.31284359  0.21187721  0.27237548  0.2392645   2.78101507  2.89427201\n",
      " 36.96352268 39.94552594]\n",
      "epoch 82, losses: [ 0.31068068  0.21237558  0.27176006  0.23841811  2.72057108  2.8667384\n",
      " 37.8273787  37.69490624]\n",
      "epoch 83, losses: [ 0.3102026   0.21271199  0.27108503  0.24014153  2.87295637  2.98048163\n",
      " 39.07779737 37.61571286]\n",
      "epoch 84, losses: [ 0.30894462  0.21276342  0.26855029  0.24030876  2.88336852  2.96057665\n",
      " 40.44618068 37.08680152]\n",
      "epoch 85, losses: [ 0.3139623   0.21139733  0.27176727  0.23855418  2.73579872  2.98743134\n",
      " 38.80722852 37.51526537]\n",
      "epoch 86, losses: [ 0.32070643  0.2068744   0.27416064  0.23787214  2.58371178  2.84405443\n",
      " 38.61345165 37.35000107]\n",
      "epoch 87, losses: [ 0.31672131  0.20926343  0.27265558  0.2378766   2.57058282  2.67210319\n",
      " 37.71550241 35.74200904]\n",
      "epoch 88, losses: [ 0.31858041  0.20771346  0.27573417  0.23660351  2.55540094  2.70987409\n",
      " 38.83976664 35.82371836]\n",
      "epoch 89, losses: [ 0.3165471   0.20839503  0.27195598  0.23733459  2.72640982  2.86919283\n",
      " 38.34774744 35.84754767]\n",
      "epoch 90, losses: [ 0.31624009  0.20932689  0.27314582  0.23693199  2.54707081  2.56906423\n",
      " 37.08438557 35.32597612]\n",
      "epoch 91, losses: [ 0.31967429  0.20717688  0.27478395  0.23664133  2.58729359  2.84020025\n",
      " 38.10537675 35.30154067]\n",
      "epoch 92, losses: [ 0.30922631  0.21131153  0.27024311  0.23802209  2.50958484  2.58539557\n",
      " 35.9683201  34.98717865]\n",
      "epoch 93, losses: [ 0.31518822  0.21040012  0.27481797  0.235621    2.88510467  2.98688953\n",
      " 38.71217724 38.10646451]\n",
      "epoch 94, losses: [ 0.31375841  0.20980403  0.27393136  0.23629569  2.63594705  2.54273203\n",
      " 35.3657242  35.50081041]\n",
      "epoch 95, losses: [ 0.31267094  0.20970474  0.27358798  0.23636986  2.58728264  2.47358583\n",
      " 34.53246287 34.51569039]\n",
      "epoch 96, losses: [ 0.31245657  0.210842    0.26918859  0.23773834  2.50642996  2.51310139\n",
      " 33.09618361 34.26592357]\n",
      "epoch 97, losses: [ 0.30433795  0.21435858  0.2759625   0.23439593  2.52476557  2.46706557\n",
      " 34.39159243 34.19770082]\n",
      "epoch 98, losses: [ 0.31841901  0.20877518  0.27571077  0.23530815  2.40740589  2.5115417\n",
      " 33.63646929 33.93727926]\n",
      "epoch 99, losses: [ 0.31605079  0.20723742  0.27678989  0.23381202  2.41016641  2.38412146\n",
      " 33.38724353 33.84779783]\n",
      "epoch 100, losses: [ 0.31402072  0.21005187  0.27707474  0.23391023  2.47915264  2.65905453\n",
      " 35.7810476  35.10142421]\n",
      "epoch 101, losses: [ 0.31269258  0.2103593   0.27515468  0.23434109  2.39722966  2.32142612\n",
      " 32.01824068 33.46795155]\n",
      "epoch 102, losses: [ 0.31652912  0.20742272  0.28096026  0.23122903  2.56455607  2.40170262\n",
      " 33.84114529 33.31633068]\n",
      "epoch 103, losses: [ 0.31531475  0.20852752  0.27734263  0.23290808  2.38627773  2.40561568\n",
      " 33.82828906 32.62049439]\n",
      "epoch 104, losses: [ 0.31436724  0.20867149  0.27719392  0.2326303   2.46862608  2.3575628\n",
      " 33.53322051 33.12378313]\n",
      "epoch 105, losses: [ 0.31842174  0.20691411  0.27817366  0.23286926  2.62315215  2.51649524\n",
      " 32.98651523 33.08672487]\n",
      "epoch 106, losses: [ 0.3157879   0.20780247  0.27355264  0.23360603  2.68268009  2.36402929\n",
      " 31.07697141 34.89601398]\n",
      "epoch 107, losses: [ 0.31845891  0.20773034  0.27585145  0.23361667  2.4267561   2.22947738\n",
      " 30.98919669 33.50618101]\n",
      "epoch 108, losses: [ 0.3138652   0.20851735  0.27855447  0.2318234   2.33242806  2.16447371\n",
      " 30.80162887 32.52222265]\n",
      "epoch 109, losses: [ 0.31375336  0.20865929  0.27771512  0.23124877  2.44564348  2.28125295\n",
      " 30.2649693  32.64777723]\n",
      "epoch 110, losses: [ 0.31277946  0.20973402  0.27351885  0.23311374  2.30268505  2.1740388\n",
      " 29.16916401 32.75904261]\n",
      "epoch 111, losses: [ 0.32201502  0.20536993  0.27903284  0.23146823  2.48476659  2.45472808\n",
      " 35.85247744 32.41220848]\n",
      "epoch 112, losses: [ 0.32457355  0.20224825  0.27603987  0.23245792  2.4001609   2.47577091\n",
      " 33.04074968 32.70894723]\n",
      "epoch 113, losses: [ 0.3033928   0.21216161  0.28146182  0.22968404  2.34320975  2.2770926\n",
      " 32.28691662 32.07807496]\n",
      "epoch 114, losses: [ 0.28403141  0.22034275  0.2784174   0.23083993  2.38707904  1.99938936\n",
      " 29.04959401 31.53699606]\n",
      "epoch 115, losses: [ 0.27995402  0.22243061  0.2763846   0.23154313  2.51934856  2.16470257\n",
      " 27.36494085 32.52567384]\n",
      "epoch 116, losses: [ 0.27589791  0.22350866  0.28180024  0.22954923  2.43852012  2.1511344\n",
      " 27.28596095 32.61671058]\n",
      "epoch 117, losses: [ 0.26774165  0.22836291  0.28116749  0.22929731  2.35494048  2.15076911\n",
      " 27.8306437  32.17566529]\n",
      "epoch 118, losses: [ 0.26365208  0.22946173  0.27963418  0.22964559  2.31987222  1.95278955\n",
      " 25.65716519 32.6268954 ]\n",
      "epoch 119, losses: [ 0.26779145  0.22669714  0.28172686  0.22918851  2.19932332  1.82410244\n",
      " 26.2404145  31.59839641]\n",
      "epoch 120, losses: [ 0.26791842  0.22756466  0.2810782   0.22836208  2.30740469  1.95370533\n",
      " 25.43868584 32.96505877]\n",
      "epoch 121, losses: [ 0.25811243  0.23209834  0.28037926  0.22847621  2.52225007  1.90277951\n",
      " 22.8669178  33.9721886 ]\n",
      "epoch 122, losses: [ 0.26285758  0.23035899  0.28037516  0.22917209  2.2995961   2.01593213\n",
      " 24.58578526 32.94390314]\n",
      "epoch 123, losses: [ 0.25820697  0.23118181  0.28027053  0.22846123  2.4584316   1.70772389\n",
      " 20.39195381 31.59179696]\n",
      "epoch 124, losses: [ 0.25088647  0.23622422  0.2756261   0.2311388   2.19201481  1.32720112\n",
      " 18.03669721 31.38128765]\n",
      "epoch 125, losses: [ 0.25077349  0.23639043  0.28247139  0.22798163  2.18987884  2.02866846\n",
      " 20.88324784 31.65290621]\n",
      "epoch 126, losses: [ 0.24863983  0.2360369   0.28322348  0.22735488  2.14930735  1.54103705\n",
      " 19.6860002  31.38323252]\n",
      "epoch 127, losses: [ 0.25831555  0.2330278   0.27811278  0.22874482  2.19947291  1.562334\n",
      " 18.82789069 31.47706743]\n",
      "epoch 128, losses: [ 0.25097173  0.23352778  0.28454036  0.22683045  2.17838265  1.75249405\n",
      " 19.79583076 31.58649573]\n",
      "epoch 129, losses: [ 0.25345465  0.23589634  0.28219807  0.2267779   2.34822572  1.38579917\n",
      " 16.53694472 32.92718084]\n",
      "epoch 130, losses: [ 0.26263968  0.23028355  0.28087963  0.22713484  2.42952987  1.62157237\n",
      " 18.87040877 33.13502228]\n",
      "epoch 131, losses: [ 0.26088565  0.23061683  0.27812781  0.2287348   2.28937868  1.61159233\n",
      " 21.49212535 32.0635973 ]\n",
      "epoch 132, losses: [ 0.25402135  0.23229379  0.28166307  0.22784854  2.22546622  1.60097931\n",
      " 20.94519403 31.92345155]\n",
      "epoch 133, losses: [ 0.25408458  0.23230231  0.28184577  0.22593432  2.26561908  1.49440599\n",
      " 19.35311161 31.53665587]\n",
      "epoch 134, losses: [ 0.25737383  0.2323688   0.28302076  0.22673165  2.20282219  1.4283554\n",
      " 19.02121201 31.88374261]\n",
      "epoch 135, losses: [ 0.26110529  0.23189849  0.27901131  0.22682965  2.19693798  1.52617428\n",
      " 18.80206158 31.96261228]\n",
      "epoch 136, losses: [ 0.25373439  0.23318035  0.27706775  0.22861321  2.21348887  1.2990323\n",
      " 16.45879289 32.4666872 ]\n",
      "epoch 137, losses: [ 0.25407634  0.23312278  0.28268323  0.22585074  2.4692799   1.37925079\n",
      " 17.39371883 33.1111605 ]\n",
      "epoch 138, losses: [ 0.25650553  0.23186037  0.27131532  0.23030294  2.27086777  1.65871524\n",
      " 18.96344114 32.06126212]\n",
      "epoch 139, losses: [ 0.26448804  0.22855847  0.28005867  0.2276901   2.20497967  1.34542872\n",
      " 17.77824138 31.27812421]\n",
      "epoch 140, losses: [ 0.25750571  0.23125456  0.27994065  0.22692996  2.22387085  1.26318733\n",
      " 16.12723142 31.80770551]\n",
      "epoch 141, losses: [ 0.25371698  0.23289304  0.28242122  0.22560673  2.15357904  1.2832197\n",
      " 17.21175288 31.28196622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 142, losses: [ 0.25709952  0.2307849   0.28291057  0.22553932  2.0921051   1.21898775\n",
      " 17.22982599 31.0615606 ]\n",
      "epoch 143, losses: [ 0.25793322  0.23020062  0.2839899   0.22381046  2.0735553   1.30678739\n",
      " 18.0467854  31.03637465]\n",
      "epoch 144, losses: [ 0.25721881  0.23123678  0.28680836  0.2232904   2.11540682  1.39830176\n",
      " 19.00828768 31.06103902]\n",
      "epoch 145, losses: [ 0.26128373  0.22854785  0.28498985  0.22369818  2.16760971  1.40309865\n",
      " 17.93784738 31.14245724]\n",
      "epoch 146, losses: [ 0.26070183  0.23074835  0.28439101  0.22396685  2.1573363   1.43650083\n",
      " 19.44852229 31.0409643 ]\n",
      "epoch 147, losses: [ 0.26190149  0.22847522  0.27928669  0.22578762  2.30868598  1.62317848\n",
      " 18.11000571 32.73197447]\n",
      "epoch 148, losses: [ 0.26302561  0.22649838  0.27943431  0.22535689  2.34609138  1.60548239\n",
      " 20.05891943 32.03880601]\n",
      "epoch 149, losses: [ 0.2626281   0.22834699  0.28033651  0.22546508  2.2219703   1.28264993\n",
      " 17.3316468  32.01912249]\n",
      "epoch 150, losses: [ 0.26060155  0.22879335  0.27801945  0.2259143   2.35525794  1.41070754\n",
      " 17.77498478 34.25989407]\n",
      "epoch 151, losses: [ 0.26082129  0.22882238  0.28124833  0.2255283   2.13116602  1.28289115\n",
      " 16.81637022 31.20007499]\n",
      "epoch 152, losses: [ 0.26545668  0.22621607  0.28006519  0.22518438  2.17869352  1.53182113\n",
      " 20.55653598 30.32772418]\n",
      "epoch 153, losses: [ 0.26425001  0.22862711  0.28038316  0.22487355  2.14296375  1.48540434\n",
      " 19.41255559 30.2724673 ]\n",
      "epoch 154, losses: [ 0.26723522  0.22389979  0.27720692  0.22626058  2.02321764  1.49409349\n",
      " 19.06096295 29.45854798]\n",
      "epoch 155, losses: [ 0.26211633  0.22705424  0.27974964  0.22500622  2.03151271  1.3254357\n",
      " 17.08338022 30.22901809]\n",
      "epoch 156, losses: [ 0.26270869  0.2276089   0.27403062  0.22753321  2.11688468  1.32314116\n",
      " 17.2184487  30.56146173]\n",
      "epoch 157, losses: [ 0.26376847  0.22664101  0.27602589  0.22649753  1.98908638  1.35392807\n",
      " 17.16043926 28.80308127]\n",
      "epoch 158, losses: [ 0.26160754  0.22592547  0.28492242  0.22269039  2.06276911  1.54445132\n",
      " 21.20177169 28.95983893]\n",
      "epoch 159, losses: [ 0.26567084  0.22676729  0.27690517  0.22651289  1.96066236  1.4836073\n",
      " 19.07790484 27.46632046]\n",
      "epoch 160, losses: [ 0.26827537  0.22415695  0.27689433  0.22469013  1.93580915  1.3897225\n",
      " 17.60188115 27.81865017]\n",
      "epoch 161, losses: [ 0.26617304  0.22557082  0.28129239  0.2246273   1.90922009  1.16896791\n",
      " 16.32065921 27.24606735]\n",
      "epoch 162, losses: [ 0.26351078  0.22677976  0.27484473  0.22523819  1.88031255  1.30400452\n",
      " 18.3344639  27.6730563 ]\n",
      "epoch 163, losses: [ 0.262626    0.22709873  0.27777654  0.22573093  1.90094102  1.3295378\n",
      " 18.55513537 26.05709709]\n",
      "epoch 164, losses: [ 0.26346602  0.22545556  0.27297988  0.22688137  1.87329717  1.51274508\n",
      " 19.01468557 25.94910211]\n",
      "epoch 165, losses: [ 0.26402638  0.22701643  0.27839501  0.22351462  1.86171394  1.38467995\n",
      " 17.92197721 25.61186299]\n",
      "epoch 166, losses: [ 0.26520647  0.22682104  0.27573412  0.22597976  1.83840465  1.15651073\n",
      " 15.76801782 25.16096213]\n",
      "epoch 167, losses: [ 0.26573625  0.2257004   0.27570644  0.22524765  1.92897102  1.51536863\n",
      " 16.57303964 25.9056313 ]\n",
      "epoch 168, losses: [ 0.26333302  0.22603109  0.2755711   0.22603048  1.84589114  1.3679732\n",
      " 17.4895664  26.70115636]\n",
      "epoch 169, losses: [ 0.26794086  0.22431229  0.27653417  0.22468053  1.85186697  1.49241276\n",
      " 20.13866272 25.3746452 ]\n",
      "epoch 170, losses: [ 0.26922473  0.22376835  0.27240126  0.2255716   1.82853566  1.34519333\n",
      " 17.37228149 24.81961475]\n",
      "epoch 171, losses: [ 0.26136622  0.22755151  0.27442948  0.22400769  1.67221845  1.30678103\n",
      " 16.45316808 24.53453727]\n",
      "epoch 172, losses: [ 0.26729745  0.22345718  0.27110164  0.22790573  1.8325596   1.36788064\n",
      " 18.39408015 24.60272882]\n",
      "epoch 173, losses: [ 0.26580385  0.22476933  0.26912394  0.22719567  1.66980622  1.17222166\n",
      " 16.52891336 24.45500636]\n",
      "epoch 174, losses: [ 0.26601896  0.22584674  0.2659273   0.22785657  1.94470444  1.30913946\n",
      " 16.51234683 27.00992405]\n",
      "epoch 175, losses: [ 0.26639382  0.22397483  0.26525053  0.2291743   1.79556395  1.18040088\n",
      " 16.5098311  26.14510658]\n",
      "epoch 176, losses: [ 0.26219586  0.22591368  0.26841313  0.22774851  1.66970937  1.32804895\n",
      " 16.96452789 24.41274933]\n",
      "epoch 177, losses: [ 0.26026274  0.22758958  0.27021301  0.22628593  1.63189329  1.36560965\n",
      " 17.76055703 24.58741383]\n",
      "epoch 178, losses: [ 0.26259957  0.22635873  0.26963395  0.22727436  1.76807188  1.17576949\n",
      " 15.76884882 24.52608965]\n",
      "epoch 179, losses: [ 0.26196745  0.22765506  0.27058896  0.22534769  1.97725509  1.26018796\n",
      " 17.49990137 23.74098674]\n",
      "epoch 180, losses: [ 0.26890219  0.22401543  0.26945203  0.22654476  1.92521028  1.68171192\n",
      " 21.07764617 24.02048904]\n",
      "epoch 181, losses: [ 0.26283365  0.22469419  0.26724613  0.2265991   1.89199759  1.40742734\n",
      " 18.4483807  25.9088027 ]\n",
      "epoch 182, losses: [ 0.2632456   0.22677288  0.26747627  0.2281485   1.70602038  1.1742115\n",
      " 16.84146418 24.42025492]\n",
      "epoch 183, losses: [ 0.27081558  0.2212365   0.26186499  0.22886368  1.65352856  1.23158401\n",
      " 15.63288106 23.57439955]\n",
      "epoch 184, losses: [ 0.26959331  0.22359417  0.26246     0.22947228  1.6129439   1.24026816\n",
      " 17.48029469 23.16441255]\n",
      "epoch 185, losses: [ 0.26317782  0.22515424  0.25941852  0.23116535  1.94143206  1.44944768\n",
      " 18.9784398  25.21863666]\n",
      "epoch 186, losses: [ 0.27049843  0.22370094  0.2573999   0.23177479  1.67031935  1.52232105\n",
      " 18.72453863 22.65803105]\n",
      "epoch 187, losses: [ 0.27212755  0.22128253  0.26023274  0.23143044  1.51408231  1.26354172\n",
      " 16.98118795 21.00065618]\n",
      "epoch 188, losses: [ 0.26939796  0.22216077  0.25989496  0.22975744  1.40608339  1.17173234\n",
      " 16.25238677 20.23715851]\n",
      "epoch 189, losses: [ 0.2695168   0.22248711  0.25569529  0.23164546  1.4466796   1.25839998\n",
      " 17.41194683 21.08566801]\n",
      "epoch 190, losses: [ 0.26956488  0.22163064  0.25279569  0.23407268  1.46799247  1.20794323\n",
      " 16.45025029 21.20110587]\n",
      "epoch 191, losses: [ 0.27095996  0.22220869  0.25610454  0.2331538   1.39389565  1.10667341\n",
      " 15.95976467 19.46662361]\n",
      "epoch 192, losses: [ 0.27199398  0.2215263   0.25119316  0.23425487  1.41556745  1.10376523\n",
      " 16.10981085 18.81395643]\n",
      "epoch 193, losses: [ 0.26624931  0.2237075   0.25912283  0.2309041   1.31834286  1.13517299\n",
      " 16.75138434 18.56128286]\n",
      "epoch 194, losses: [ 0.263888    0.22373708  0.25459776  0.2327356   1.37067001  1.21545169\n",
      " 17.56747095 19.26049912]\n",
      "epoch 195, losses: [ 0.2690399   0.22291776  0.25850338  0.23128435  1.69186694  1.7140169\n",
      " 20.81669493 20.61612835]\n",
      "epoch 196, losses: [ 0.26985804  0.2223561   0.25606585  0.23195507  1.38387104  1.63762413\n",
      " 17.12325808 19.3162283 ]\n",
      "epoch 197, losses: [ 0.2747921   0.22027115  0.25980525  0.23036447  1.36690791  1.40949914\n",
      " 18.19141031 19.29099905]\n",
      "epoch 198, losses: [ 0.27173609  0.22095672  0.25530454  0.23198499  1.27666627  1.27388508\n",
      " 17.49428548 18.4269466 ]\n",
      "epoch 199, losses: [ 0.26921075  0.22483668  0.25673283  0.23084171  1.26214561  0.96754193\n",
      " 13.68008121 18.64967565]\n",
      "epoch 200, losses: [ 0.27139331  0.22201187  0.25517823  0.23277349  1.41083522  1.43755238\n",
      " 19.0713362  18.67025742]\n",
      "epoch 201, losses: [ 0.27130538  0.21966575  0.25030524  0.23416472  1.39287088  1.23818432\n",
      " 17.78537608 18.61664624]\n",
      "epoch 202, losses: [ 0.26649662  0.22427477  0.25076735  0.23524472  1.43383577  1.18526175\n",
      " 16.64668029 18.98020612]\n",
      "epoch 203, losses: [ 0.26635328  0.22314906  0.25639219  0.23089822  1.38826301  1.31520379\n",
      " 17.34959135 17.86956098]\n",
      "epoch 204, losses: [ 0.27275415  0.22128087  0.25459328  0.23247584  1.53916724  1.45772875\n",
      " 18.13877108 19.18810624]\n",
      "epoch 205, losses: [ 0.26962704  0.22351342  0.25780385  0.23046232  1.3142005   1.12504345\n",
      " 15.76425151 18.28772624]\n",
      "epoch 206, losses: [ 0.2788042   0.21724057  0.25419632  0.23212182  1.26523151  1.48300509\n",
      " 19.84372041 17.55320139]\n",
      "epoch 207, losses: [ 0.27326655  0.2199305   0.25140458  0.23369964  1.27252229  1.33423223\n",
      " 17.62824597 17.59914716]\n",
      "epoch 208, losses: [ 0.27654094  0.21967151  0.25643661  0.23195748  1.29268719  1.276283\n",
      " 18.94730489 17.19481331]\n",
      "epoch 209, losses: [ 0.27383457  0.22037492  0.26113388  0.22946011  1.15680389  1.13258341\n",
      " 16.53570299 16.79308341]\n",
      "epoch 210, losses: [ 0.27242407  0.22162906  0.25441112  0.23098717  1.4208244   1.23947915\n",
      " 15.9356852  19.73430302]\n",
      "epoch 211, losses: [ 0.27258188  0.22148999  0.25675361  0.23145282  1.46689832  1.27984852\n",
      " 16.95130916 20.16524503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 212, losses: [ 0.27358688  0.22134049  0.25697448  0.23069488  1.37205114  1.03503976\n",
      " 14.25509429 17.5594961 ]\n",
      "epoch 213, losses: [ 0.26925001  0.22275992  0.25554195  0.2302985   1.21883199  1.02713868\n",
      " 14.88783189 17.37299063]\n",
      "epoch 214, losses: [ 0.26964947  0.22240099  0.25456234  0.23214046  1.12792987  1.06882982\n",
      " 15.5836714  16.82671299]\n",
      "epoch 215, losses: [ 0.26859439  0.22163513  0.25070311  0.23300168  1.16154869  1.24080157\n",
      " 18.11996616 16.81172335]\n",
      "epoch 216, losses: [ 0.26762561  0.22256789  0.25391782  0.23242512  1.32217838  1.15562916\n",
      " 15.29859026 19.1491739 ]\n",
      "epoch 217, losses: [ 0.27142718  0.22234538  0.26118443  0.2286411   1.50447709  1.48213333\n",
      " 18.76679998 18.53799404]\n",
      "epoch 218, losses: [ 0.28545028  0.21506275  0.25978443  0.2285463   1.31207797  1.42155831\n",
      " 19.5795909  17.79800662]\n",
      "epoch 219, losses: [ 0.27481478  0.2193303   0.25546031  0.23153795  1.23545573  1.3028917\n",
      " 17.01762288 17.00273764]\n",
      "epoch 220, losses: [ 0.28267011  0.21551408  0.24915397  0.23377762  1.23694051  1.42468409\n",
      " 18.32317162 17.3656879 ]\n",
      "epoch 221, losses: [ 0.27302358  0.22065925  0.2536931   0.23098862  1.18069492  1.22539864\n",
      " 16.31750589 16.72434676]\n",
      "epoch 222, losses: [ 0.27749415  0.21824178  0.25376513  0.23185089  1.13763223  1.20207083\n",
      " 16.46026835 16.92773282]\n",
      "epoch 223, losses: [ 0.27421681  0.21908417  0.25163334  0.23223777  1.14060557  1.15314481\n",
      " 16.81239089 16.47020545]\n",
      "epoch 224, losses: [ 0.27127109  0.21989292  0.25059667  0.23353781  1.11611041  1.07459573\n",
      " 15.72367126 16.38731261]\n",
      "epoch 225, losses: [ 0.27079303  0.22062096  0.25530788  0.23201886  1.43507803  1.64262961\n",
      " 18.76856055 18.58653545]\n",
      "epoch 226, losses: [ 0.27911777  0.21830702  0.25592598  0.2296376   1.23940408  1.55127114\n",
      " 17.67050239 17.36637003]\n",
      "epoch 227, losses: [ 0.27923845  0.21801142  0.25068796  0.23274527  1.15119785  1.18019132\n",
      " 15.81815325 16.56999983]\n",
      "epoch 228, losses: [ 0.27282204  0.22011842  0.25072429  0.23340944  1.16492033  1.12174368\n",
      " 15.32706573 16.56252477]\n",
      "epoch 229, losses: [ 0.27065253  0.22317793  0.25570165  0.23076197  1.26613748  1.52985936\n",
      " 15.73825547 16.70793664]\n",
      "epoch 230, losses: [ 0.27113017  0.21803775  0.25227201  0.23227937  1.46666277  1.60252358\n",
      " 19.46280151 18.42917654]\n",
      "epoch 231, losses: [ 0.28612854  0.21635875  0.26103235  0.22852659  1.22136115  1.35486898\n",
      " 19.48945982 16.41654735]\n",
      "epoch 232, losses: [ 0.28367348  0.21552792  0.25141964  0.23235678  1.39664051  1.22890754\n",
      " 17.68648805 16.14163213]\n",
      "epoch 233, losses: [ 0.28520266  0.21504593  0.25800107  0.22976886  1.1220387   1.37956219\n",
      " 18.48910816 16.18540515]\n",
      "epoch 234, losses: [ 0.27956529  0.21632588  0.25871383  0.22779014  1.15871972  1.11249508\n",
      " 15.73058494 16.32263519]\n",
      "epoch 235, losses: [ 0.28412548  0.21403836  0.25546507  0.23059271  1.26632406  1.24282049\n",
      " 17.70366717 17.40863809]\n",
      "epoch 236, losses: [ 0.28257297  0.21514695  0.25578913  0.23166713  1.36889612  1.12243415\n",
      " 15.77721327 19.50150695]\n",
      "epoch 237, losses: [ 0.27879138  0.21537893  0.25276292  0.23043634  1.21571852  1.30595471\n",
      " 18.40066725 16.93387914]\n",
      "epoch 238, losses: [ 0.27459514  0.21974298  0.2522397   0.23211555  1.17854265  1.10842962\n",
      " 16.1894255  16.04948669]\n",
      "epoch 239, losses: [ 0.27598575  0.21900779  0.24900848  0.23226706  1.14132458  1.06840404\n",
      " 15.59527264 16.20246212]\n",
      "epoch 240, losses: [ 0.27772637  0.21863208  0.25125636  0.23310932  1.23304291  1.21272214\n",
      " 16.86418101 16.61832068]\n",
      "epoch 241, losses: [ 0.27561431  0.21966184  0.25178034  0.23235334  1.4609384   1.7091432\n",
      " 19.39158733 17.20067983]\n",
      "epoch 242, losses: [ 0.28191837  0.21292559  0.2585322   0.22949579  1.37534561  1.39318839\n",
      " 19.92173863 17.25659961]\n",
      "epoch 243, losses: [ 0.28118626  0.21737842  0.25729065  0.22854799  1.20385707  1.10342155\n",
      " 15.51982771 16.43596873]\n",
      "epoch 244, losses: [ 0.2742965   0.22037353  0.25445643  0.23083536  1.09118083  1.02799426\n",
      " 14.75842252 15.82757549]\n",
      "epoch 245, losses: [ 0.27382243  0.21939329  0.25171845  0.23157903  1.08752464  1.16246798\n",
      " 16.94429626 15.7645834 ]\n",
      "epoch 246, losses: [ 0.27201847  0.22068377  0.24935812  0.2325505   1.10316227  1.07777402\n",
      " 15.63286796 15.67665649]\n",
      "epoch 247, losses: [ 0.28286904  0.21650126  0.24900952  0.23341523  1.35286161  1.47732679\n",
      " 18.98736064 16.96124916]\n",
      "epoch 248, losses: [ 0.28536955  0.21601619  0.26278346  0.22757904  1.35573656  1.23511667\n",
      " 16.36922659 17.9068845 ]\n",
      "epoch 249, losses: [ 0.28521817  0.21407042  0.24569972  0.23305779  1.23964884  1.20001052\n",
      " 16.84747626 16.61508252]\n",
      "epoch 250, losses: [ 0.28356524  0.21445498  0.25544373  0.23100728  1.14888929  1.14037233\n",
      " 16.5947908  16.40686647]\n",
      "epoch 251, losses: [ 0.2778196   0.21566235  0.25264634  0.23171769  1.17005208  1.24579755\n",
      " 18.19728127 15.71895433]\n",
      "epoch 252, losses: [ 0.28834836  0.21462977  0.25312492  0.23017394  1.20203294  1.36754705\n",
      " 16.08592706 16.35931561]\n",
      "epoch 253, losses: [ 0.28272051  0.21491343  0.25684507  0.22991161  1.17106573  1.35114569\n",
      " 17.77606138 17.42904159]\n",
      "epoch 254, losses: [ 0.28149567  0.21704191  0.25526264  0.23020358  1.07154511  1.05704342\n",
      " 15.33189496 15.80008668]\n",
      "epoch 255, losses: [ 0.27965888  0.21689379  0.25719968  0.22854902  1.09106593  1.10023986\n",
      " 15.9472982  15.69067755]\n",
      "epoch 256, losses: [ 0.27760593  0.21978047  0.251485    0.23254937  1.07276414  0.97910707\n",
      " 14.12170214 15.66037146]\n",
      "epoch 257, losses: [ 0.27474202  0.21874144  0.24566173  0.23321788  1.0590404   1.16813917\n",
      " 17.13493804 15.62674805]\n",
      "epoch 258, losses: [ 0.2734994   0.21991671  0.24843022  0.23319174  1.06169097  1.17027591\n",
      " 16.58844407 15.69159351]\n",
      "epoch 259, losses: [ 0.28408812  0.21618951  0.25681862  0.22891686  1.51296538  1.55747501\n",
      " 16.85064671 18.62266219]\n",
      "epoch 260, losses: [ 0.28624048  0.21481429  0.2577244   0.2290911   1.30844252  1.32532188\n",
      " 17.80197275 16.51466688]\n",
      "epoch 261, losses: [ 0.28428358  0.21375635  0.25273412  0.23174946  1.125307    1.11452046\n",
      " 16.00639551 15.8734053 ]\n",
      "epoch 262, losses: [ 0.28409308  0.21595161  0.25804886  0.22833314  1.21508294  1.12244482\n",
      " 16.00864583 17.19531016]\n",
      "epoch 263, losses: [ 0.28653253  0.2134335   0.25819     0.22863334  1.37922826  1.2264387\n",
      " 17.02859633 16.91237822]\n",
      "epoch 264, losses: [ 0.28472463  0.21387379  0.26302972  0.22647224  1.19441853  1.16631607\n",
      " 17.03783167 16.67519717]\n",
      "epoch 265, losses: [ 0.27980529  0.21653933  0.26170296  0.22646922  1.10143956  1.07534033\n",
      " 15.63587477 16.12474739]\n",
      "epoch 266, losses: [ 0.278665    0.21645712  0.25323496  0.22950939  1.07998641  1.09523269\n",
      " 15.79664499 15.90955989]\n",
      "epoch 267, losses: [ 0.27906006  0.21816691  0.25374693  0.23025566  1.06652056  1.07031601\n",
      " 14.84712714 15.73013913]\n",
      "epoch 268, losses: [ 0.28279083  0.21456886  0.25795712  0.22885241  1.05294112  1.2882015\n",
      " 17.49703587 15.76004885]\n",
      "epoch 269, losses: [ 0.28131115  0.21580356  0.26003613  0.22703439  1.29715455  1.09708374\n",
      " 15.612145   15.90366005]\n",
      "epoch 270, losses: [ 0.2835882   0.21461502  0.25109376  0.23089236  1.17640815  1.27658763\n",
      " 17.72754674 16.18501226]\n",
      "epoch 271, losses: [ 0.28953724  0.21229598  0.26181166  0.22698151  1.62764423  1.53829335\n",
      " 20.39731883 18.09036499]\n",
      "epoch 272, losses: [ 0.28981771  0.21679398  0.25510764  0.22904771  1.29690132  1.11340854\n",
      " 14.89525356 17.07262165]\n",
      "epoch 273, losses: [ 0.29004197  0.21034976  0.25065308  0.23187388  1.19555669  1.19269223\n",
      " 17.40593783 15.72756226]\n",
      "epoch 274, losses: [ 0.28933448  0.21303393  0.25094936  0.23156253  1.11929999  1.0572598\n",
      " 15.45518984 15.62116189]\n",
      "epoch 275, losses: [ 0.2870979   0.21703462  0.25015016  0.23097596  1.12632091  1.03327311\n",
      " 15.12998222 15.59745336]\n",
      "epoch 276, losses: [ 0.28284445  0.21412159  0.24334203  0.23498344  1.0634804   1.15884722\n",
      " 17.01533973 15.58486857]\n",
      "epoch 277, losses: [ 0.28191607  0.21763662  0.24531288  0.23373709  1.05496659  1.01834386\n",
      " 14.91439265 15.57182871]\n",
      "epoch 278, losses: [ 0.27963874  0.21511612  0.24352058  0.23594785  1.0533362   1.22518818\n",
      " 18.02094902 15.55892452]\n",
      "epoch 279, losses: [ 0.27741805  0.21560293  0.23366109  0.23905375  1.05210597  1.19560764\n",
      " 17.58833882 15.54701417]\n",
      "epoch 280, losses: [ 0.27578063  0.21926653  0.23544032  0.23837494  1.05062428  1.09879177\n",
      " 16.13350941 15.53397931]\n",
      "epoch 281, losses: [ 0.28565379  0.21402881  0.24829766  0.23328985  1.63841537  1.5710724\n",
      " 18.76375189 19.09765875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 282, losses: [ 0.29303502  0.21190709  0.25681675  0.23023118  1.34326236  1.4058092\n",
      " 18.29791611 17.18407949]\n",
      "epoch 283, losses: [ 0.28813929  0.21396176  0.25169289  0.23030576  1.34071957  1.21631368\n",
      " 16.39421234 16.35949754]\n",
      "epoch 284, losses: [ 0.282396    0.21421704  0.25362872  0.22996045  1.12613728  1.21551603\n",
      " 16.95621589 15.88444729]\n",
      "epoch 285, losses: [ 0.28250615  0.2163609   0.252202    0.23032946  1.06704543  1.07935664\n",
      " 15.44835017 15.589198  ]\n",
      "epoch 286, losses: [ 0.28123126  0.21510868  0.25509398  0.2301581   1.04896217  1.10631644\n",
      " 15.12032112 15.62029262]\n",
      "epoch 287, losses: [ 0.28041196  0.21661616  0.25149562  0.22994153  1.0423878   1.16552672\n",
      " 16.85290689 15.55653779]\n",
      "epoch 288, losses: [ 0.28297929  0.2138246   0.24283696  0.23477299  1.08168311  1.31712444\n",
      " 18.80654874 15.6107837 ]\n",
      "epoch 289, losses: [ 0.29605013  0.2111644   0.25350666  0.23145043  1.11549936  1.48686538\n",
      " 16.79415328 16.14667426]\n",
      "epoch 290, losses: [ 0.28806167  0.21102255  0.25691331  0.22873995  1.36456966  1.44540476\n",
      " 17.82783268 18.06250403]\n",
      "epoch 291, losses: [ 0.28758838  0.21336697  0.25930761  0.22735606  1.19514029  1.23282726\n",
      " 17.06347056 16.26760124]\n",
      "epoch 292, losses: [ 0.2889462   0.21125759  0.25135939  0.23163608  1.06978228  1.24276418\n",
      " 17.79198387 15.76475435]\n",
      "epoch 293, losses: [ 0.29015743  0.21417863  0.24569795  0.2345612   1.06016386  1.15048192\n",
      " 16.6397177  15.62370765]\n",
      "epoch 294, losses: [ 0.28532775  0.21634338  0.24574996  0.23353577  1.08448214  1.00779896\n",
      " 14.45894191 15.59712062]\n",
      "epoch 295, losses: [ 0.28912914  0.21118723  0.25018022  0.23197697  1.34132094  1.56055022\n",
      " 18.97761934 15.91803632]\n",
      "epoch 296, losses: [ 0.29545616  0.21112227  0.26066594  0.22773432  1.19520375  1.30214788\n",
      " 16.26525367 17.48679801]\n",
      "epoch 297, losses: [ 0.29019048  0.21077203  0.25665019  0.22781023  1.14787923  1.18709494\n",
      " 16.04885657 16.67215579]\n",
      "epoch 298, losses: [ 0.28817926  0.20951146  0.25067738  0.23130604  1.22569864  1.25076457\n",
      " 18.41784901 15.59777746]\n",
      "epoch 299, losses: [ 0.28659312  0.2150432   0.25420655  0.23042738  1.06969489  1.04094896\n",
      " 15.22840426 15.55993963]\n",
      "epoch 300, losses: [ 0.28342605  0.21641373  0.25030281  0.23072593  1.03913065  1.16278096\n",
      " 17.07280513 15.53981206]\n",
      "epoch 301, losses: [ 0.28052829  0.21647547  0.25055807  0.23191822  1.0372882   1.06296854\n",
      " 15.58919303 15.52381655]\n",
      "epoch 302, losses: [ 0.27970218  0.21386255  0.24723061  0.23282576  1.03609661  1.25688031\n",
      " 18.5044412  15.51175138]\n",
      "epoch 303, losses: [ 0.27749583  0.21876543  0.24548276  0.23394823  1.03463095  1.04105561\n",
      " 15.26628377 15.49871971]\n",
      "epoch 304, losses: [ 0.27547955  0.21458282  0.23949647  0.23594852  1.03366024  1.2194967\n",
      " 17.94612725 15.48692912]\n",
      "epoch 305, losses: [ 0.2736998   0.22072614  0.24283244  0.23545198  1.03229831  1.0239389\n",
      " 15.02538565 15.47225266]\n",
      "epoch 306, losses: [ 0.27394004  0.21751231  0.24071623  0.2358885   1.0311403   1.12111825\n",
      " 16.48767764 15.45881683]\n",
      "epoch 307, losses: [ 0.27315228  0.21986631  0.23687122  0.2375975   1.03016757  1.18114744\n",
      " 17.39280972 15.44513987]\n",
      "epoch 308, losses: [ 0.28261607  0.21576002  0.24180991  0.23627524  1.5954565   1.80389033\n",
      " 19.83065089 18.72503975]\n",
      "epoch 309, losses: [ 0.29235204  0.21563997  0.26169042  0.22642711  1.69530522  1.86844027\n",
      " 16.16513758 19.91476185]\n",
      "epoch 310, losses: [ 0.29500822  0.20800655  0.25338919  0.23020548  1.13352776  1.46850682\n",
      " 18.4735072  16.01804075]\n",
      "epoch 311, losses: [ 0.29101706  0.21527881  0.25610056  0.22966253  1.06621152  1.15812259\n",
      " 15.97976361 15.74128514]\n",
      "epoch 312, losses: [ 0.28829949  0.2106912   0.25710538  0.22819338  1.05328644  1.35940235\n",
      " 17.81542401 15.70845741]\n",
      "epoch 313, losses: [ 0.28916903  0.21162959  0.25654315  0.22859961  1.11337241  1.17041624\n",
      " 15.80780892 16.08557382]\n",
      "epoch 314, losses: [ 0.28684529  0.21498615  0.2532159   0.2296694   1.17506882  1.05465424\n",
      " 15.21477342 15.71954932]\n",
      "epoch 315, losses: [ 0.29613062  0.20964955  0.25546421  0.22942668  1.17306264  1.39670047\n",
      " 18.28688796 15.88376986]\n",
      "epoch 316, losses: [ 0.29649807  0.21299207  0.25428725  0.23038945  1.14222759  0.96339206\n",
      " 13.80117587 15.77600502]\n",
      "epoch 317, losses: [ 0.29497837  0.20929857  0.26062548  0.22679778  1.32611338  1.15964611\n",
      " 16.75817882 17.43644674]\n",
      "epoch 318, losses: [ 0.29532572  0.21506862  0.26319427  0.22555701  1.24775854  0.96367144\n",
      " 13.90016473 17.23342146]\n",
      "epoch 319, losses: [ 0.29379034  0.21296609  0.25571032  0.22977281  1.07234555  1.0907954\n",
      " 15.70590226 15.82819212]\n",
      "epoch 320, losses: [ 0.28876667  0.2093987   0.25461184  0.23011534  1.04827446  1.22964041\n",
      " 18.06692947 15.68761099]\n",
      "epoch 321, losses: [ 0.28474495  0.2141092   0.25028372  0.23128825  1.05218341  1.08925291\n",
      " 15.98803119 15.88042406]\n",
      "epoch 322, losses: [ 0.28911733  0.2164263   0.25784224  0.22851906  1.09678066  1.18200805\n",
      " 14.85888316 16.08996383]\n",
      "epoch 323, losses: [ 0.29282945  0.20748649  0.26691336  0.22427541  1.23128979  1.44818824\n",
      " 18.19011541 16.70385773]\n",
      "epoch 324, losses: [ 0.2943654   0.21134036  0.26246086  0.22591631  1.06529663  1.1487931\n",
      " 16.51685468 15.86503009]\n",
      "epoch 325, losses: [ 0.29706272  0.20835903  0.26370053  0.22513413  1.03473055  1.17915787\n",
      " 17.17485113 15.41501821]\n",
      "epoch 326, losses: [ 0.29193061  0.21120998  0.26212696  0.22660225  1.0265833   1.12476983\n",
      " 16.50547246 15.37398534]\n",
      "epoch 327, losses: [ 0.2907957   0.21099928  0.2542998   0.22893671  1.0251007   1.10136734\n",
      " 16.15857066 15.35851112]\n",
      "epoch 328, losses: [ 0.28835738  0.20986504  0.25045525  0.23076361  1.02400139  1.13577093\n",
      " 16.68339803 15.34729563]\n",
      "epoch 329, losses: [ 0.2852673   0.21669763  0.25092428  0.23032584  1.02285494  0.97415284\n",
      " 14.28130924 15.33523161]\n",
      "epoch 330, losses: [ 0.283176    0.21583404  0.24532419  0.23429888  1.02202456  1.04806819\n",
      " 15.39117971 15.32451964]\n",
      "epoch 331, losses: [ 0.28203309  0.21362177  0.24504485  0.23417789  1.02101521  1.11014577\n",
      " 16.32935569 15.31466432]\n",
      "epoch 332, losses: [ 0.287749    0.21348817  0.25740302  0.22929738  1.74943978  1.31515144\n",
      " 17.11712042 18.63073623]\n",
      "epoch 333, losses: [ 0.30325346  0.20924282  0.26809032  0.22383739  1.56592556  1.74802897\n",
      " 18.4468267  17.66363714]\n",
      "epoch 334, losses: [ 0.29787232  0.2112394   0.27141511  0.22240728  1.09963103  1.09877865\n",
      " 15.49695312 15.79666577]\n",
      "epoch 335, losses: [ 0.29568851  0.20791321  0.27406244  0.22160487  1.10599438  1.38326915\n",
      " 17.26632642 16.19646993]\n",
      "epoch 336, losses: [ 0.29555809  0.20996958  0.26549481  0.22443262  1.16000479  1.16309884\n",
      " 16.6074854  16.17111029]\n",
      "epoch 337, losses: [ 0.29835049  0.21032998  0.26629449  0.22361172  1.19281592  1.06951398\n",
      " 15.57118746 15.7378176 ]\n",
      "epoch 338, losses: [ 0.29645066  0.21010138  0.26476882  0.22523443  1.04090603  1.13277619\n",
      " 16.50607582 15.50040225]\n",
      "epoch 339, losses: [ 0.2935277   0.20943555  0.26111034  0.22623268  1.05453978  1.20697534\n",
      " 17.29299807 15.41843462]\n",
      "epoch 340, losses: [ 0.29281691  0.21102702  0.25972793  0.22670668  1.02553552  1.0655387\n",
      " 15.6378447  15.34626608]\n",
      "epoch 341, losses: [ 0.28910963  0.21270131  0.25664157  0.2285637   1.01583824  1.08310385\n",
      " 15.89598208 15.32665727]\n",
      "epoch 342, losses: [ 0.29082106  0.21360346  0.2578944   0.22728655  1.13527073  1.21522454\n",
      " 16.48115343 15.44316037]\n",
      "epoch 343, losses: [ 0.30283435  0.21017664  0.26013625  0.22718682  1.24292462  1.4786705\n",
      " 18.13979739 15.86913635]\n",
      "epoch 344, losses: [ 0.2977682   0.20644725  0.26447818  0.22442318  1.10711331  1.4973573\n",
      " 18.60441162 15.90020666]\n",
      "epoch 345, losses: [ 0.30355328  0.20956334  0.2679645   0.224251    1.15650658  1.21465529\n",
      " 15.44889512 16.77551959]\n",
      "epoch 346, losses: [ 0.30662327  0.20846816  0.26509366  0.224925    1.1953894   1.16739596\n",
      " 15.60525205 16.42554307]\n",
      "epoch 347, losses: [ 0.30520608  0.20575958  0.26246211  0.22491182  1.08157059  1.11716612\n",
      " 16.14708882 15.7970781 ]\n",
      "epoch 348, losses: [ 0.30304518  0.20794892  0.26562314  0.22503198  1.02800685  1.04877086\n",
      " 15.22525175 15.35737269]\n",
      "epoch 349, losses: [ 0.2985887   0.21060829  0.26123867  0.22649908  1.01846414  1.11440425\n",
      " 16.3411765  15.25907104]\n",
      "epoch 350, losses: [ 0.29769693  0.21011338  0.25381115  0.22965826  1.01824708  1.16711363\n",
      " 17.07538283 15.26373516]\n",
      "epoch 351, losses: [ 0.29333714  0.20690024  0.25209494  0.23044343  1.01582346  1.17956915\n",
      " 17.33455156 15.23245403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 352, losses: [ 0.28962337  0.21355586  0.25200208  0.23017495  1.01445469  1.14403975\n",
      " 16.80871198 15.21760862]\n",
      "epoch 353, losses: [ 0.28695849  0.21286163  0.25221937  0.23116868  1.01373837  1.04560987\n",
      " 15.33576002 15.2080267 ]\n",
      "epoch 354, losses: [ 0.28494653  0.21498707  0.24923121  0.23144425  1.01261629  1.08641003\n",
      " 15.95969647 15.19800974]\n",
      "epoch 355, losses: [ 0.28387421  0.21584094  0.24404674  0.23375535  1.01165475  1.12255532\n",
      " 16.49689396 15.18727743]\n",
      "epoch 356, losses: [ 0.28260402  0.21731543  0.24141617  0.23494647  1.01075902  0.99456203\n",
      " 14.5850484  15.17718817]\n",
      "epoch 357, losses: [ 0.28132156  0.21472409  0.24358493  0.23473892  1.00976817  1.11984802\n",
      " 16.46466607 15.16545887]\n",
      "epoch 358, losses: [ 0.28041356  0.2158265   0.24110981  0.23576057  1.00893767  1.09655964\n",
      " 16.12294981 15.15567584]\n",
      "epoch 359, losses: [ 0.2891098   0.21312936  0.24952856  0.23311798  1.4209702   1.5470299\n",
      " 18.63197471 18.90117993]\n",
      "epoch 360, losses: [ 0.30050053  0.20719403  0.26207042  0.22610935  1.27867553  1.35747611\n",
      " 18.24592132 16.36168118]\n",
      "epoch 361, losses: [ 0.29803637  0.21218431  0.26558249  0.22452273  1.02045063  1.0745945\n",
      " 15.49928003 15.24958267]\n",
      "epoch 362, losses: [ 0.29624891  0.20951122  0.26026239  0.22789372  1.03047272  1.30948116\n",
      " 16.06994215 15.51789079]\n",
      "epoch 363, losses: [ 0.29583035  0.2104652   0.25851589  0.22688469  1.2061994   1.11963707\n",
      " 16.26194802 16.48090805]\n",
      "epoch 364, losses: [ 0.30068229  0.2113998   0.26673424  0.22508379  1.17737429  1.16343617\n",
      " 16.11063936 15.55304948]\n",
      "epoch 365, losses: [ 0.30340618  0.20523512  0.26587632  0.22372806  1.05523331  1.42082454\n",
      " 17.30522353 15.62695296]\n",
      "epoch 366, losses: [ 0.2956802   0.21375873  0.26234502  0.2269317   1.01806556  1.02598356\n",
      " 14.69324338 15.28961777]\n",
      "epoch 367, losses: [ 0.2937886   0.20885223  0.26091997  0.22564912  1.01709334  1.14867741\n",
      " 16.689751   15.25605303]\n",
      "epoch 368, losses: [ 0.29191145  0.2071932   0.25526223  0.2283711   1.01149968  1.18383179\n",
      " 17.38829667 15.18573601]\n",
      "epoch 369, losses: [ 0.2893523   0.21168452  0.25002181  0.23160105  1.00995217  1.15353279\n",
      " 16.94535536 15.16601104]\n",
      "epoch 370, losses: [ 0.28767045  0.21363835  0.24822868  0.23197206  1.00899154  1.12437008\n",
      " 16.52293622 15.15359063]\n",
      "epoch 371, losses: [ 0.28878628  0.21324396  0.24401751  0.2337722   1.01030254  1.13616659\n",
      " 16.24179788 15.14953715]\n",
      "epoch 372, losses: [ 0.30572933  0.20619846  0.26679039  0.22570928  1.43769064  1.84391223\n",
      " 18.86665823 16.93887839]\n",
      "epoch 373, losses: [ 0.30808859  0.20624577  0.27485268  0.22078316  1.43109587  1.23464743\n",
      " 15.9159658  17.21017413]\n",
      "epoch 374, losses: [ 0.30275928  0.20706364  0.26678094  0.22381665  1.09705057  1.23700275\n",
      " 16.62921721 15.3846385 ]\n",
      "epoch 375, losses: [ 0.29998955  0.21003104  0.26855314  0.22371374  1.02349538  1.15022093\n",
      " 16.27450068 15.27603915]\n",
      "epoch 376, losses: [ 0.29987121  0.20622298  0.26606441  0.22391095  1.01790297  1.22573196\n",
      " 17.50469654 15.25294187]\n",
      "epoch 377, losses: [ 0.30618965  0.20845921  0.26498243  0.22479314  1.03453144  1.1479724\n",
      " 16.25525105 15.23020222]\n",
      "epoch 378, losses: [ 0.30191217  0.20393364  0.26350695  0.22543538  1.06287892  1.42517616\n",
      " 18.43034087 15.50541572]\n",
      "epoch 379, losses: [ 0.30191068  0.21087963  0.26340696  0.2254488   1.0189065   1.06931239\n",
      " 15.3983186  15.27573037]\n",
      "epoch 380, losses: [ 0.31151585  0.20368647  0.26746139  0.22363662  1.03154089  1.29514363\n",
      " 17.54498187 15.4650962 ]\n",
      "epoch 381, losses: [ 0.30370351  0.2062926   0.26959623  0.22342464  1.15331682  1.24270052\n",
      " 15.8775835  16.4608736 ]\n",
      "epoch 382, losses: [ 0.30120439  0.2101788   0.26835514  0.22258342  1.17058535  1.17796918\n",
      " 16.38734157 15.70980963]\n",
      "epoch 383, losses: [ 0.30324117  0.2073993   0.26171313  0.22576762  1.11513771  1.09561758\n",
      " 15.59279776 15.6628918 ]\n",
      "epoch 384, losses: [ 0.30306657  0.20342708  0.26491061  0.22489664  1.30190192  1.25338421\n",
      " 18.28028938 16.18607907]\n",
      "epoch 385, losses: [ 0.30193811  0.20741779  0.27105389  0.22247283  1.11162068  1.15776926\n",
      " 16.91447899 15.10935343]\n",
      "epoch 386, losses: [ 0.30209866  0.20550236  0.26266894  0.22477792  1.01541905  1.12983811\n",
      " 16.58391476 14.99754837]\n",
      "epoch 387, losses: [ 0.29905606  0.20423248  0.2555973   0.2281771   1.01206681  1.19589776\n",
      " 17.59047666 14.96549341]\n",
      "epoch 388, losses: [ 0.29593942  0.20921698  0.25164878  0.22972812  1.01118786  1.12489052\n",
      " 16.52516309 14.95391093]\n",
      "epoch 389, losses: [ 0.29269339  0.21584113  0.25336051  0.22993711  1.00954726  0.95202951\n",
      " 13.94345258 14.94396181]\n",
      "epoch 390, losses: [ 0.29066948  0.21480098  0.25197164  0.23035039  1.00861886  0.9839955\n",
      " 14.427423   14.93453182]\n",
      "epoch 391, losses: [ 0.28926368  0.21583412  0.25026801  0.23116922  1.00765451  1.02598802\n",
      " 15.05890999 14.92473767]\n",
      "epoch 392, losses: [ 0.28746519  0.21507631  0.24964688  0.2315706   1.00688228  0.94894376\n",
      " 13.90627329 14.91502422]\n",
      "epoch 393, losses: [ 0.28606319  0.21666773  0.24785878  0.23171191  1.00608982  1.08689273\n",
      " 15.9825694  14.9067407 ]\n",
      "epoch 394, losses: [ 0.28624795  0.20746823  0.2497816   0.23092068  1.00522621  1.24501194\n",
      " 18.36232719 14.8928943 ]\n",
      "epoch 395, losses: [ 0.2860991   0.21592039  0.24793041  0.23269281  1.00449946  1.05947573\n",
      " 15.45965523 14.88738822]\n",
      "epoch 396, losses: [ 0.32626986  0.20263126  0.26716028  0.22518861  1.80889263  1.75962314\n",
      " 19.19969791 20.18747206]\n",
      "epoch 397, losses: [ 0.30969891  0.20321843  0.2700357   0.22299752  1.25115596  1.17452236\n",
      " 16.30329105 16.21993741]\n",
      "epoch 398, losses: [ 0.30923691  0.20293445  0.26827494  0.2229408   1.09601226  1.22530551\n",
      " 17.53955581 15.22534334]\n",
      "epoch 399, losses: [ 0.30598926  0.20955081  0.26896507  0.22342252  1.01005296  1.05469555\n",
      " 15.26682236 15.01304652]\n",
      "epoch 400, losses: [ 0.3025136   0.20603125  0.26510814  0.22517762  1.00464743  1.23087736\n",
      " 18.10549802 14.95141918]\n",
      "epoch 401, losses: [ 0.29937374  0.21056779  0.26438155  0.22475437  1.00296625  1.0129347\n",
      " 14.84778684 14.93301324]\n",
      "epoch 402, losses: [ 0.29995937  0.21028324  0.25286492  0.22904116  1.00245159  1.09497144\n",
      " 16.08913115 14.92114136]\n",
      "epoch 403, losses: [ 0.29787873  0.2090438   0.25990287  0.22762778  1.02937835  1.23539112\n",
      " 14.74276189 15.18854405]\n",
      "epoch 404, losses: [ 0.30848608  0.21202287  0.2718819   0.22225734  1.31949111  1.27427872\n",
      " 14.79207514 16.60935717]\n",
      "epoch 405, losses: [ 0.31583159  0.20481397  0.27459304  0.22210293  1.51235082  1.22624407\n",
      " 17.01338284 16.56135745]\n",
      "epoch 406, losses: [ 0.30864849  0.2037366   0.26538228  0.22381325  1.22841666  1.2673978\n",
      " 17.82028226 15.31094829]\n",
      "epoch 407, losses: [ 0.30710474  0.20934626  0.26048198  0.22713413  1.06633931  0.9724312\n",
      " 14.14588247 14.9880536 ]\n",
      "epoch 408, losses: [ 0.30340227  0.21042462  0.26328208  0.2250668   1.01080701  1.0721276\n",
      " 15.51870256 14.94322486]\n",
      "epoch 409, losses: [ 0.29935422  0.20879622  0.25948685  0.22773277  1.00262834  1.17952543\n",
      " 17.25682667 14.93129056]\n",
      "epoch 410, losses: [ 0.2960642   0.21238313  0.25387574  0.22897819  1.00144291  0.99296592\n",
      " 14.54797333 14.91988293]\n",
      "epoch 411, losses: [ 0.29457103  0.2159028   0.25066307  0.23150005  1.00045077  0.98269525\n",
      " 14.413125   14.90224499]\n",
      "epoch 412, losses: [ 0.29912683  0.21301452  0.26871089  0.22458871  1.17787111  1.13267826\n",
      " 15.63452929 15.97957703]\n",
      "epoch 413, losses: [ 0.30924496  0.20324997  0.27175067  0.22148646  1.18059088  1.3276347\n",
      " 18.59468588 15.43894868]\n",
      "epoch 414, losses: [ 0.31804625  0.20759628  0.27294348  0.22185006  1.06510252  1.12888358\n",
      " 15.00784375 15.36681559]\n",
      "epoch 415, losses: [ 0.31483395  0.2040438   0.26682632  0.22385354  1.02801616  1.18263424\n",
      " 15.00902046 15.21178862]\n",
      "epoch 416, losses: [ 0.31267142  0.20473481  0.2667454   0.22401551  1.00417499  1.06732924\n",
      " 15.61353447 14.95566292]\n",
      "epoch 417, losses: [ 0.30823344  0.20615017  0.26168648  0.22523476  1.00152407  1.06451554\n",
      " 15.61917825 14.91825887]\n",
      "epoch 418, losses: [ 0.30436041  0.21217621  0.26061643  0.2275981   1.00022599  0.87953538\n",
      " 12.84990448 14.89529636]\n",
      "epoch 419, losses: [ 0.30138353  0.2057714   0.25874861  0.22748164  0.99942537  1.08176299\n",
      " 15.89652328 14.78888997]\n",
      "epoch 420, losses: [ 0.29922079  0.20963225  0.25359305  0.22909735  0.99844091  1.0641799\n",
      " 15.63805185 14.77801145]\n",
      "epoch 421, losses: [ 0.29661393  0.21180349  0.25129933  0.23132156  0.99735604  1.09949824\n",
      " 16.17091332 14.76549217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 422, losses: [ 0.29508806  0.21219754  0.25208345  0.23085994  0.99668121  1.1000583\n",
      " 16.18606628 14.75673636]\n",
      "epoch 423, losses: [ 0.29414732  0.2133365   0.25317299  0.23000059  0.99677799  1.07912106\n",
      " 15.42629611 14.76292098]\n",
      "epoch 424, losses: [ 0.29326602  0.21071523  0.25817318  0.22905714  1.12955828  1.08171133\n",
      " 15.91563591 14.88019859]\n",
      "epoch 425, losses: [ 0.31956808  0.19925644  0.27741434  0.22021041  1.73419455  2.03671081\n",
      " 20.7119244  18.53261441]\n",
      "epoch 426, losses: [ 0.31594271  0.20349603  0.27323578  0.22172466  1.13889032  1.36384068\n",
      " 18.03323132 15.40213484]\n",
      "epoch 427, losses: [ 0.30623677  0.20715378  0.27490559  0.22065003  1.04366899  1.1813901\n",
      " 16.2804866  15.4859958 ]\n",
      "epoch 428, losses: [ 0.30558749  0.20436846  0.27440505  0.221288    1.00734524  1.19589985\n",
      " 17.16224415 14.89341111]\n",
      "epoch 429, losses: [ 0.30411344  0.21027053  0.27492145  0.22018957  1.0024874   0.97919883\n",
      " 14.21417713 14.83688836]\n",
      "epoch 430, losses: [ 0.30111593  0.20874751  0.27016279  0.22198624  1.00033578  1.10267538\n",
      " 16.17593112 14.79750501]\n",
      "epoch 431, losses: [ 0.2997598   0.20568742  0.26373335  0.2251763   0.99898157  1.17798719\n",
      " 17.21592872 14.78415838]\n",
      "epoch 432, losses: [ 0.29873027  0.20976597  0.26588786  0.22420334  1.00622993  1.12095402\n",
      " 15.32736763 14.9111343 ]\n",
      "epoch 433, losses: [ 0.30505286  0.21014903  0.26808888  0.22355814  1.25180467  1.33440869\n",
      " 16.83234451 15.42339887]\n",
      "epoch 434, losses: [ 0.30836035  0.20639761  0.28216666  0.21868269  1.28945224  1.24675367\n",
      " 16.42360802 15.57741036]\n",
      "epoch 435, losses: [ 0.31246378  0.20350311  0.27445976  0.22140119  1.10030839  1.40236975\n",
      " 18.01116839 15.01253348]\n",
      "epoch 436, losses: [ 0.30822179  0.20553497  0.27455844  0.22039972  1.07002338  1.11866858\n",
      " 16.01651771 15.16641983]\n",
      "epoch 437, losses: [ 0.30637617  0.21057913  0.27660867  0.22123533  1.01055905  0.97856985\n",
      " 14.33337491 14.82149376]\n",
      "epoch 438, losses: [ 0.30378262  0.21173805  0.26611115  0.22357907  0.99994971  0.95581441\n",
      " 13.98516146 14.78682417]\n",
      "epoch 439, losses: [ 0.30176047  0.21083719  0.26257776  0.22611088  0.99852706  1.03719751\n",
      " 15.2351479  14.76914762]\n",
      "epoch 440, losses: [ 0.30019463  0.20599258  0.25757282  0.22779144  0.99750553  1.1881392\n",
      " 17.49171761 14.758583  ]\n",
      "epoch 441, losses: [ 0.29859377  0.21095161  0.25820711  0.22874154  0.99664331  1.10411062\n",
      " 16.23817261 14.74987427]\n",
      "epoch 442, losses: [ 0.29719965  0.21234553  0.25426318  0.22930703  0.99576174  1.06238551\n",
      " 15.61418466 14.74116773]\n",
      "epoch 443, losses: [ 0.29547668  0.20926763  0.25267923  0.22937487  0.99501583  1.13816249\n",
      " 16.76326287 14.73231831]\n",
      "epoch 444, losses: [ 0.29436423  0.20968264  0.25424778  0.23021078  0.99431707  1.16565953\n",
      " 17.1788944  14.72386905]\n",
      "epoch 445, losses: [ 0.29317261  0.21115233  0.25318695  0.22970544  0.99359974  1.11588511\n",
      " 16.43408619 14.71471957]\n",
      "epoch 446, losses: [ 0.2919885   0.21466475  0.25234039  0.23128461  0.99271028  1.03982628\n",
      " 15.29248483 14.70325823]\n",
      "epoch 447, losses: [ 0.29025379  0.21464236  0.25208882  0.23110756  0.99200095  1.05222947\n",
      " 15.48723209 14.69376539]\n",
      "epoch 448, losses: [ 0.2884197   0.21557644  0.24809858  0.23233317  0.99146286  1.07865119\n",
      " 15.87971148 14.6855165 ]\n",
      "epoch 449, losses: [ 0.2874192   0.21696545  0.25009535  0.23245508  0.9904369   1.02229147\n",
      " 15.039558   14.67251724]\n",
      "epoch 450, losses: [ 0.30970922  0.2066515   0.2746505   0.22273391  1.94690352  2.08760937\n",
      " 20.14347599 19.40949344]\n",
      "epoch 451, losses: [ 0.31106769  0.20823587  0.27828897  0.21996173  1.18091308  1.19700103\n",
      " 14.9910067  16.17246593]\n",
      "epoch 452, losses: [ 0.31134921  0.20703386  0.27790097  0.2203361   1.08490008  1.25554068\n",
      " 17.70377289 15.01485133]\n",
      "epoch 453, losses: [ 0.3070063   0.209251    0.28497216  0.21698306  1.03449713  1.19864948\n",
      " 17.14998117 15.1568469 ]\n",
      "epoch 454, losses: [ 0.30670938  0.20950731  0.27411547  0.22249006  1.00260922  1.06546365\n",
      " 15.2262961  14.82545832]\n",
      "epoch 455, losses: [ 0.30930814  0.20887309  0.27005113  0.22198976  0.99639339  1.00927987\n",
      " 14.80138669 14.73491897]\n",
      "epoch 456, losses: [ 0.30497368  0.2083091   0.26994894  0.22269339  0.99460395  1.00712492\n",
      " 14.76885552 14.7164858 ]\n",
      "epoch 457, losses: [ 0.30108509  0.21215602  0.26538609  0.2246471   0.99340932  0.97253327\n",
      " 14.25351891 14.7042156 ]\n",
      "epoch 458, losses: [ 0.29911505  0.20882825  0.26296424  0.22586378  0.99242518  1.06494473\n",
      " 15.65046436 14.69417595]\n",
      "epoch 459, losses: [ 0.29598752  0.2071411   0.2643444   0.2254478   0.99154747  1.16286541\n",
      " 17.13020255 14.68553451]\n",
      "epoch 460, losses: [ 0.2944634   0.21388994  0.26305241  0.22574915  0.99084844  1.05858281\n",
      " 15.56497676 14.6782926 ]\n",
      "epoch 461, losses: [ 0.29322001  0.21215063  0.25816216  0.22824293  0.99028772  1.04401294\n",
      " 15.35397814 14.67209658]\n",
      "epoch 462, losses: [ 0.29213218  0.21197867  0.26000187  0.22731988  0.98977439  1.11214331\n",
      " 16.37691315 14.66641011]\n",
      "epoch 463, losses: [ 0.29108435  0.21233642  0.26048892  0.22730064  0.98921517  1.07750812\n",
      " 15.85969019 14.65882544]\n",
      "epoch 464, losses: [ 0.29012239  0.21603009  0.25983024  0.22815964  0.98875994  1.07521155\n",
      " 15.82252439 14.65354372]\n",
      "epoch 465, losses: [ 0.28881212  0.21513434  0.25892917  0.2271911   0.98816274  0.94616273\n",
      " 13.89042338 14.64593285]\n",
      "epoch 466, losses: [ 0.28773729  0.21501025  0.2555487   0.22942431  0.98778575  1.11767002\n",
      " 16.47134057 14.63909946]\n",
      "epoch 467, losses: [ 0.2864831   0.21549162  0.24952874  0.23145234  0.9870301   1.06760807\n",
      " 15.72004092 14.63076532]\n",
      "epoch 468, losses: [ 0.29124592  0.21273058  0.25678147  0.23058056  1.18870303  1.51716994\n",
      " 15.99309036 16.69658417]\n",
      "epoch 469, losses: [ 0.32424334  0.19795819  0.27824537  0.21988469  1.61032178  2.03440705\n",
      " 19.55597042 18.05306903]\n",
      "epoch 470, losses: [ 0.31865337  0.20319476  0.27466813  0.22143413  1.11126554  1.1840865\n",
      " 16.94319055 15.00712969]\n",
      "epoch 471, losses: [ 0.30963965  0.21056718  0.26614088  0.22442415  1.07957271  0.94154405\n",
      " 13.72069079 15.02894871]\n",
      "epoch 472, losses: [ 0.3191392   0.19811435  0.27270011  0.22302497  1.02669968  1.25620836\n",
      " 17.70058208 14.97997928]\n",
      "epoch 473, losses: [ 0.31609806  0.20443922  0.2692793   0.22400237  1.12130282  1.11091047\n",
      " 15.8762632  14.7525103 ]\n",
      "epoch 474, losses: [ 0.31450248  0.20672306  0.26608061  0.22448446  0.99480796  1.04856435\n",
      " 15.35814787 14.69544985]\n",
      "epoch 475, losses: [ 0.31021483  0.20671878  0.26176615  0.22729014  0.99158831  1.05890471\n",
      " 15.55547463 14.66927677]\n",
      "epoch 476, losses: [ 0.3056916   0.20857769  0.26180833  0.22674115  0.98989497  1.07053598\n",
      " 15.73461049 14.65453637]\n",
      "epoch 477, losses: [ 0.30119514  0.21121117  0.26083375  0.22724679  0.98878803  1.03513398\n",
      " 15.20234359 14.64271372]\n",
      "epoch 478, losses: [ 0.29875573  0.21351224  0.25886797  0.2286187   0.98837552  0.99888571\n",
      " 14.66592122 14.63787825]\n",
      "epoch 479, losses: [ 0.29529945  0.21317588  0.25499478  0.22997305  0.98728028  1.00576556\n",
      " 14.78639169 14.62719527]\n",
      "epoch 480, losses: [ 0.29276146  0.21531558  0.25737705  0.22943191  0.98658345  0.94345869\n",
      " 13.84642855 14.61871298]\n",
      "epoch 481, losses: [ 0.29063623  0.21486839  0.25259474  0.23051075  0.98603936  1.01857509\n",
      " 14.97711732 14.6131149 ]\n",
      "epoch 482, losses: [ 0.31400366  0.20470167  0.27824791  0.22181205  1.67115684  1.87715231\n",
      " 19.49772474 17.38194641]\n",
      "epoch 483, losses: [ 0.31774188  0.20405975  0.27983753  0.2193809   1.28847083  1.29775395\n",
      " 17.26466652 15.97330217]\n",
      "epoch 484, losses: [ 0.31297313  0.20286438  0.2729195   0.22196446  1.12934658  1.18904342\n",
      " 16.06070995 15.27838493]\n",
      "epoch 485, losses: [ 0.3111425   0.19998393  0.26980912  0.22242685  1.04558142  1.1620207\n",
      " 16.8536168  15.50777325]\n",
      "epoch 486, losses: [ 0.31459203  0.20303702  0.26960246  0.2235891   0.99772257  1.20388742\n",
      " 17.48606787 14.94357882]\n",
      "epoch 487, losses: [ 0.31197025  0.20101933  0.26607518  0.22467631  1.00000297  1.18141891\n",
      " 16.20055614 14.96035989]\n",
      "epoch 488, losses: [ 0.31206766  0.20627635  0.28357794  0.21790871  1.00237395  1.20438981\n",
      " 15.26636247 14.90215083]\n",
      "epoch 489, losses: [ 0.30705903  0.20971575  0.27466512  0.22125203  0.99460822  1.05795364\n",
      " 15.11615655 14.72515278]\n",
      "epoch 490, losses: [ 0.30464827  0.20547939  0.26474389  0.22479388  0.98940427  1.09275389\n",
      " 16.04892347 14.65916637]\n",
      "epoch 491, losses: [ 0.30161545  0.21293854  0.26660974  0.22368575  0.9881959   0.93868598\n",
      " 13.6446747  14.64072139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 492, losses: [ 0.29980023  0.21047026  0.25982055  0.22655427  0.98739859  1.07553556\n",
      " 15.70253828 14.63312005]\n",
      "epoch 493, losses: [ 0.31842184  0.20804952  0.27754601  0.22159219  1.19195208  1.53233018\n",
      " 15.88250188 16.01862839]\n",
      "epoch 494, losses: [ 0.31740934  0.20549918  0.28622953  0.2171919   1.18228954  1.23391446\n",
      " 16.28153459 15.69696064]\n",
      "epoch 495, losses: [ 0.31412487  0.21090769  0.27638034  0.21927623  1.00710221  0.99809383\n",
      " 14.52395073 14.80050177]\n",
      "epoch 496, losses: [ 0.31615691  0.20320357  0.27529921  0.22072175  1.10149996  1.11227643\n",
      " 15.93857817 14.74823438]\n",
      "epoch 497, losses: [ 0.32021545  0.20120777  0.27731019  0.21975131  1.00417639  1.11944308\n",
      " 16.25121592 14.72869951]\n",
      "epoch 498, losses: [ 0.31334586  0.20095625  0.27326484  0.22126643  1.02831604  1.19752476\n",
      " 17.52481898 14.67087266]\n",
      "epoch 499, losses: [ 0.31032462  0.21005071  0.27156131  0.22248619  0.99674587  1.03518425\n",
      " 15.20127709 14.65082383]\n",
      "epoch 500, losses: [ 0.30611076  0.20654538  0.26806876  0.22312189  0.99123407  1.15370063\n",
      " 16.99513744 14.63984122]\n",
      "epoch 501, losses: [ 0.30372646  0.21022698  0.26003868  0.22747065  0.99495861  1.03413747\n",
      " 15.20254101 14.63024546]\n",
      "epoch 502, losses: [ 0.30179927  0.21151142  0.26048224  0.22660966  0.99225569  1.0004627\n",
      " 14.70630847 14.62320766]\n",
      "epoch 503, losses: [ 0.29980678  0.20757945  0.26067065  0.22706736  0.98707044  1.11527816\n",
      " 16.42477503 14.61503759]\n",
      "epoch 504, losses: [ 0.29768043  0.21387331  0.25925126  0.2285345   0.98605035  1.06695948\n",
      " 15.70479982 14.60884901]\n",
      "epoch 505, losses: [ 0.29533057  0.2138353   0.25766927  0.22773237  0.98535379  1.05867811\n",
      " 15.58090175 14.60043003]\n",
      "epoch 506, losses: [ 0.2935537   0.21857851  0.25239113  0.23014484  0.98469959  0.95984384\n",
      " 14.10577277 14.59408011]\n",
      "epoch 507, losses: [ 0.29525065  0.21566786  0.25349895  0.23084589  0.98436403  1.00993735\n",
      " 14.86118333 14.58877552]\n",
      "epoch 508, losses: [ 0.29582509  0.21290322  0.25557038  0.22913359  0.98444024  1.03797122\n",
      " 15.26542334 14.59158437]\n",
      "epoch 509, losses: [ 0.3193765   0.2039563   0.28117298  0.22018419  1.72711479  2.48536159\n",
      " 17.71617669 21.15621243]\n",
      "epoch 510, losses: [ 0.31842334  0.20671896  0.28083328  0.21882492  1.29507615  1.39255055\n",
      " 15.98393949 15.86618919]\n",
      "epoch 511, losses: [ 0.31975835  0.20545031  0.28125975  0.21870104  1.12616398  1.07914456\n",
      " 15.2576281  15.29368567]\n",
      "epoch 512, losses: [ 0.31410352  0.19916412  0.27297699  0.22209592  1.06915622  1.30493554\n",
      " 19.2450684  14.80432058]\n",
      "epoch 513, losses: [ 0.31090766  0.20073146  0.27288155  0.22164879  1.17008666  1.43670646\n",
      " 18.13484004 14.97287332]\n",
      "epoch 514, losses: [ 0.30948344  0.20610511  0.27218998  0.22269815  1.08289423  1.20853814\n",
      " 16.19259645 15.11402163]\n",
      "epoch 515, losses: [ 0.31042231  0.20738114  0.27517636  0.22016509  1.03399986  1.05708494\n",
      " 15.25171939 14.70834784]\n",
      "epoch 516, losses: [ 0.30534372  0.2105938   0.26944465  0.22365602  0.9906602   1.00032212\n",
      " 14.54135712 14.6585477 ]\n",
      "epoch 517, losses: [ 0.30163659  0.2081016   0.26713709  0.22355282  0.98824042  1.05559871\n",
      " 15.3941518  14.63378817]\n",
      "epoch 518, losses: [ 0.29946849  0.21308659  0.26627941  0.22479682  0.98714066  0.98526857\n",
      " 14.23150398 14.62086913]\n",
      "epoch 519, losses: [ 0.29763687  0.20826695  0.26796844  0.22480829  0.98617701  1.16389311\n",
      " 17.12978034 14.60925841]\n",
      "epoch 520, losses: [ 0.29649969  0.21133956  0.2601587   0.2261187   0.98534195  1.04557494\n",
      " 15.35913607 14.60040621]\n",
      "epoch 521, losses: [ 0.29612272  0.21419882  0.26382572  0.2259503   0.98467947  1.0469702\n",
      " 15.38433888 14.59323263]\n",
      "epoch 522, losses: [ 0.29531546  0.21707807  0.26145206  0.22624035  0.98410318  0.97518368\n",
      " 14.20625487 14.58881719]\n",
      "epoch 523, losses: [ 0.30125935  0.21331613  0.26613683  0.22587298  1.25786224  1.44256365\n",
      " 15.63578424 15.48108982]\n",
      "epoch 524, losses: [ 0.3173166   0.2068958   0.28332114  0.21767556  1.34355215  1.44785375\n",
      " 17.52027282 16.38862263]\n",
      "epoch 525, losses: [ 0.31790129  0.20055088  0.2827051   0.21878613  1.12532377  1.28786586\n",
      " 17.99691186 15.46533496]\n",
      "epoch 526, losses: [ 0.31883233  0.20305976  0.28515959  0.21647717  1.03724112  1.15201058\n",
      " 16.91739557 14.79378419]\n",
      "epoch 527, losses: [ 0.31927324  0.2087171   0.27696634  0.22038828  0.99155783  0.91843457\n",
      " 13.43848358 14.65073   ]\n",
      "epoch 528, losses: [ 0.31425266  0.20338672  0.2705531   0.22218658  0.98870882  1.11920904\n",
      " 16.45367587 14.62598788]\n",
      "epoch 529, losses: [ 0.31001745  0.20763875  0.26914124  0.22283254  0.98733813  1.03262356\n",
      " 15.17134955 14.6128454 ]\n",
      "epoch 530, losses: [ 0.30645094  0.21328205  0.268618    0.22336406  0.98611346  0.89321077\n",
      " 13.08066508 14.60261747]\n",
      "epoch 531, losses: [ 0.30393423  0.20743044  0.26065464  0.22657975  0.98542566  1.08666963\n",
      " 15.99441158 14.59639345]\n",
      "epoch 532, losses: [ 0.30209885  0.21107332  0.26217455  0.22616898  0.98467618  0.9819402\n",
      " 14.41616099 14.58805699]\n",
      "epoch 533, losses: [ 0.30042945  0.20798236  0.2594848   0.22685467  0.98397559  1.12759634\n",
      " 16.60901444 14.58193137]\n",
      "epoch 534, losses: [ 0.29865326  0.20746322  0.25614742  0.22880234  0.98352789  1.13380403\n",
      " 16.70258507 14.57816119]\n",
      "epoch 535, losses: [ 0.29741773  0.21233849  0.25652275  0.22889997  0.98294176  1.08421916\n",
      " 15.96338388 14.57044679]\n",
      "epoch 536, losses: [ 0.29604579  0.21376405  0.25426916  0.22953952  0.98229567  1.05583642\n",
      " 15.53356462 14.56386764]\n",
      "epoch 537, losses: [ 0.29484724  0.21229163  0.25839428  0.22850464  0.98199123  1.1111668\n",
      " 16.37060722 14.55991832]\n",
      "epoch 538, losses: [ 0.29381651  0.21128795  0.25351121  0.23007765  0.98143666  1.09666612\n",
      " 16.14744803 14.5532452 ]\n",
      "epoch 539, losses: [ 0.29246729  0.21522686  0.25288317  0.23072676  0.98072716  0.96594517\n",
      " 14.1912519  14.54616105]\n",
      "epoch 540, losses: [ 0.29476816  0.21636881  0.25364036  0.23059835  0.98129537  0.98611146\n",
      " 14.50030294 14.5403905 ]\n",
      "epoch 541, losses: [ 0.29222354  0.21161035  0.24964633  0.23241792  0.97975704  1.06083981\n",
      " 15.61969623 14.53317915]\n",
      "epoch 542, losses: [ 0.2945377   0.21777727  0.25310993  0.23154     0.97930329  1.00386732\n",
      " 14.76491516 14.52363259]\n",
      "epoch 543, losses: [ 0.29150878  0.21655652  0.25217574  0.23125736  0.97855608  0.98281221\n",
      " 14.44934325 14.51968046]\n",
      "epoch 544, losses: [ 0.30392899  0.21002894  0.260709    0.22939872  1.64302227  1.66536674\n",
      " 19.54354156 18.8239082 ]\n",
      "epoch 545, losses: [ 0.32935609  0.20101198  0.28558771  0.21750451  1.54029831  1.33572915\n",
      " 17.33138657 17.01062187]\n",
      "epoch 546, losses: [ 0.32685472  0.19631967  0.2773266   0.21954494  1.09450786  1.21133722\n",
      " 17.27076381 14.9709651 ]\n",
      "epoch 547, losses: [ 0.32096854  0.20604935  0.27370668  0.22212772  1.03139758  1.10292673\n",
      " 16.17986489 14.62799955]\n",
      "epoch 548, losses: [ 0.32197231  0.20191726  0.2720168   0.22200319  0.9866903   1.10268193\n",
      " 16.20109065 14.58619251]\n",
      "epoch 549, losses: [ 0.31751675  0.2047253   0.26822881  0.22338848  0.98368423  1.07322139\n",
      " 15.76613133 14.56598825]\n",
      "epoch 550, losses: [ 0.31160378  0.20826199  0.26432379  0.2266405   0.98192005  1.02853313\n",
      " 15.10174183 14.55090429]\n",
      "epoch 551, losses: [ 0.30702044  0.20875528  0.26462117  0.22600179  0.98087593  1.0834964\n",
      " 15.94002798 14.54096347]\n",
      "epoch 552, losses: [ 0.30401591  0.20680209  0.26075078  0.22675802  0.97998612  1.02734365\n",
      " 15.09716416 14.53191945]\n",
      "epoch 553, losses: [ 0.30220228  0.21216773  0.2633734   0.22702227  0.97921486  0.97108174\n",
      " 14.25371479 14.52448648]\n",
      "epoch 554, losses: [ 0.30438386  0.20961199  0.26898551  0.22433795  1.38126335  1.39574061\n",
      " 16.04524759 15.83637577]\n",
      "epoch 555, losses: [ 0.32798769  0.20428697  0.27892811  0.21994335  1.24753809  1.39645148\n",
      " 16.84197128 15.358409  ]\n",
      "epoch 556, losses: [ 0.31938473  0.20370364  0.28353803  0.2177479   1.08392901  1.25101645\n",
      " 17.52127166 15.69218562]\n",
      "epoch 557, losses: [ 0.32066974  0.20606595  0.28780077  0.21679276  1.190979    1.21892844\n",
      " 15.71947236 15.95460872]\n",
      "epoch 558, losses: [ 0.31390211  0.20877309  0.28219435  0.21805933  1.00929314  1.0517323\n",
      " 15.2917875  14.75978445]\n",
      "epoch 559, losses: [ 0.30934504  0.21013364  0.27326584  0.22124929  0.99316666  0.99340931\n",
      " 14.40980077 14.60420555]\n",
      "epoch 560, losses: [ 0.30619314  0.21463139  0.27713268  0.22008575  0.9905951   0.96032349\n",
      " 14.06208315 14.57651434]\n",
      "epoch 561, losses: [ 0.30369945  0.21033698  0.27247905  0.22134757  0.98906344  1.06329096\n",
      " 15.61684416 14.56075609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 562, losses: [ 0.30334474  0.20644011  0.26596879  0.22528915  0.98653371  1.111953\n",
      " 16.35712187 14.54989016]\n",
      "epoch 563, losses: [ 0.31917022  0.21104347  0.27725823  0.2203207   1.11767533  1.38837903\n",
      " 15.74734914 15.16943342]\n",
      "epoch 564, losses: [ 0.32139563  0.20209005  0.28509958  0.21692038  1.03709229  1.22490843\n",
      " 15.91622131 15.08784101]\n",
      "epoch 565, losses: [ 0.32143506  0.20378012  0.27706625  0.22042578  1.01505032  1.05769252\n",
      " 15.53765541 14.76464167]\n",
      "epoch 566, losses: [ 0.32160162  0.20561285  0.27900707  0.2191191   0.98751339  1.03545681\n",
      " 14.82527319 14.6150042 ]\n",
      "epoch 567, losses: [ 0.314832    0.20698669  0.2771049   0.21949069  0.99830964  1.12790885\n",
      " 15.20208138 14.81481283]\n",
      "epoch 568, losses: [ 0.31355288  0.2071615   0.27346377  0.22162437  0.98347042  1.0170304\n",
      " 14.90813889 14.5704466 ]\n",
      "epoch 569, losses: [ 0.31109379  0.20938733  0.27701696  0.21973593  1.43826157  1.20168201\n",
      " 14.77147298 16.18533876]\n",
      "epoch 570, losses: [ 0.31715925  0.20589291  0.28279182  0.2180475   1.29202437  1.0939712\n",
      " 15.40161803 16.03813557]\n",
      "epoch 571, losses: [ 0.32116074  0.20244341  0.28449473  0.21685963  1.0826162   1.16954805\n",
      " 17.15507972 14.67487239]\n",
      "epoch 572, losses: [ 0.31520749  0.20890094  0.27905816  0.21914321  0.99366657  0.98340213\n",
      " 14.41025755 14.59123135]\n",
      "epoch 573, losses: [ 0.31170697  0.20780615  0.27949223  0.21886226  0.99122875  1.0238526\n",
      " 15.02188669 14.57325885]\n",
      "epoch 574, losses: [ 0.30854231  0.20747899  0.27099428  0.22158381  0.98956319  1.08379616\n",
      " 15.92692953 14.56251485]\n",
      "epoch 575, losses: [ 0.30576675  0.21241335  0.27173732  0.22160549  1.01703216  1.05322072\n",
      " 15.25821094 14.59492032]\n",
      "epoch 576, losses: [ 0.32129466  0.20038126  0.28323563  0.21874048  1.03443059  1.37477781\n",
      " 17.70415617 15.25787373]\n",
      "epoch 577, losses: [ 0.3174763   0.21045889  0.28446232  0.21671631  1.2757621   1.03784957\n",
      " 14.42357712 15.07924953]\n",
      "epoch 578, losses: [ 0.32019167  0.20847066  0.28403738  0.21759801  1.17877948  1.37346689\n",
      " 14.80976547 15.11299979]\n",
      "epoch 579, losses: [ 0.31752698  0.20598651  0.2806748   0.21839034  1.02092211  1.15134686\n",
      " 15.66840711 14.80337115]\n",
      "epoch 580, losses: [ 0.31642066  0.20701887  0.27884034  0.21859317  0.98929266  1.01031305\n",
      " 14.60428365 14.65700473]\n",
      "epoch 581, losses: [ 0.3120356   0.21335638  0.27633655  0.22008421  0.98450589  0.914624\n",
      " 13.34615233 14.59221959]\n",
      "epoch 582, losses: [ 0.30864655  0.20803183  0.27269315  0.2210545   0.98293029  1.10137932\n",
      " 16.15572789 14.57302589]\n",
      "epoch 583, losses: [ 0.30591369  0.20972627  0.26905759  0.22250657  0.98176118  1.00513975\n",
      " 14.63977725 14.56000268]\n",
      "epoch 584, losses: [ 0.30416088  0.2093819   0.26695901  0.2247495   0.98079117  1.0824809\n",
      " 15.90178175 14.54701805]\n",
      "epoch 585, losses: [ 0.30299929  0.2115965   0.26688442  0.22375271  0.98008401  1.02650913\n",
      " 15.06547619 14.5377553 ]\n",
      "epoch 586, losses: [ 0.30194984  0.20431607  0.26624221  0.22406037  0.97932864  1.20675667\n",
      " 17.77491033 14.5284111 ]\n",
      "epoch 587, losses: [ 0.30111735  0.20814061  0.26433444  0.22506963  0.97881932  1.14945312\n",
      " 16.92543    14.52390705]\n",
      "epoch 588, losses: [ 0.30071124  0.20958339  0.26300444  0.22594406  0.97851807  1.04597393\n",
      " 15.36954753 14.52704316]\n",
      "epoch 589, losses: [ 0.32576863  0.20700672  0.28548084  0.21862592  1.69527245  1.60080753\n",
      " 15.91846995 17.76998358]\n",
      "epoch 590, losses: [ 0.33278637  0.19854508  0.28593496  0.21593825  1.30455226  1.13671157\n",
      " 16.1926707  14.97943075]\n",
      "epoch 591, losses: [ 0.32308415  0.19942336  0.28235266  0.21801182  1.02265438  1.16672688\n",
      " 16.50897481 14.65210943]\n",
      "epoch 592, losses: [ 0.32032248  0.20233576  0.28140271  0.21827244  0.98933612  1.11758867\n",
      " 16.41916006 14.57936174]\n",
      "epoch 593, losses: [ 0.31779086  0.20812344  0.27326263  0.22166553  0.98493129  1.03055397\n",
      " 15.13862907 14.55078452]\n",
      "epoch 594, losses: [ 0.31541648  0.20262603  0.27431801  0.22090542  0.98307275  1.11279202\n",
      " 16.37401077 14.5381778 ]\n",
      "epoch 595, losses: [ 0.31341038  0.20721521  0.26923803  0.22362002  0.98157307  1.04788667\n",
      " 15.41321944 14.52739862]\n",
      "epoch 596, losses: [ 0.31163127  0.2140137   0.26672035  0.22393595  0.98061915  0.93593602\n",
      " 13.72821365 14.52177159]\n",
      "epoch 597, losses: [ 0.30934381  0.21086455  0.26401054  0.22523996  0.97980348  1.04062655\n",
      " 15.3056558  14.51480079]\n",
      "epoch 598, losses: [ 0.30699987  0.20346152  0.26250302  0.22623348  0.97919919  1.15609277\n",
      " 17.03883927 14.50707223]\n",
      "epoch 599, losses: [ 0.30609181  0.21714186  0.26112591  0.22657354  0.97962762  0.8736916\n",
      " 12.80706653 14.50194594]\n",
      "epoch 600, losses: [ 0.3048841   0.21386998  0.26006159  0.22752736  0.97804079  0.94040905\n",
      " 13.81198564 14.49759962]\n",
      "epoch 601, losses: [ 0.3028051   0.20706173  0.25825309  0.22820479  0.97759657  1.06532781\n",
      " 15.69138833 14.49252825]\n",
      "epoch 602, losses: [ 0.3013174   0.21367329  0.25748463  0.22858667  0.97674506  1.04890159\n",
      " 15.44830371 14.48475865]\n",
      "epoch 603, losses: [ 0.30050936  0.21092162  0.25553696  0.22896058  0.9763167   1.09731852\n",
      " 16.17638219 14.48269546]\n",
      "epoch 604, losses: [ 0.2996851   0.21104333  0.2574333   0.22942464  0.97592245  1.06323865\n",
      " 15.662355   14.47868122]\n",
      "epoch 605, losses: [ 0.29880573  0.21284002  0.25617139  0.22946702  0.97524314  1.07654148\n",
      " 15.86998605 14.47101723]\n",
      "epoch 606, losses: [ 0.29780221  0.21576322  0.25525345  0.22969049  0.97474604  1.00661694\n",
      " 14.82176492 14.46202198]\n",
      "epoch 607, losses: [ 0.29658497  0.21410862  0.25256769  0.23175911  0.97398967  0.95658988\n",
      " 14.08202277 14.45377641]\n",
      "epoch 608, losses: [ 0.29601821  0.22089853  0.25402341  0.23051681  0.97364307  0.90720137\n",
      " 13.33758219 14.44782081]\n",
      "epoch 609, losses: [ 0.31160243  0.2105582   0.27912871  0.22156345  1.63210376  1.90230979\n",
      " 18.41979652 19.91790091]\n",
      "epoch 610, losses: [ 0.32838816  0.19577153  0.28736047  0.21676695  1.22488634  1.48415499\n",
      " 18.41097262 16.00334912]\n",
      "epoch 611, losses: [ 0.3239818   0.20269998  0.28446597  0.21728859  1.05235009  1.14105054\n",
      " 16.44011676 15.02195355]\n",
      "epoch 612, losses: [ 0.31898548  0.21079051  0.27930448  0.21913149  0.99797631  0.8924773\n",
      " 13.02413481 14.81479592]\n",
      "epoch 613, losses: [ 0.31457069  0.20475325  0.27561745  0.22120073  0.9954386   1.08385307\n",
      " 15.92740409 14.81652991]\n",
      "epoch 614, losses: [ 0.31211881  0.2027676   0.27406741  0.22157136  0.97960439  1.17768802\n",
      " 17.34349329 14.62264497]\n",
      "epoch 615, losses: [ 0.30882659  0.21194091  0.27216175  0.22203488  0.9775051   1.01667138\n",
      " 14.94754015 14.49241556]\n",
      "epoch 616, losses: [ 0.30515697  0.20815085  0.26911682  0.22337504  0.97649387  1.10536988\n",
      " 16.28789409 14.48110858]\n",
      "epoch 617, losses: [ 0.30275613  0.21319166  0.26352316  0.22577469  0.97550653  1.03506511\n",
      " 15.23239172 14.46978733]\n",
      "epoch 618, losses: [ 0.32791615  0.19929826  0.2741469   0.2229205   1.00759121  1.66431319\n",
      " 17.88953917 14.97690242]\n",
      "epoch 619, losses: [ 0.31733201  0.20532186  0.28129606  0.21862212  1.00505216  1.40720993\n",
      " 15.36937718 14.93966976]\n",
      "epoch 620, losses: [ 0.31926329  0.20600559  0.27576359  0.22029171  1.00987214  1.17781104\n",
      " 15.16947473 14.67076741]\n",
      "epoch 621, losses: [ 0.31147206  0.20787185  0.26830911  0.22345474  0.97948613  1.08850828\n",
      " 15.7668051  14.53607228]\n",
      "epoch 622, losses: [ 0.30821922  0.20853183  0.26963896  0.22400871  0.9762823   1.05730078\n",
      " 15.54830356 14.48381396]\n",
      "epoch 623, losses: [ 0.30600065  0.21261397  0.2661503   0.22419643  0.97527642  0.97344205\n",
      " 14.29235984 14.46818796]\n",
      "epoch 624, losses: [ 0.30280302  0.21261967  0.25779327  0.22784954  0.97455524  1.02045944\n",
      " 15.02302618 14.45754449]\n",
      "epoch 625, losses: [ 0.29983797  0.20786879  0.26194027  0.22718171  0.97388027  1.15227275\n",
      " 16.99969031 14.44887568]\n",
      "epoch 626, losses: [ 0.29803124  0.20941642  0.25987502  0.22812773  0.9732461   1.17084242\n",
      " 17.28438109 14.44034137]\n",
      "epoch 627, losses: [ 0.29732305  0.21519755  0.25884194  0.22807998  0.97267897  1.04633552\n",
      " 15.41574658 14.43257957]\n",
      "epoch 628, losses: [ 0.29594211  0.21952764  0.25407213  0.23051153  0.97226684  0.92789418\n",
      " 13.64645645 14.42851517]\n",
      "epoch 629, losses: [ 0.29482706  0.21573987  0.25459086  0.23055922  0.97169195  0.99054912\n",
      " 14.58892713 14.42113672]\n",
      "epoch 630, losses: [ 0.29385307  0.21358962  0.25509216  0.2309607   0.9718946   1.0873828\n",
      " 16.04606887 14.41980206]\n",
      "epoch 631, losses: [ 0.29301697  0.21635853  0.2538598   0.23106834  0.97098583  1.10691345\n",
      " 16.34453172 14.41133005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 632, losses: [ 0.29167203  0.21460994  0.25254105  0.23179089  0.97060114  1.04464997\n",
      " 15.40737333 14.40673918]\n",
      "epoch 633, losses: [ 0.29313562  0.21718355  0.25373951  0.23109459  0.97019796  1.02901929\n",
      " 15.18152564 14.40022713]\n",
      "epoch 634, losses: [ 0.29626607  0.2137683   0.25702227  0.23050614  1.15948432  1.08699575\n",
      " 15.69319851 15.66212442]\n",
      "epoch 635, losses: [ 0.33404965  0.19896805  0.28798453  0.21753735  1.9682452   1.78331978\n",
      " 19.18623701 17.61050687]\n",
      "epoch 636, losses: [ 0.33459424  0.2017902   0.28344167  0.21701919  1.07022406  1.13922781\n",
      " 15.71046509 14.96730754]\n",
      "epoch 637, losses: [ 0.3282184   0.19963071  0.28524674  0.21791128  1.00036808  1.14694296\n",
      " 16.73611932 14.56862948]\n",
      "epoch 638, losses: [ 0.32171384  0.20568784  0.27564765  0.22116499  0.98058712  1.02656568\n",
      " 15.09777076 14.47741936]\n",
      "epoch 639, losses: [ 0.3158369   0.21339533  0.27289031  0.22187259  0.97712659  0.91822526\n",
      " 13.48244966 14.45430707]\n",
      "epoch 640, losses: [ 0.3101931   0.2119081   0.27268535  0.22271955  0.97534195  1.01411023\n",
      " 14.92867334 14.44031388]\n",
      "epoch 641, losses: [ 0.30654448  0.21496194  0.26957346  0.22321873  0.9741172   0.85426537\n",
      " 12.5341843  14.43104468]\n",
      "epoch 642, losses: [ 0.3043317   0.21491471  0.26480137  0.22508102  0.97315335  1.03884737\n",
      " 15.30669876 14.42142391]\n",
      "epoch 643, losses: [ 0.30301146  0.21025361  0.26155292  0.22744755  0.97219579  1.05359911\n",
      " 15.53666792 14.41295685]\n",
      "epoch 644, losses: [ 0.30171861  0.21308458  0.26134656  0.22738806  0.97165537  0.96132688\n",
      " 14.157433   14.40825588]\n",
      "epoch 645, losses: [ 0.30046199  0.21586253  0.25986198  0.22805128  0.9711228   0.96581204\n",
      " 14.22333667 14.4017255 ]\n",
      "epoch 646, losses: [ 0.29881951  0.215051    0.25939761  0.2275236   0.97067813  1.01759105\n",
      " 15.00344383 14.3973641 ]\n",
      "epoch 647, losses: [ 0.29662264  0.21801219  0.25506213  0.23067858  0.97022247  1.00767261\n",
      " 14.85740121 14.39127659]\n",
      "epoch 648, losses: [ 0.29491023  0.21505749  0.25662667  0.22955209  0.96982258  0.98670634\n",
      " 14.54852708 14.38777286]\n",
      "epoch 649, losses: [ 0.2929649   0.21848738  0.2527178   0.23148927  0.9694946   1.00738418\n",
      " 14.85863332 14.38298392]\n",
      "epoch 650, losses: [ 0.29088877  0.21686776  0.25200773  0.23256906  0.96910324  1.02945205\n",
      " 15.19739091 14.37837308]\n",
      "epoch 651, losses: [ 0.28904933  0.22092352  0.25060378  0.23308781  0.96876301  0.90429278\n",
      " 13.32001304 14.37382397]\n",
      "epoch 652, losses: [ 0.28854446  0.21316953  0.24843074  0.23341575  0.96834234  1.1388825\n",
      " 16.84021886 14.36900946]\n",
      "epoch 653, losses: [ 0.28759441  0.22062865  0.2508582   0.23304185  0.96805128  0.96552994\n",
      " 14.24676521 14.36653251]\n",
      "epoch 654, losses: [ 0.28774316  0.21896133  0.2501291   0.23285613  0.96761789  1.054479\n",
      " 15.581489   14.36157382]\n",
      "epoch 655, losses: [ 0.28717254  0.2153968   0.24970498  0.2338215   0.96742501  1.08102083\n",
      " 15.98457984 14.35609004]\n",
      "epoch 656, losses: [ 0.30270144  0.21456409  0.2620818   0.23006132  1.53045365  1.73796672\n",
      " 17.74362487 17.74266775]\n",
      "epoch 657, losses: [ 0.32956039  0.20352902  0.28085268  0.21864497  1.68227915  1.40483285\n",
      " 16.35715804 17.1046048 ]\n",
      "epoch 658, losses: [ 0.32455141  0.20332799  0.28805034  0.21767434  1.09798368  1.17970084\n",
      " 15.78576383 15.37833963]\n",
      "epoch 659, losses: [ 0.32279967  0.20285779  0.28193615  0.21797106  1.02080502  1.04867086\n",
      " 14.91841809 14.83116711]\n",
      "epoch 660, losses: [ 0.31922227  0.20470086  0.28391447  0.21806496  1.01673773  1.11463452\n",
      " 16.37554678 14.78634185]\n",
      "epoch 661, losses: [ 0.3157957   0.20364259  0.28259792  0.21784869  0.99548339  1.12252677\n",
      " 16.52306417 14.60956924]\n",
      "epoch 662, losses: [ 0.31420334  0.20713214  0.27776     0.21995249  0.98094367  1.02711258\n",
      " 15.10311206 14.45433107]\n",
      "epoch 663, losses: [ 0.31066562  0.2056504   0.27366361  0.22218566  0.97311988  1.10224987\n",
      " 16.23576522 14.43075647]\n",
      "epoch 664, losses: [ 0.30759528  0.20788088  0.27151343  0.22239729  0.97196095  1.13503319\n",
      " 16.75131251 14.41810879]\n",
      "epoch 665, losses: [ 0.30545041  0.20831912  0.26849609  0.2242617   0.97090455  1.04897656\n",
      " 15.46012925 14.40394998]\n",
      "epoch 666, losses: [ 0.30136216  0.21225798  0.26847204  0.22336887  0.97020335  1.04303799\n",
      " 15.37518393 14.39597473]\n",
      "epoch 667, losses: [ 0.29832181  0.21309205  0.26644179  0.22567117  0.96927189  1.05651969\n",
      " 15.58026506 14.38557233]\n",
      "epoch 668, losses: [ 0.29549969  0.21314966  0.26430869  0.22483454  0.96863366  1.07636872\n",
      " 15.88454521 14.37766937]\n",
      "epoch 669, losses: [ 0.29331738  0.2145652   0.26248468  0.22637958  0.96809704  1.03334036\n",
      " 15.24486551 14.3705984 ]\n",
      "epoch 670, losses: [ 0.2914607   0.21759008  0.26027935  0.22848578  0.96756337  1.06109609\n",
      " 15.66366667 14.3640989 ]\n",
      "epoch 671, losses: [ 0.28935156  0.21747144  0.26296212  0.22737446  0.96717564  1.03343764\n",
      " 15.25246056 14.35828898]\n",
      "epoch 672, losses: [ 0.28793002  0.21485821  0.25769271  0.22862892  0.96693709  1.08125593\n",
      " 15.97453624 14.35530234]\n",
      "epoch 673, losses: [ 0.28639837  0.22375092  0.25692323  0.22987581  0.96633475  0.93461698\n",
      " 13.77283497 14.34696675]\n",
      "epoch 674, losses: [ 0.28458683  0.22214756  0.253093    0.23179022  0.96617263  1.0191458\n",
      " 15.04792067 14.34491341]\n",
      "epoch 675, losses: [ 0.28394001  0.21819083  0.25626623  0.2300524   0.96595636  1.09539545\n",
      " 16.19752944 14.34143231]\n",
      "epoch 676, losses: [ 0.30600808  0.21563116  0.27076704  0.2266534   1.27271319  1.73585929\n",
      " 17.77157068 16.50917512]\n",
      "epoch 677, losses: [ 0.33384191  0.20166264  0.29945797  0.2125697   1.60779402  1.25886072\n",
      " 16.61758535 17.86427879]\n",
      "epoch 678, losses: [ 0.32548922  0.20730849  0.2857905   0.21694217  1.05340497  1.04735493\n",
      " 14.83476903 14.83638545]\n",
      "epoch 679, losses: [ 0.3205443   0.20785683  0.28907685  0.21614368  0.99971657  1.11936136\n",
      " 14.34308429 14.80187984]\n",
      "epoch 680, losses: [ 0.31651576  0.20935377  0.28407406  0.21781123  0.98082907  1.01437485\n",
      " 14.79016303 14.53286855]\n",
      "epoch 681, losses: [ 0.31113563  0.2099746   0.2771486   0.21962558  0.97539604  1.01497878\n",
      " 14.90741272 14.45282877]\n",
      "epoch 682, losses: [ 0.30754189  0.20915514  0.27302778  0.2225886   0.97290841  1.05263425\n",
      " 15.49290513 14.42255176]\n",
      "epoch 683, losses: [ 0.30445     0.21352951  0.27376629  0.22159821  0.97123888  1.01659486\n",
      " 14.96290367 14.40257462]\n",
      "epoch 684, losses: [ 0.30221491  0.21743231  0.26994843  0.22359611  0.96987807  0.91742375\n",
      " 13.48740363 14.38650055]\n",
      "epoch 685, losses: [ 0.3001206   0.21586406  0.26767921  0.22429804  0.96892459  0.97978686\n",
      " 14.42419912 14.37530925]\n",
      "epoch 686, losses: [ 0.29748511  0.21688941  0.26948108  0.22397993  0.96824273  1.00565813\n",
      " 14.82103362 14.36623881]\n",
      "epoch 687, losses: [ 0.2948515   0.21657014  0.26746376  0.2244817   0.96755189  1.03956133\n",
      " 15.33427064 14.35837585]\n",
      "epoch 688, losses: [ 0.29272173  0.21651294  0.2623464   0.2267143   0.96700616  1.02543933\n",
      " 15.13411264 14.35021178]\n",
      "epoch 689, losses: [ 0.29075681  0.21553096  0.26084542  0.22729019  0.96684133  1.03241613\n",
      " 15.2326614  14.347049  ]\n",
      "epoch 690, losses: [ 0.28909315  0.21436849  0.26351997  0.22693239  0.9660815   1.15790464\n",
      " 17.12284643 14.34055302]\n",
      "epoch 691, losses: [ 0.28747985  0.22130239  0.25891995  0.22837702  0.96554928  0.98434335\n",
      " 14.51892089 14.3347173 ]\n",
      "epoch 692, losses: [ 0.28614163  0.21848609  0.25742264  0.22939578  0.96531718  1.05829898\n",
      " 15.63392973 14.33144881]\n",
      "epoch 693, losses: [ 0.28474988  0.22808158  0.25655784  0.22965131  0.96507203  0.87028772\n",
      " 12.8183018  14.3260807 ]\n",
      "epoch 694, losses: [ 0.28314774  0.222879    0.25712137  0.23006749  0.96485601  0.9463055\n",
      " 13.95873223 14.32174876]\n",
      "epoch 695, losses: [ 0.28261987  0.22174017  0.2537449   0.23108421  0.96453538  1.05872245\n",
      " 15.64705244 14.32010244]\n",
      "epoch 696, losses: [ 0.28216216  0.22489037  0.25509716  0.23158312  0.96423916  0.89873428\n",
      " 13.24856777 14.31488218]\n",
      "epoch 697, losses: [ 0.28217096  0.21954249  0.25707674  0.2298458   0.96403491  1.00352501\n",
      " 14.82075793 14.31228943]\n",
      "epoch 698, losses: [ 0.31162029  0.2139348   0.29110207  0.21773486  1.73567464  1.85431123\n",
      " 16.77143225 17.699206  ]\n",
      "epoch 699, losses: [ 0.32742038  0.20154426  0.29379214  0.2141098   1.21587482  1.29211785\n",
      " 16.65775839 15.29544669]\n",
      "epoch 700, losses: [ 0.3207356   0.1995313   0.29817622  0.21236757  1.02630376  1.40436479\n",
      " 16.9390954  14.87970449]\n",
      "epoch 701, losses: [ 0.31536808  0.2062468   0.2907718   0.21507429  0.97989598  1.08306738\n",
      " 15.81158208 14.52835995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 702, losses: [ 0.31278704  0.20797967  0.28254872  0.21833399  0.97684358  1.08907082\n",
      " 15.70062996 14.47534877]\n",
      "epoch 703, losses: [ 0.30807236  0.20841382  0.27945423  0.21912758  0.97079223  1.03667715\n",
      " 15.24740899 14.40047594]\n",
      "epoch 704, losses: [ 0.3037874   0.21159094  0.27453789  0.22161041  0.96882873  1.04890348\n",
      " 15.46199188 14.3754604 ]\n",
      "epoch 705, losses: [ 0.3003813   0.21523549  0.27051305  0.22265152  0.96750121  0.97070298\n",
      " 14.29302679 14.35752083]\n",
      "epoch 706, losses: [ 0.2967655   0.21426437  0.2659147   0.22477658  0.96643659  1.00370344\n",
      " 14.79174261 14.3436737 ]\n",
      "epoch 707, losses: [ 0.29347678  0.21605126  0.26431898  0.22581064  0.96548726  1.01697869\n",
      " 15.00296586 14.33149616]\n",
      "epoch 708, losses: [ 0.29068205  0.21699363  0.26114416  0.22684043  0.96482774  1.10608942\n",
      " 16.34346221 14.32320711]\n",
      "epoch 709, losses: [ 0.28779932  0.21635054  0.25928953  0.22830577  0.96430007  1.11244198\n",
      " 16.44251697 14.31673951]\n",
      "epoch 710, losses: [ 0.28570579  0.21920927  0.26057769  0.22842239  0.96387017  1.01826467\n",
      " 15.03249981 14.31114125]\n",
      "epoch 711, losses: [ 0.28415717  0.21437906  0.25887084  0.22885639  0.96348139  1.18200622\n",
      " 17.49606545 14.30537949]\n",
      "epoch 712, losses: [ 0.28154486  0.22014227  0.25855428  0.22871358  0.96323862  1.06447065\n",
      " 15.73370618 14.301819  ]\n",
      "epoch 713, losses: [ 0.27979687  0.21869292  0.25828327  0.22886944  0.96304608  1.04396011\n",
      " 15.4259903  14.29849537]\n",
      "epoch 714, losses: [ 0.27949826  0.22863221  0.25241198  0.23170707  0.9628063   0.79406974\n",
      " 11.67098807 14.29427498]\n",
      "epoch 715, losses: [ 0.27864231  0.22176831  0.25579102  0.2309442   0.96266684  0.99513399\n",
      " 14.69511109 14.29228485]\n",
      "epoch 716, losses: [ 0.27879533  0.22359484  0.25499038  0.2305569   0.9624433   1.00708672\n",
      " 14.87390085 14.28925896]\n",
      "epoch 717, losses: [ 0.29278847  0.21869268  0.26575327  0.22796004  1.56246337  1.18639793\n",
      " 16.00364532 16.75656627]\n",
      "epoch 718, losses: [ 0.32436608  0.20351089  0.28875903  0.21624299  1.51497169  1.56790572\n",
      " 17.17473408 16.14119393]\n",
      "epoch 719, losses: [ 0.33049143  0.20234026  0.2818236   0.21840389  1.08926584  1.07416466\n",
      " 15.33227819 14.62901473]\n",
      "epoch 720, losses: [ 0.31628294  0.20869239  0.28635249  0.21693196  1.06870956  1.0429829\n",
      " 15.10115062 14.95558754]\n",
      "epoch 721, losses: [ 0.3149552   0.206607    0.28930132  0.21522055  0.99364759  1.1486078\n",
      " 14.63029704 14.65993535]\n",
      "epoch 722, losses: [ 0.31031714  0.20850049  0.28045031  0.21846279  0.97957395  1.10780347\n",
      " 16.33963642 14.37349845]\n",
      "epoch 723, losses: [ 0.30678958  0.21240092  0.27656183  0.22060918  0.96919008  0.93646974\n",
      " 13.779372   14.3477561 ]\n",
      "epoch 724, losses: [ 0.3043457   0.20809881  0.27035529  0.22261489  0.96754483  1.09575992\n",
      " 16.17953908 14.33408718]\n",
      "epoch 725, losses: [ 0.30234426  0.21045114  0.27173586  0.22225691  0.9663362   1.02652999\n",
      " 15.14591175 14.32304112]\n",
      "epoch 726, losses: [ 0.30038001  0.21271815  0.26885721  0.22401755  0.96538923  1.01844941\n",
      " 15.02432257 14.31439721]\n",
      "epoch 727, losses: [ 0.29767226  0.21970879  0.26666396  0.22496106  0.96462853  0.92689314\n",
      " 13.6446434  14.30634442]\n",
      "epoch 728, losses: [ 0.29446428  0.21355413  0.26265959  0.22615422  0.96400646  1.10648728\n",
      " 16.34793966 14.29906661]\n",
      "epoch 729, losses: [ 0.29094249  0.21718662  0.26379695  0.2268523   0.96349903  1.04901615\n",
      " 15.48859792 14.29398706]\n",
      "epoch 730, losses: [ 0.28805649  0.21889051  0.26098391  0.22730456  0.96308369  0.98032952\n",
      " 14.46360389 14.28903675]\n",
      "epoch 731, losses: [ 0.28541957  0.22303648  0.25833274  0.22943258  0.96254784  0.97961079\n",
      " 14.45086357 14.28271921]\n",
      "epoch 732, losses: [ 0.28301521  0.22184783  0.25750243  0.22890821  0.96228502  1.00779748\n",
      " 14.87975742 14.28142637]\n",
      "epoch 733, losses: [ 0.28176635  0.2193721   0.25941015  0.22917252  0.96198147  1.02756632\n",
      " 15.17429069 14.27836825]\n",
      "epoch 734, losses: [ 0.28020438  0.22099322  0.25380084  0.23090505  0.96176066  1.06250749\n",
      " 15.70142046 14.27518507]\n",
      "epoch 735, losses: [ 0.27865674  0.21734432  0.256755    0.23011866  0.96145537  1.17425627\n",
      " 17.37857757 14.27083421]\n",
      "epoch 736, losses: [ 0.27771965  0.22438149  0.25595347  0.23061699  0.96143049  0.97804792\n",
      " 14.43399205 14.26942783]\n",
      "epoch 737, losses: [ 0.29724656  0.21267769  0.27149594  0.22576745  1.63896897  1.58457098\n",
      " 18.74421337 17.90961992]\n",
      "epoch 738, losses: [ 0.33186144  0.20143577  0.29075902  0.21573072  1.42893492  1.28027999\n",
      " 16.57081776 15.34181442]\n",
      "epoch 739, losses: [ 0.32288458  0.2023452   0.29560436  0.21385748  1.05122929  1.15911752\n",
      " 16.19422509 14.59412338]\n",
      "epoch 740, losses: [ 0.31419002  0.20682972  0.28171033  0.21855525  0.98271652  1.03911474\n",
      " 14.93115439 14.43630567]\n",
      "epoch 741, losses: [ 0.30978615  0.20972383  0.28041916  0.21845837  0.9722737   0.99078837\n",
      " 14.57035649 14.35081868]\n",
      "epoch 742, losses: [ 0.30643171  0.210361    0.27764035  0.22000768  0.96918242  1.06247754\n",
      " 15.66821543 14.32445187]\n",
      "epoch 743, losses: [ 0.3033956   0.20677213  0.27483453  0.22130458  0.9672416   1.08267668\n",
      " 15.9795285  14.30932612]\n",
      "epoch 744, losses: [ 0.30091371  0.20483994  0.26716669  0.22450612  0.96586359  1.15624059\n",
      " 17.08321658 14.29905055]\n",
      "epoch 745, losses: [ 0.29823517  0.20891977  0.26656563  0.22503164  0.96481929  1.16399448\n",
      " 17.20540037 14.29025378]\n",
      "epoch 746, losses: [ 0.29501849  0.21970571  0.26569895  0.22519147  0.96395555  0.92596176\n",
      " 13.63557105 14.28393353]\n",
      "epoch 747, losses: [ 0.29192744  0.21989552  0.26294908  0.22675254  0.96316289  0.95331936\n",
      " 14.05204131 14.27715087]\n",
      "epoch 748, losses: [ 0.28893226  0.21307214  0.26415432  0.2265331   0.96259266  1.12537595\n",
      " 16.63658192 14.27246907]\n",
      "epoch 749, losses: [ 0.28656645  0.22778984  0.26078497  0.22750617  0.9620439   0.87592802\n",
      " 12.89342826 14.26889535]\n",
      "epoch 750, losses: [ 0.28440868  0.21398734  0.26110652  0.22776394  0.96164249  1.13491148\n",
      " 16.77338918 14.26467004]\n",
      "epoch 751, losses: [ 0.28253918  0.2179521   0.25780711  0.229969    0.96128579  1.07758939\n",
      " 15.91749049 14.26172463]\n",
      "epoch 752, losses: [ 0.28051393  0.22008834  0.2573865   0.23008647  0.96115527  1.0347544\n",
      " 15.28016041 14.25984973]\n",
      "epoch 753, losses: [ 0.27861022  0.21792177  0.25848438  0.2293151   0.96091026  1.11780734\n",
      " 16.52867821 14.25662316]\n",
      "epoch 754, losses: [ 0.27832529  0.22226293  0.25540579  0.23073743  0.96071564  1.04806806\n",
      " 15.47783671 14.25426653]\n",
      "epoch 755, losses: [ 0.27712314  0.22605367  0.25579106  0.23040919  0.96050426  0.96422699\n",
      " 14.22406718 14.25260678]\n",
      "epoch 756, losses: [ 0.2753335   0.22693892  0.25420029  0.23173313  0.96034218  0.90993474\n",
      " 13.40827212 14.25065793]\n",
      "epoch 757, losses: [ 0.27691731  0.22203468  0.25699229  0.2298902   0.9603703   1.0347155\n",
      " 15.27701105 14.25144262]\n",
      "epoch 758, losses: [ 0.27596123  0.21690793  0.26398504  0.22814788  0.96070983  1.20010412\n",
      " 17.75548034 14.25637784]\n",
      "epoch 759, losses: [ 0.31613729  0.20885851  0.29210922  0.21804823  2.07368598  1.57981764\n",
      " 18.9506474  19.09296072]\n",
      "epoch 760, losses: [ 0.32271186  0.20472007  0.298677    0.21236156  1.39570627  1.35753791\n",
      " 16.00088338 15.52352034]\n",
      "epoch 761, losses: [ 0.32320873  0.20330614  0.29929185  0.2120695   1.06225131  1.22407927\n",
      " 16.43651454 14.70499225]\n",
      "epoch 762, losses: [ 0.31669777  0.20331756  0.29200816  0.2152742   0.98924413  1.14529421\n",
      " 16.84899924 14.42785619]\n",
      "epoch 763, losses: [ 0.31319101  0.21087622  0.28142179  0.21775321  0.97393593  0.96199364\n",
      " 14.14070776 14.35672656]\n",
      "epoch 764, losses: [ 0.30885986  0.2081084   0.27954519  0.21926127  0.97032249  1.01560905\n",
      " 14.96171128 14.32881173]\n",
      "epoch 765, losses: [ 0.3057188   0.21023593  0.27681444  0.22077601  0.96789642  1.01323522\n",
      " 14.92814925 14.30924941]\n",
      "epoch 766, losses: [ 0.30292372  0.20894009  0.27752575  0.2200338   0.96607141  1.03186131\n",
      " 15.20966078 14.29361215]\n",
      "epoch 767, losses: [ 0.30084685  0.21054313  0.27109171  0.22279711  0.96478842  1.05704032\n",
      " 15.59011219 14.28269985]\n",
      "epoch 768, losses: [ 0.29922119  0.2117256   0.27054411  0.22340344  0.96371049  1.0578554\n",
      " 15.60520021 14.27218832]\n",
      "epoch 769, losses: [ 0.29786161  0.20684006  0.26997906  0.22372687  0.96287916  1.19047327\n",
      " 17.59743351 14.26522505]\n",
      "epoch 770, losses: [ 0.29634435  0.21190603  0.26751694  0.22453655  0.9621637   1.0286307\n",
      " 15.16882497 14.25689481]\n",
      "epoch 771, losses: [ 0.29437369  0.20883572  0.26423155  0.22598728  0.96165283  1.10575338\n",
      " 16.32933672 14.25181552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 772, losses: [ 0.29215983  0.21655183  0.26190375  0.22701014  0.96121661  0.97154508\n",
      " 14.31653837 14.24771928]\n",
      "epoch 773, losses: [ 0.28982479  0.21604815  0.26030366  0.22861863  0.96079088  1.06082989\n",
      " 15.6589972  14.24218925]\n",
      "epoch 774, losses: [ 0.28779075  0.21869219  0.26132854  0.22853219  0.9603325   0.98725891\n",
      " 14.55528896 14.23853325]\n",
      "epoch 775, losses: [ 0.28556701  0.21670582  0.26135043  0.22725872  0.9599078   1.08850541\n",
      " 16.07675942 14.234308  ]\n",
      "epoch 776, losses: [ 0.28315402  0.21974061  0.26087117  0.22878051  0.95961128  1.06107641\n",
      " 15.66343309 14.23195854]\n",
      "epoch 777, losses: [ 0.2951478   0.21690824  0.26347432  0.2282123   1.12111006  1.41393506\n",
      " 16.73912119 15.5423497 ]\n",
      "epoch 778, losses: [ 0.33375784  0.19838237  0.30236611  0.21171376  1.53990924  1.61835006\n",
      " 18.19785184 17.4090548 ]\n",
      "epoch 779, losses: [ 0.32384083  0.20768799  0.29671829  0.21338607  1.09387357  1.16597978\n",
      " 14.31704751 15.22691961]\n",
      "epoch 780, losses: [ 0.32438782  0.20552169  0.29481897  0.21318925  1.00553235  1.04343814\n",
      " 14.88897272 14.53647462]\n",
      "epoch 781, losses: [ 0.31825623  0.20518606  0.28800011  0.21582521  0.97436398  1.0907315\n",
      " 15.97814417 14.40207517]\n",
      "epoch 782, losses: [ 0.31442641  0.20391422  0.28178915  0.21792175  0.96903436  1.0852031\n",
      " 15.93098431 14.35416453]\n",
      "epoch 783, losses: [ 0.3110694   0.2027084   0.2777933   0.21972036  0.96669495  1.12540515\n",
      " 16.54695346 14.3249721 ]\n",
      "epoch 784, losses: [ 0.30826959  0.20780659  0.27399507  0.22101097  0.96509502  1.04610984\n",
      " 15.37171281 14.30446435]\n",
      "epoch 785, losses: [ 0.30596139  0.20856135  0.2723223   0.22144353  0.96393352  1.03165192\n",
      " 15.16290074 14.28792762]\n",
      "epoch 786, losses: [ 0.30306276  0.20569285  0.27289052  0.22213204  1.11108034  1.06202022\n",
      " 15.68356695 14.27897714]\n",
      "epoch 787, losses: [ 0.30017225  0.20836968  0.28778831  0.21697904  1.62036987  1.01350415\n",
      "  8.107646   15.79877341]\n",
      "epoch 788, losses: [ 0.29562442  0.21332331  0.29320046  0.21339337  1.23279629  0.32592922\n",
      "  3.79601676 14.88050368]\n",
      "epoch 789, losses: [ 0.29521133  0.21205011  0.28517088  0.21617761  0.99961442  0.25545692\n",
      "  3.45959492 14.37489607]\n",
      "epoch 790, losses: [ 0.29666064  0.21470712  0.27928256  0.21934884  0.97987903  0.16815843\n",
      "  2.24495181 14.3227966 ]\n",
      "epoch 791, losses: [ 0.29313839  0.21978898  0.27769116  0.21965643  0.97801934  0.13118\n",
      "  1.68762252 14.30132431]\n",
      "epoch 792, losses: [ 0.28191119  0.21865457  0.27281012  0.22087831  0.97475964  0.11317978\n",
      "  1.43704125 14.28642633]\n",
      "epoch 793, losses: [ 0.27673133  0.22709716  0.27231162  0.22242451  1.41478367  0.18605718\n",
      "  2.01864231 14.3192163 ]\n",
      "epoch 794, losses: [ 0.27709715  0.23138583  0.29790546  0.21269664  1.43426769  0.85270475\n",
      "  2.78264344 16.2797017 ]\n",
      "epoch 795, losses: [ 0.26768408  0.23090195  0.2975789   0.21195208  1.23363848  0.40744841\n",
      "  1.77633823 15.11149188]\n",
      "epoch 796, losses: [ 0.27592674  0.22943587  0.29446734  0.21274933  0.99695803  0.29291209\n",
      "  3.1591707  14.54163977]\n",
      "epoch 797, losses: [ 0.26946357  0.23033761  0.29285735  0.21354716  1.00088874  0.19231931\n",
      "  1.61356613 14.47551683]\n",
      "epoch 798, losses: [ 0.26590251  0.23187916  0.29460473  0.21292233  0.99447656  0.23863117\n",
      "  1.17748069 14.62804316]\n",
      "epoch 799, losses: [ 0.25950832  0.23316029  0.28933445  0.21440396  0.98156253  0.13251643\n",
      "  1.36059285 14.44703744]\n",
      "epoch 800, losses: [ 0.2531772   0.24104375  0.28586147  0.2155658   0.97540227  0.10107129\n",
      "  1.14196878 14.35386048]\n",
      "epoch 801, losses: [ 0.2460923   0.24656069  0.28330346  0.21742262  0.97343265  0.09448758\n",
      "  1.09766627 14.32725918]\n",
      "epoch 802, losses: [ 0.23315161  0.25683354  0.27893161  0.21785151  0.97136388  0.1074115\n",
      "  1.31231858 14.30908112]\n",
      "epoch 803, losses: [ 0.22661331  0.26411291  0.27822561  0.21911733  0.97009462  0.12093919\n",
      "  1.5159619  14.29547064]\n",
      "epoch 804, losses: [ 0.21993202  0.27332079  0.2763025   0.21917992  0.96955783  0.10526673\n",
      "  1.25416173 14.29201067]\n",
      "epoch 805, losses: [ 0.21295573  0.26753164  0.27610686  0.22029106  0.96783643  0.13138461\n",
      "  1.68640745 14.27613181]\n",
      "epoch 806, losses: [ 0.20161637  0.27666328  0.2724287   0.22094674  0.96832944  0.13028333\n",
      "  1.68213074 14.26610052]\n",
      "epoch 807, losses: [ 0.19326877  0.28726274  0.26841109  0.22309546  0.96679403  0.10093179\n",
      "  1.24193312 14.26041671]\n",
      "epoch 808, losses: [ 0.23321395  0.26173374  0.30132055  0.21177091  1.53002161  1.49639444\n",
      "  4.0751041  18.02452712]\n",
      "epoch 809, losses: [ 0.24748692  0.24827701  0.30044195  0.21068683  1.08686564  0.37824134\n",
      "  1.51549128 14.92913857]\n",
      "epoch 810, losses: [ 0.25964684  0.24195363  0.29756089  0.21180716  1.17397106  0.26203516\n",
      "  2.87158043 14.53231079]\n",
      "epoch 811, losses: [ 0.24760261  0.24449899  0.29041789  0.21457585  0.98436834  0.14561901\n",
      "  1.3861634  14.4382922 ]\n",
      "epoch 812, losses: [ 0.24092702  0.25063222  0.28541078  0.21498323  0.98704043  0.20826458\n",
      "  1.10941278 14.57162302]\n",
      "epoch 813, losses: [ 0.232366    0.25670144  0.28341462  0.21678415  0.97515149  0.11117091\n",
      "  1.30172097 14.3824605 ]\n",
      "epoch 814, losses: [ 0.22567206  0.25736275  0.28367174  0.2164494   0.97264349  0.12692852\n",
      "  1.26672571 14.35594745]\n",
      "epoch 815, losses: [ 0.21531739  0.27376151  0.28325263  0.21608084  0.97791073  0.10700518\n",
      "  1.27504601 14.32563528]\n",
      "epoch 816, losses: [ 0.21189561  0.2681591   0.28036902  0.21761448  0.96932351  0.11831008\n",
      "  1.47517082 14.30453571]\n",
      "epoch 817, losses: [ 0.2070756   0.27255001  0.27832913  0.21854883  0.97359382  0.10581596\n",
      "  1.30346201 14.28546036]\n",
      "epoch 818, losses: [ 0.22078065  0.26698429  0.27582643  0.21982771  0.97389624  0.10603678\n",
      "  1.30668629 14.27970486]\n",
      "epoch 819, losses: [ 0.21683361  0.27316611  0.2733979   0.22108412  0.9667708   0.10210762\n",
      "  1.2603613  14.26472397]\n",
      "epoch 820, losses: [ 0.20076878  0.27884125  0.26976613  0.22269465  0.96631226  0.12836595\n",
      "  1.65150232 14.25694222]\n",
      "epoch 821, losses: [ 0.19864732  0.28436732  0.2703287   0.22243055  0.96621445  0.08636121\n",
      "  1.02760246 14.25196932]\n",
      "epoch 822, losses: [ 0.19218023  0.28673416  0.26702569  0.22324851  0.96520318  0.1124299\n",
      "  1.4176656  14.2478316 ]\n",
      "epoch 823, losses: [ 0.21237656  0.26599357  0.28815475  0.21749892  1.35142619  1.51757375\n",
      "  3.43325707 17.43316577]\n",
      "epoch 824, losses: [ 0.23214332  0.25936     0.30111957  0.21115916  1.10489514  0.59338396\n",
      "  1.91812625 15.49546649]\n",
      "epoch 825, losses: [ 0.24153382  0.2440899   0.29468753  0.21199625  0.99957651  0.30387596\n",
      "  1.98204629 14.79137789]\n",
      "epoch 826, losses: [ 0.22504418  0.25693024  0.29803675  0.21176475  0.98745369  0.19761794\n",
      "  1.29611117 14.61484629]\n",
      "epoch 827, losses: [ 0.21862809  0.26874306  0.29193361  0.21301105  0.97643477  0.14168156\n",
      "  1.47380363 14.46105039]\n",
      "epoch 828, losses: [ 0.21150665  0.27173644  0.28778073  0.21505252  0.97240339  0.12604412\n",
      "  1.48178056 14.39570295]\n",
      "epoch 829, losses: [ 0.20621814  0.27195297  0.2835558   0.21656283  0.97014358  0.10780496\n",
      "  1.24552443 14.35773555]\n",
      "epoch 830, losses: [ 0.20057946  0.27280779  0.2798486   0.21782866  0.96890503  0.09132633\n",
      "  1.02641679 14.33174731]\n",
      "epoch 831, losses: [ 0.19490708  0.28235589  0.279429    0.21833554  0.96736022  0.09045477\n",
      "  1.01843049 14.3110918 ]\n",
      "epoch 832, losses: [ 0.20627447  0.27349514  0.28337559  0.21790141  1.2017467   0.47340724\n",
      "  1.23863701 15.07065139]\n",
      "epoch 833, losses: [ 0.25929283  0.24346869  0.30206151  0.20992567  1.28033344  0.40671521\n",
      "  3.35345434 15.61678113]\n",
      "epoch 834, losses: [ 0.24085467  0.24182786  0.29457403  0.21253639  1.0724543   0.224572\n",
      "  1.93327425 14.61676933]\n",
      "epoch 835, losses: [ 0.24366783  0.24797769  0.29470874  0.21233686  0.9968582   0.15066797\n",
      "  1.48055455 14.46810354]\n",
      "epoch 836, losses: [ 0.23574538  0.25254711  0.28904405  0.21415022  0.97558277  0.10953555\n",
      "  1.30251022 14.3575352 ]\n",
      "epoch 837, losses: [ 0.22471631  0.26106807  0.28395385  0.2166948   0.97159355  0.09384364\n",
      "  1.06880694 14.32789951]\n",
      "epoch 838, losses: [ 0.2129002   0.27076566  0.28058383  0.21823291  0.9698481   0.1189723\n",
      "  1.46297739 14.30912548]\n",
      "epoch 839, losses: [ 0.20631709  0.27735565  0.27980604  0.21757814  0.96838254  0.09177212\n",
      "  1.0569177  14.29535948]\n",
      "epoch 840, losses: [ 0.21879943  0.26487961  0.28541455  0.21672715  1.12403164  0.71694528\n",
      "  3.7839422  14.73326182]\n",
      "epoch 841, losses: [ 0.24848307  0.24528712  0.30175918  0.21055085  1.16037206  0.81609403\n",
      "  1.26544926 15.93019248]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 842, losses: [ 0.23398056  0.2517536   0.29776353  0.21141306  1.02025002  0.29540675\n",
      "  1.44622655 14.77278807]\n",
      "epoch 843, losses: [ 0.23292289  0.25038301  0.29574471  0.21203478  0.98652705  0.1636023\n",
      "  1.10853962 14.59701756]\n",
      "epoch 844, losses: [ 0.22867463  0.25718396  0.28927005  0.21390493  0.98349509  0.11512557\n",
      "  1.09239687 14.44574548]\n",
      "epoch 845, losses: [ 0.2187835   0.26113142  0.28476168  0.21638026  0.97283378  0.09581134\n",
      "  1.05222542 14.38415457]\n",
      "epoch 846, losses: [ 0.20870438  0.27264004  0.28539301  0.21580098  0.97041543  0.09421356\n",
      "  1.0543973  14.35204265]\n",
      "epoch 847, losses: [ 0.20021068  0.27551223  0.28490254  0.2158964   0.9687952   0.09115529\n",
      "  1.02670308 14.32998275]\n",
      "epoch 848, losses: [ 0.19484174  0.28052457  0.28007753  0.21787992  0.9677515   0.09063122\n",
      "  1.02389329 14.31373199]\n",
      "epoch 849, losses: [ 0.18893383  0.28861673  0.2794343   0.21811251  0.96685785  0.08904104\n",
      "  1.01801909 14.30033411]\n",
      "epoch 850, losses: [ 0.18390097  0.28559569  0.27829332  0.21818216  0.96573502  0.08851522\n",
      "  1.01663072 14.28808682]\n",
      "epoch 851, losses: [ 0.18042287  0.28973892  0.276551    0.21947808  0.96497977  0.08774859\n",
      "  1.00634261 14.27820746]\n",
      "epoch 852, losses: [ 0.17543278  0.29687309  0.27586032  0.2196922   0.96435921  0.08719299\n",
      "  1.00179049 14.27003455]\n",
      "epoch 853, losses: [ 0.17178877  0.2978035   0.27391495  0.22010561  0.97163894  0.0880139\n",
      "  1.01656716 14.26195915]\n",
      "epoch 854, losses: [ 0.17589001  0.29529801  0.27391484  0.22164302  0.96408294  0.08606395\n",
      "  1.0021983  14.25580045]\n",
      "epoch 855, losses: [ 0.22579855  0.26295442  0.29611986  0.21419237  1.42003942  1.46469696\n",
      "  4.29960496 16.57332144]\n",
      "epoch 856, losses: [ 0.25191336  0.23450516  0.30924141  0.20750914  1.20390579  0.60785106\n",
      "  2.66613127 16.0702595 ]\n",
      "epoch 857, losses: [ 0.23743059  0.24539119  0.30205459  0.2097486   1.0899523   0.37005397\n",
      "  1.37767167 15.13061862]\n",
      "epoch 858, losses: [ 0.2326049   0.25215997  0.29647008  0.21172619  1.02680263  0.1929971\n",
      "  1.16877996 14.66013491]\n",
      "epoch 859, losses: [ 0.22922253  0.24936971  0.29072663  0.21456403  1.00405688  0.23625401\n",
      "  1.05304286 14.64182313]\n",
      "epoch 860, losses: [ 0.22159976  0.25797848  0.29505197  0.21220013  0.97693894  0.098133\n",
      "  1.02235253 14.45709734]\n",
      "epoch 861, losses: [ 0.21523366  0.26374674  0.28809041  0.21454243  0.97271746  0.09451833\n",
      "  1.02195575 14.3875195 ]\n",
      "epoch 862, losses: [ 0.20840535  0.27029427  0.2840884   0.21574122  0.97042509  0.092638\n",
      "  1.01573244 14.3529016 ]\n",
      "epoch 863, losses: [ 0.19981309  0.27627571  0.28161215  0.21794752  0.96895671  0.09186014\n",
      "  1.01685341 14.32873089]\n",
      "epoch 864, losses: [ 0.19357772  0.28074537  0.27932099  0.21817196  0.96753453  0.09121719\n",
      "  1.02486795 14.31039949]\n",
      "epoch 865, losses: [ 0.18566899  0.28880326  0.27648495  0.21929488  0.96608628  0.08862106\n",
      "  0.99870842 14.29572286]\n",
      "epoch 866, losses: [ 0.17738962  0.2901843   0.27535764  0.21966213  0.96507483  0.08861487\n",
      "  1.00093169 14.28303391]\n",
      "epoch 867, losses: [ 0.17589817  0.291458    0.27537483  0.22036057  0.96437675  0.08763383\n",
      "  0.98916199 14.27442468]\n",
      "epoch 868, losses: [ 0.16989436  0.2879519   0.27590091  0.21991111  0.96375208  0.08626353\n",
      "  0.97579782 14.26747306]\n",
      "epoch 869, losses: [ 0.16500379  0.30548077  0.27304086  0.22111832  0.96315164  0.08605947\n",
      "  0.98783452 14.26006209]\n",
      "epoch 870, losses: [ 0.16247588  0.30195737  0.27275529  0.22108852  0.96248659  0.08508006\n",
      "  0.97172899 14.25339829]\n",
      "epoch 871, losses: [ 0.16077443  0.30210998  0.27025517  0.22263602  0.9618725   0.08454353\n",
      "  0.96556573 14.24754221]\n",
      "epoch 872, losses: [ 0.15998668  0.30318335  0.27068574  0.22280377  0.96144792  0.08450834\n",
      "  0.96975839 14.24262742]\n",
      "epoch 873, losses: [ 0.15891446  0.30226709  0.27120101  0.22257555  0.96095237  0.08547445\n",
      "  0.9844726  14.23581998]\n",
      "epoch 874, losses: [ 0.16273971  0.2979116   0.26891781  0.22377861  0.96110286  0.08387859\n",
      "  0.96248365 14.23368292]\n",
      "epoch 875, losses: [ 0.20002408  0.27750576  0.28532617  0.21838373  1.29805597  1.26305872\n",
      "  4.2919226  15.6896115 ]\n",
      "epoch 876, losses: [ 0.21297072  0.26272082  0.31135364  0.20745444  1.34603407  0.77175933\n",
      "  1.67342955 16.64720327]\n",
      "epoch 877, losses: [ 0.23217566  0.24596088  0.3055195   0.21003621  1.05881741  0.26452066\n",
      "  1.15856983 14.8567188 ]\n",
      "epoch 878, losses: [ 0.22916495  0.25875124  0.30259206  0.20962272  1.02352577  0.33708255\n",
      "  1.15276398 14.76872432]\n",
      "epoch 879, losses: [ 0.22483017  0.25357511  0.30621967  0.20831103  0.98636307  0.14166782\n",
      "  1.05427307 14.60587068]\n",
      "epoch 880, losses: [ 0.21505646  0.26394403  0.30194598  0.20878751  0.97897408  0.1759802\n",
      "  1.01931046 14.50781492]\n",
      "epoch 881, losses: [ 0.20977574  0.26263501  0.29686229  0.21238542  0.97416722  0.11161519\n",
      "  1.00509912 14.43009032]\n",
      "epoch 882, losses: [ 0.20133435  0.27261073  0.29382143  0.21268945  0.98860362  0.32118343\n",
      "  0.99572126 14.67968629]\n",
      "epoch 883, losses: [ 0.18789809  0.27828444  0.29654171  0.21171477  0.9744128   0.12646342\n",
      "  0.99722163 14.4685974 ]\n",
      "epoch 884, losses: [ 0.18212119  0.28621373  0.29730967  0.21184424  0.98062898  0.12970256\n",
      "  0.97469109 14.49059843]\n",
      "epoch 885, losses: [ 0.19979427  0.27544858  0.297549    0.2120236   1.21502729  0.22977846\n",
      "  1.80622657 14.96359368]\n",
      "epoch 886, losses: [ 0.22938063  0.25222458  0.31138959  0.20691126  1.04175528  0.30740382\n",
      "  1.69707939 14.68466222]\n",
      "epoch 887, losses: [ 0.22465323  0.25832887  0.31123228  0.20723523  1.06486918  0.24177886\n",
      "  1.49343364 14.83553728]\n",
      "epoch 888, losses: [ 0.21408174  0.26128368  0.30399279  0.2090542   1.04117705  0.2557447\n",
      "  1.68925596 14.68256985]\n",
      "epoch 889, losses: [ 0.222667    0.26025805  0.29944052  0.21078585  1.03448429  0.70251932\n",
      "  1.45474317 14.87242862]\n",
      "epoch 890, losses: [ 0.22241068  0.25794975  0.3018407   0.2103263   0.98765596  0.19908219\n",
      "  1.31991679 14.58755238]\n",
      "epoch 891, losses: [ 0.20946443  0.26681644  0.29274571  0.21186052  0.97335684  0.09595863\n",
      "  1.02365213 14.38491238]\n",
      "epoch 892, losses: [ 0.19700299  0.27401637  0.28599168  0.21545534  0.96964354  0.09161537\n",
      "  0.99225254 14.33844295]\n",
      "epoch 893, losses: [ 0.18768838  0.28212182  0.28253545  0.2169265   0.9677575   0.09077228\n",
      "  1.00597471 14.3130372 ]\n",
      "epoch 894, losses: [ 0.18166446  0.28892294  0.28348314  0.2165429   0.9662693   0.0893416\n",
      "  0.98766418 14.29367053]\n",
      "epoch 895, losses: [ 0.17810787  0.28469566  0.2820224   0.21654591  0.96504191  0.08777259\n",
      "  0.97622562 14.27916502]\n",
      "epoch 896, losses: [ 0.17171101  0.29044457  0.27907008  0.21806114  0.96388738  0.08763072\n",
      "  0.98879602 14.26680246]\n",
      "epoch 897, losses: [ 0.16788361  0.29452669  0.27917997  0.21864371  0.9630186   0.08609004\n",
      "  0.97221927 14.2562268 ]\n",
      "epoch 898, losses: [ 0.1651811   0.29453252  0.27697928  0.21915001  0.96232623  0.08487246\n",
      "  0.95948437 14.24850791]\n",
      "epoch 899, losses: [ 0.16350733  0.29310997  0.2771473   0.21898853  0.96179042  0.08567342\n",
      "  0.96847072 14.2412848 ]\n",
      "epoch 900, losses: [ 0.16281169  0.29500017  0.27226068  0.22150382  0.9611625   0.08480343\n",
      "  0.96484323 14.23391595]\n",
      "epoch 901, losses: [ 0.16200825  0.29799117  0.2753771   0.22026607  0.96075366  0.08336203\n",
      "  0.94308145 14.22832264]\n",
      "epoch 902, losses: [ 0.1621043   0.29903957  0.27608097  0.22027759  0.96048015  0.08253598\n",
      "  0.93337888 14.22628774]\n",
      "epoch 903, losses: [ 0.16150245  0.29646037  0.27309929  0.22069824  0.96005501  0.08189561\n",
      "  0.92609998 14.21974415]\n",
      "epoch 904, losses: [ 0.15996459  0.30122883  0.27280636  0.2222064   0.95961329  0.08211771\n",
      "  0.92437177 14.21547132]\n",
      "epoch 905, losses: [ 0.15963076  0.30319249  0.27212778  0.2215845   0.95927754  0.08106199\n",
      "  0.91384711 14.21229406]\n",
      "epoch 906, losses: [ 0.16006795  0.2964685   0.27168886  0.2232599   0.95899235  0.08188754\n",
      "  0.92070715 14.20789201]\n",
      "epoch 907, losses: [ 0.19927905  0.28049976  0.29515442  0.21520492  1.6164514   0.80130096\n",
      "  2.76418725 16.88998978]\n",
      "epoch 908, losses: [ 0.23388863  0.24649513  0.30420508  0.20930529  1.22570522  0.65488339\n",
      "  2.58352474 15.46583943]\n",
      "epoch 909, losses: [ 0.21276003  0.25870764  0.2948731   0.21255417  1.03860298  0.44529414\n",
      "  1.45328093 14.85698625]\n",
      "epoch 910, losses: [ 0.21219896  0.25867019  0.29930488  0.21123265  1.00098315  0.15417225\n",
      "  1.09447954 14.55336555]\n",
      "epoch 911, losses: [ 0.20297702  0.26770618  0.29450143  0.2126258   0.97524066  0.09658604\n",
      "  1.03824713 14.39489452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 912, losses: [ 0.19513349  0.27159364  0.29061853  0.21395097  0.97067751  0.0934735\n",
      "  1.01634538 14.33761093]\n",
      "epoch 913, losses: [ 0.18930496  0.27740046  0.28535592  0.21608006  0.96749725  0.08986832\n",
      "  0.98630723 14.30254647]\n",
      "epoch 914, losses: [ 0.1850562   0.28137131  0.28342998  0.21707359  0.9652739   0.08792089\n",
      "  0.96889179 14.27822772]\n",
      "epoch 915, losses: [ 0.18068793  0.28163647  0.28336817  0.21772682  0.96363391  0.08648647\n",
      "  0.95949649 14.2600707 ]\n",
      "epoch 916, losses: [ 0.17785795  0.28459878  0.28217679  0.21821919  0.96257052  0.08529299\n",
      "  0.94827271 14.24731601]\n",
      "epoch 917, losses: [ 0.1749021   0.28902674  0.28095447  0.21785956  0.96177541  0.08322411\n",
      "  0.92410225 14.2382298 ]\n",
      "epoch 918, losses: [ 0.17379177  0.29126243  0.27759559  0.21970795  0.9610367   0.08300483\n",
      "  0.91802332 14.22954221]\n",
      "epoch 919, losses: [ 0.17277278  0.28721543  0.28036316  0.21908268  0.96056471  0.08439551\n",
      "  0.94206033 14.22214903]\n",
      "epoch 920, losses: [ 0.17191011  0.28805812  0.27727645  0.21976426  0.95999556  0.08406687\n",
      "  0.93889415 14.2161493 ]\n",
      "epoch 921, losses: [ 0.17180778  0.28424037  0.27770105  0.22016789  0.959644    0.08243676\n",
      "  0.91291991 14.21108925]\n",
      "epoch 922, losses: [ 0.23515079  0.25331326  0.30925273  0.21025498  1.38894812  1.21569363\n",
      "  3.69111915 17.30500591]\n",
      "epoch 923, losses: [ 0.24585998  0.24337844  0.31443344  0.20635728  1.13224036  0.73830594\n",
      "  1.94840113 15.60919425]\n",
      "epoch 924, losses: [ 0.23996005  0.24346725  0.3033676   0.2096974   1.02407998  0.20259015\n",
      "  1.16680155 14.84500891]\n",
      "epoch 925, losses: [ 0.22740596  0.25310208  0.29885283  0.21246302  0.99485858  0.11614052\n",
      "  1.01732833 14.5679621 ]\n",
      "epoch 926, losses: [ 0.22081286  0.25667854  0.29716528  0.21215363  0.97963805  0.09609639\n",
      "  0.9912522  14.45147578]\n",
      "epoch 927, losses: [ 0.2148409   0.25890212  0.29306614  0.21372143  0.96976195  0.09028518\n",
      "  0.95104101 14.33793917]\n",
      "epoch 928, losses: [ 0.21055432  0.26131351  0.28815686  0.21546729  0.96691579  0.08847223\n",
      "  0.94252616 14.30311143]\n",
      "epoch 929, losses: [ 0.20594239  0.26704873  0.28664457  0.21634163  0.96520895  0.08695214\n",
      "  0.93621585 14.27933145]\n",
      "epoch 930, losses: [ 0.19822775  0.26954549  0.28713253  0.21599944  0.96372083  0.08604616\n",
      "  0.93204121 14.26191036]\n",
      "epoch 931, losses: [ 0.1923167   0.27721116  0.28313788  0.2179638   0.96264034  0.08579324\n",
      "  0.93273942 14.24899323]\n",
      "epoch 932, losses: [ 0.19209513  0.27451586  0.28071505  0.2182092   0.96191635  0.0848531\n",
      "  0.91154903 14.24126438]\n",
      "epoch 933, losses: [ 0.18958671  0.27722652  0.28074796  0.21870929  0.96111204  0.08493154\n",
      "  0.93592828 14.2292922 ]\n",
      "epoch 934, losses: [ 0.18757668  0.27409225  0.28098356  0.21902846  0.96039489  0.08411313\n",
      "  0.92152831 14.22159694]\n",
      "epoch 935, losses: [ 0.18581656  0.28066808  0.27909641  0.21940337  0.95994285  0.08346487\n",
      "  0.92097488 14.2162945 ]\n",
      "epoch 936, losses: [ 0.1848223   0.27579948  0.27758841  0.22039178  0.95943168  0.0825928\n",
      "  0.90924617 14.2096714 ]\n",
      "epoch 937, losses: [ 0.18391603  0.27592155  0.27750059  0.22022371  0.95906875  0.08051637\n",
      "  0.88503714 14.20685793]\n",
      "epoch 938, losses: [ 0.184277    0.27771821  0.27758975  0.22053561  0.95873633  0.0814548\n",
      "  0.89706014 14.20174116]\n",
      "epoch 939, losses: [ 0.18430921  0.27929814  0.27734628  0.2206381   0.95842384  0.08125709\n",
      "  0.89319987 14.19906784]\n",
      "epoch 940, losses: [ 0.18424504  0.28120193  0.27692259  0.22103322  0.95820498  0.08188469\n",
      "  0.90037147 14.19733783]\n",
      "epoch 941, losses: [ 0.18408115  0.27873086  0.27959266  0.21979693  0.95802368  0.08082017\n",
      "  0.88528496 14.19399144]\n",
      "epoch 942, losses: [ 0.18472466  0.28084307  0.27313998  0.22296179  0.95768166  0.08003195\n",
      "  0.87015106 14.19105879]\n",
      "epoch 943, losses: [ 0.24730623  0.24826285  0.31118429  0.20996337  1.65839241  1.64665896\n",
      "  4.45644827 16.92049501]\n",
      "epoch 944, losses: [ 0.2608544   0.23431403  0.30563111  0.21041741  1.10226123  0.57464798\n",
      "  1.22214715 15.92648911]\n",
      "epoch 945, losses: [ 0.25135734  0.23831393  0.30491368  0.20999808  0.99125981  0.25362912\n",
      "  1.04955314 14.65735777]\n",
      "epoch 946, losses: [ 0.24665795  0.24058768  0.29967725  0.21131484  0.98671252  0.29213083\n",
      "  1.00609932 14.60793275]\n",
      "epoch 947, losses: [ 0.23691001  0.2480509   0.29634853  0.21268224  0.98001559  0.23006344\n",
      "  0.9675072  14.50087005]\n",
      "epoch 948, losses: [ 0.23001086  0.25589155  0.29362992  0.21383572  0.9702269   0.12089516\n",
      "  0.92511698 14.35483772]\n",
      "epoch 949, losses: [ 0.22343537  0.25586322  0.29133887  0.21492074  0.96747189  0.09902674\n",
      "  0.91567152 14.31667486]\n",
      "epoch 950, losses: [ 0.2158294   0.25833754  0.2897115   0.21562685  0.96438837  0.10194182\n",
      "  0.91972598 14.27660752]\n",
      "epoch 951, losses: [ 0.20645225  0.26246124  0.28556623  0.21607781  0.96257824  0.14115192\n",
      "  0.89686636 14.25223362]\n",
      "epoch 952, losses: [ 0.20191324  0.26840249  0.28383293  0.21767597  0.96240418  0.099608\n",
      "  0.89033982 14.25665086]\n",
      "epoch 953, losses: [ 0.19985356  0.27057073  0.28499835  0.21771175  0.96062439  0.08432714\n",
      "  0.89307407 14.22533696]\n",
      "epoch 954, losses: [ 0.20676353  0.26967228  0.28668608  0.21713222  0.96860052  0.23773348\n",
      "  0.88593409 14.36530027]\n",
      "epoch 955, losses: [ 0.24444625  0.2476296   0.31152731  0.20900331  1.81736291  0.84190884\n",
      "  4.05982205 17.16214292]\n",
      "epoch 956, losses: [ 0.24891444  0.2423996   0.30430133  0.21004954  1.12773747  0.32848337\n",
      "  1.70764919 14.72909618]\n",
      "epoch 957, losses: [ 0.23578147  0.24676935  0.3008222   0.21141654  1.04336025  0.2269358\n",
      "  1.21053419 14.60172799]\n",
      "epoch 958, losses: [ 0.24242285  0.24879769  0.29487373  0.21351363  1.00503537  0.12413825\n",
      "  1.45158012 14.46039831]\n",
      "epoch 959, losses: [ 0.24149715  0.24697217  0.2938203   0.21342137  0.97772996  0.09518955\n",
      "  0.99187081 14.34814757]\n",
      "epoch 960, losses: [ 0.23354685  0.24793478  0.293037    0.2146233   0.96828026  0.09041486\n",
      "  0.95289882 14.29507835]\n",
      "epoch 961, losses: [ 0.226589    0.25430827  0.28752226  0.21636319  0.9722857   0.09035907\n",
      "  0.9672637  14.2718602 ]\n",
      "epoch 962, losses: [ 0.22191548  0.25604322  0.28677449  0.21652175  0.96647931  0.08663171\n",
      "  0.92363744 14.25537255]\n",
      "epoch 963, losses: [ 0.21569496  0.25954536  0.28573656  0.2170783   0.96322753  0.08482955\n",
      "  0.89847656 14.24243309]\n",
      "epoch 964, losses: [ 0.21057173  0.26364047  0.2862424   0.21683698  0.96202487  0.08474101\n",
      "  0.90328977 14.23202318]\n",
      "epoch 965, losses: [ 0.20770845  0.26188223  0.28888061  0.21633405  0.9612771   0.08390002\n",
      "  0.89245518 14.22289187]\n",
      "epoch 966, losses: [ 0.2055398   0.26532838  0.2811315   0.21886511  0.96047324  0.08362321\n",
      "  0.8898631  14.21511893]\n",
      "epoch 967, losses: [ 0.20394214  0.27116536  0.28071551  0.21957671  0.95980555  0.08296591\n",
      "  0.8853028  14.20906889]\n",
      "epoch 968, losses: [ 0.2026803   0.26988726  0.2840413   0.21844797  0.95917037  0.08050162\n",
      "  0.85354348 14.20246687]\n",
      "epoch 969, losses: [ 0.20203838  0.26813711  0.28093434  0.21959864  0.95864737  0.08034354\n",
      "  0.8523613  14.19722694]\n",
      "epoch 970, losses: [ 0.20287717  0.26849129  0.2766504   0.22121128  0.95836453  0.08168532\n",
      "  0.86996459 14.19394125]\n",
      "epoch 971, losses: [ 0.20148714  0.26877105  0.27947219  0.2204084   0.95794174  0.08041161\n",
      "  0.85486179 14.18952879]\n",
      "epoch 972, losses: [ 0.2016296   0.26869167  0.27798529  0.22123869  0.9576476   0.0810187\n",
      "  0.86014613 14.18544961]\n",
      "epoch 973, losses: [ 0.20176676  0.26903069  0.28047065  0.22048752  0.95730745  0.07935388\n",
      "  0.84205409 14.1828811 ]\n",
      "epoch 974, losses: [ 0.20145783  0.26891048  0.27963885  0.22028358  0.95703198  0.07912788\n",
      "  0.83279023 14.18081151]\n",
      "epoch 975, losses: [ 0.20113505  0.26977957  0.27773157  0.22165656  0.95684169  0.08065011\n",
      "  0.84972843 14.17815384]\n",
      "epoch 976, losses: [ 0.20279333  0.27004573  0.2757008   0.2222388   0.95665952  0.08018977\n",
      "  0.8399828  14.17707068]\n",
      "epoch 977, losses: [ 0.20301557  0.26858552  0.27634337  0.22266396  0.95638356  0.07952843\n",
      "  0.82756732 14.17471978]\n",
      "epoch 978, losses: [ 0.20549828  0.26761451  0.27595262  0.22263972  0.95636326  0.07914261\n",
      "  0.82409031 14.17338985]\n",
      "epoch 979, losses: [ 0.25455621  0.24482856  0.30765794  0.21179639  1.48168448  1.09480535\n",
      "  4.95210529 16.41175767]\n",
      "epoch 980, losses: [ 0.25078904  0.24241062  0.30292736  0.21121839  1.13048897  0.6086738\n",
      "  2.29956777 15.10040731]\n",
      "epoch 981, losses: [ 0.25162308  0.24360577  0.30105745  0.21219339  1.04390809  0.31901337\n",
      "  1.87140402 14.94465799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 982, losses: [ 0.25895503  0.23917125  0.30162223  0.21151318  0.99945109  0.12980768\n",
      "  1.22178397 14.40199431]\n",
      "epoch 983, losses: [ 0.25576148  0.2408407   0.29872391  0.21257267  0.96971559  0.09156533\n",
      "  0.91954056 14.29537729]\n",
      "epoch 984, losses: [ 0.24810953  0.24356966  0.29600622  0.21378922  0.96612238  0.08842134\n",
      "  0.90244711 14.25987628]\n",
      "epoch 985, losses: [ 0.24406862  0.24720432  0.28977613  0.21608915  0.96383408  0.08511312\n",
      "  0.87752136 14.23773621]\n"
     ]
    }
   ],
   "source": [
    "model = CycleGAN(log_dir)\n",
    "    \n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    losses = model.train(train_loader)\n",
    "\n",
    "    print('epoch %d, losses: %s' % (epoch + 1, losses))\n",
    "    writer.add_scalar('loss_G_A', losses[0], epoch)\n",
    "    writer.add_scalar('loss_D_A', losses[1], epoch)\n",
    "    writer.add_scalar('loss_G_B', losses[2], epoch)\n",
    "    writer.add_scalar('loss_D_B', losses[3], epoch)\n",
    "    writer.add_scalar('loss_cycle_A', losses[4], epoch)\n",
    "    writer.add_scalar('loss_cycle_B', losses[5], epoch)\n",
    "    writer.add_scalar('loss_idt_A', losses[6], epoch)\n",
    "    writer.add_scalar('loss_idt_B', losses[7], epoch)\n",
    "    \n",
    "    if epoch % save_epoch_freq == 0:\n",
    "        model.save('epoch%d' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### test_dataset = UnalignedDataset(is_train=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128, 24])\n",
      "torch.Size([1, 4, 128, 24])\n",
      "['data/0903/testFOJ/FOY0402JOY3_6.npy']\n",
      "['data/0903/testFOA/FOY0111ANG3_4.npy']\n"
     ]
    }
   ],
   "source": [
    "batch = iter(test_loader).next()\n",
    "print(batch['A'].shape)\n",
    "print(batch['B'].shape)\n",
    "print(batch['path_A'])\n",
    "print(batch['path_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "#imshow(make_grid(batch['A'], nrow=2))\n",
    "#plt.axis('off')\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:73: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "/home/yasuda/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "model = CycleGAN()\n",
    "model.log_dir = 'SJnim/' \n",
    "model.load('epoch1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"/home/yasuda/03_MANA/CycleGAN-Q/data/0114/testFS\")\n",
    "num_files = len(filenames)\n",
    "filename = []\n",
    "output = []\n",
    "for i in filenames:\n",
    "    fileload = np.load(\"/home/yasuda/03_MANA/CycleGAN-Q/data/0114/testFS/\"+i)\n",
    "    img_torch = torch.from_numpy(fileload)\n",
    "    img_torch = img_torch.unsqueeze(0)\n",
    "    #img_torch = img_torch.unsqueeze(0)\n",
    "    filetorch = img_torch.type(torch.cuda.FloatTensor)\n",
    "    #print(filetorch.shape)\n",
    "    fake_A = model.netG_B(filetorch)\n",
    "    #fake_img = model.netG_B(filetorch)\n",
    "    out = fake_A.cpu().detach().numpy()\n",
    "    out = np.resize(out, [7, 128, 24])\n",
    "    #print(out.shape)\n",
    "    np.save(\"/home/yasuda/03_MANA/CycleGAN-Q/data/0114/convFZJ2S/\"+i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"/home/yasuda/03_MANA/CycleGAN-Q/data/0114/testFS\")\n",
    "num_files = len(filenames)\n",
    "filename = []\n",
    "output = []\n",
    "for i in filenames:\n",
    "    fileload = np.load(\"/home/yasuda/03_MANA/CycleGAN-Q/data/0114/testFS/\"+i)\n",
    "    img_torch = torch.from_numpy(fileload)\n",
    "    img_torch = img_torch.unsqueeze(0)\n",
    "    #img_torch = img_torch.unsqueeze(0)\n",
    "    filetorch = img_torch.type(torch.cuda.FloatTensor)\n",
    "    #print(filetorch.shape)\n",
    "    fake_A = model.netG_B(filetorch)\n",
    "    #fake_img = model.netG_B(filetorch)\n",
    "    out = fake_A.cpu().detach().numpy()\n",
    "    out = np.resize(out, [7, 128, 24])\n",
    "    #print(out.shape)\n",
    "    np.save(\"/home/yasuda/03_MANA/CycleGAN-Q/data/0114/convFZS2J/\"+i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/0514/output_fake_A_H/[].npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-47813f003b02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/0514/output_fake_A_H/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.1/envs/pytorch3/lib/python3.5/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/0514/output_fake_A_H/[].npy'"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir(\"/home/yasuda/03_MANA/CycleGAN-Q/data/1026/testMA\")\n",
    "num_files = len(filenames)\n",
    "filename = []\n",
    "output = []\n",
    "for i in filenames:\n",
    "    fileload = np.load(\"/home/yasuda/03_MANA/CycleGAN-Q/data/1026/testMA/\"+i)\n",
    "    img_torch = torch.from_numpy(fileload)\n",
    "    img_torch = img_torch.unsqueeze(0)\n",
    "    #img_torch = img_torch.unsqueeze(0)\n",
    "    filetorch = img_torch.type(torch.cuda.FloatTensor)\n",
    "    #print(filetorch.shape)\n",
    "    fake_A = model.netG_B(filetorch)\n",
    "    #fake_img = model.netG_B(filetorch)\n",
    "    out = fake_A.cpu().detach().numpy()\n",
    "    out = np.resize(out, [7, 128, 24])\n",
    "    #print(out.shape)\n",
    "    np.save(\"/home/yasuda/03_MANA/CycleGAN-Q/data/1026/convMZA2J/\"+i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 23.5, 127.5, -0.5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x2880 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(make_grid(batch['B'], nrow=2))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(make_grid(fake_A.data, nrow=2))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-45290ea7ef63>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-45290ea7ef63>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tensorboard --logdir \"logs\"\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
