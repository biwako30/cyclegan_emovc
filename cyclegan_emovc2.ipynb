{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": []
    }
   ],
   "source": [
    "# Respect & reference https://github.com/aidiary/pytorch-examples/blob/master/180306-cyclegan-horse2zebra.ipynb\n",
    "# Respect & reference https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": []
    }
   ],
   "source": [
    "# hyperparameters\n",
    "load_size = 286  # オリジナルの画像はこのサイズにリサイズ\n",
    "fine_size = 256  # 286x286の画像からランダムに256x256をcrop\n",
    "batch_size = 1\n",
    "num_epoch = 2000\n",
    "\n",
    "lr = 0.001  # initial learning rate for adam\n",
    "beta1 = 0.5  # momentum term of adam\n",
    "\n",
    "save_epoch_freq = 5\n",
    "log_dir = 'logs'\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print('cuda available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnalignedDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, is_train):\n",
    "        super(torch.utils.data.Dataset, self).__init__()\n",
    "\n",
    "        root_dir = os.path.join('data', '0209')\n",
    "        \n",
    "        if is_train:\n",
    "            dir_A = os.path.join(root_dir, 'trainMS')\n",
    "            dir_B = os.path.join(root_dir, 'trainMA')\n",
    "        else:\n",
    "            dir_A = os.path.join(root_dir, 'testMS')\n",
    "            dir_B = os.path.join(root_dir, 'testMA')\n",
    "\n",
    "        self.image_paths_A = self._make_dataset(dir_A)\n",
    "        self.image_paths_B = self._make_dataset(dir_B)\n",
    "\n",
    "        self.size_A = len(self.image_paths_A)\n",
    "        self.size_B = len(self.image_paths_B)\n",
    "\n",
    "        #!!!\n",
    "        self.transform = self._make_transform(is_train)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index_A = index % self.size_A\n",
    "        path_A = self.image_paths_A[index_A]\n",
    "        \n",
    "        # クラスBの画像はランダムに選択\n",
    "        index_B = random.randint(0, self.size_B - 1)\n",
    "        path_B = self.image_paths_B[index_B]\n",
    "\n",
    "        #img_A = Image.open(path_A).convert('RGB')\n",
    "        #img_B = Image.open(path_B).convert('RGB')\n",
    "        nimg_A = np.load(path_A)\n",
    "        nimg_B = np.load(path_B)\n",
    "        #nimg_A = np.resize(nimg_A, [64,24])\n",
    "        #nimg_B = np.resize(nimg_B, [64,24])\n",
    "        #nimg_A = nimg_A.T\n",
    "        #nimg_B = nimg_B.T\n",
    "        img_A = torch.from_numpy(nimg_A)\n",
    "        img_B = torch.from_numpy(nimg_B)\n",
    "        #img_A = img_A.unsqueeze(0)\n",
    "        #img_B = img_B.unsqueeze(0)\n",
    "        \n",
    "        # データ拡張\n",
    "        # label data no kotodeha ?\n",
    "        #A = self.transform(img_A)\n",
    "        #B = self.transform(img_B)\n",
    "        #A = A.type(torch.cuda.FloatTensor)\n",
    "        #B = B.type(torch.cuda.FloatTensor)\n",
    "        A = img_A.type(torch.cuda.FloatTensor)\n",
    "        B = img_B.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        \n",
    "        return {'A': A, 'B': B, 'path_A': path_A, 'path_B': path_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(self.size_A, self.size_B)\n",
    "\n",
    "    def _make_dataset(self, dir):\n",
    "        images = []\n",
    "        for fname in os.listdir(dir):\n",
    "            #if fname.endswith('.jpg'):\n",
    "            if fname.endswith('.npy'):\n",
    "                path = os.path.join(dir, fname)\n",
    "                images.append(path)\n",
    "        sorted(images)\n",
    "        return images\n",
    "\n",
    "    def _make_transform(self, is_train):\n",
    "        transforms_list = []\n",
    "        #!!!\n",
    "        #transforms_list.append(transforms.Resize((load_size, load_size), Image.BICUBIC))\n",
    "        #transforms_list.append(transforms.RandomCrop(fine_size))\n",
    "        #if is_train:\n",
    "            #transforms_list.append(transforms.RandomHorizontalFlip())\n",
    "        #transforms_list.append(transforms.ToTensor())\n",
    "        #transforms_list.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))  # [0, 1] => [-1, 1]\n",
    "        return transforms.Compose(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainJ = np.load(\"\")\n",
    "#file_list1 = sorted(glob.glob(\"\"))\n",
    "#x1_train = []\n",
    "#for i in tqdm(file_list1[:30]):\n",
    "#    x1_train.append(np.load(i))\n",
    "#x1_train = np.vstack(x1_train)\n",
    "#x1_train = np.log10(x1_train)/10.\n",
    "\n",
    "train_dataset = UnalignedDataset(is_train=True)\n",
    "#train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x1_train))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE TEST\n",
    "batch = iter(train_loader).next()\n",
    "#print(batch)\n",
    "print(batch['A'].shape)\n",
    "print(batch['B'].shape)\n",
    "print(batch['path_A'])\n",
    "print(batch['path_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの描画\n",
    "def imshow(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    npimg = 0.5 * (npimg + 1)  # [-1,1] => [0, 1]\n",
    "    # [c, h, w] => [h, w, c]\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = iter(train_loader).next()\n",
    "images_A = batch['A']  # horses\n",
    "images_B = batch['B']  # zebras\n",
    "\n",
    "plt.figure(figsize=(10, 20))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(make_grid(images_A, nrow=4))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(make_grid(images_B, nrow=4))\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        conv_block = []\n",
    "        conv_block += [nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim, dim*4, kernel_size=3),\n",
    "                       nn.InstanceNorm2d(dim*4),\n",
    "                       nn.GLU(1),\n",
    "                       #nn.Sigmoid(),\n",
    "                       nn.ReflectionPad2d(1),\n",
    "                       nn.Conv2d(dim*2, dim, kernel_size=3),\n",
    "                       nn.InstanceNorm2d(dim)]\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "\n",
    "            nn.Conv2d(7, 64, kernel_size=(7,4)),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "\n",
    "            nn.Conv2d(32, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.model2 = nn.Sequential(\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            ResNetBlock(128),\n",
    "            #ResNetBlock(256),\n",
    "            #ResNetBlock(256),\n",
    "            #ResNetBlock(256),\n",
    "        )\n",
    "        \n",
    "        self.model3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=(3,2), stride=2, padding=1, output_padding=(1,0)),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=(3,2), stride=2, padding=1, output_padding=(1,0)),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            \n",
    "            #nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            #nn.InstanceNorm2d(4),\n",
    "            #nn.GLU(1),\n",
    "\n",
    "            #nn.ConvTranspose2d(64, 2, kernel_size=7, stride=1, padding=0, output_padding=0),\n",
    "            #nn.InstanceNorm2d(8),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(8, 7, kernel_size=(7,5), stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # initialize weights\n",
    "        self.model1.apply(self._init_weights)\n",
    "        self.model2.apply(self._init_weights)\n",
    "        self.model3.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.model1(input.cuda())\n",
    "        y = self.model2(x)\n",
    "        return self.model3(y)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, kernel_size=(4,2), stride=1, padding=1),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(32, 128, kernel_size=(4,2), stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 256, kernel_size=(4,2), stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(128, 512, kernel_size=(4,2), stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.GLU(1),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(256, 1, kernel_size=(4,2), stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # initialize weights\n",
    "        self.model.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input.cuda())\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)\n",
    "\n",
    "gen = Generator()\n",
    "print_network(gen)\n",
    "\n",
    "disc = Discriminator()\n",
    "print_network(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        # プールを使わないときはそのまま返す\n",
    "        if self.pool_size == 0:\n",
    "            return Variable(images)\n",
    "        return_images = []\n",
    "        for image in images:\n",
    "            # バッチの次元を削除して3Dテンソルに\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size - 1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.real_label_var = None\n",
    "        self.fake_label_var = None\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def get_target_tensor(self, input, target_is_real):\n",
    "        target_tensor = None\n",
    "        if target_is_real:\n",
    "            # 高速化のため？\n",
    "            # varがNoneのままか形状が違うときに作り直す\n",
    "            create_label = ((self.real_label_var is None) or (self.real_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                real_tensor = torch.ones(input.size())\n",
    "                if cuda:\n",
    "                    real_tensor = real_tensor.cuda()\n",
    "                self.real_label_var = Variable(real_tensor, requires_grad=False)\n",
    "            target_tensor = self.real_label_var\n",
    "        else:\n",
    "            create_label = ((self.fake_label_var is None) or (self.fake_label_var.numel() != input.numel()))\n",
    "            if create_label:\n",
    "                fake_tensor = torch.zeros(input.size())\n",
    "                if cuda:\n",
    "                    fake_tensor = fake_tensor.cuda()\n",
    "                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n",
    "            target_tensor = self.fake_label_var\n",
    "        return target_tensor\n",
    "\n",
    "    def __call__(self, input, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
    "        return self.loss(input, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN(object):\n",
    "    \n",
    "    def __init__(self, log_dir='logs'):\n",
    "        self.netG_A = Generator()\n",
    "        self.netG_B = Generator()\n",
    "        self.netD_A = Discriminator()\n",
    "        self.netD_B = Discriminator()\n",
    "\n",
    "        if cuda:\n",
    "            self.netG_A.cuda()\n",
    "            self.netG_B.cuda()\n",
    "            self.netD_A.cuda()\n",
    "            self.netD_B.cuda()\n",
    "\n",
    "        self.fake_A_pool = ImagePool(50)\n",
    "        self.fake_B_pool = ImagePool(50)\n",
    "\n",
    "        # targetが本物か偽物かで代わるのでオリジナルのGANLossクラスを作成\n",
    "        self.criterionGAN = GANLoss()\n",
    "        self.criterionCycle = torch.nn.L1Loss()\n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "\n",
    "        # Generatorは2つのパラメータを同時に更新\n",
    "        self.optimizer_G = torch.optim.Adam(\n",
    "            itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n",
    "            lr=lr,\n",
    "            betas=(beta1, 0.999))\n",
    "        self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(), lr=lr*0.00001, betas=(beta1, 0.999))\n",
    "        self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(), lr=lr*0.00001, betas=(beta1, 0.999))\n",
    "        self.optimizers = []\n",
    "        self.optimizers.append(self.optimizer_G)\n",
    "        self.optimizers.append(self.optimizer_D_A)\n",
    "        self.optimizers.append(self.optimizer_D_B)\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "    \n",
    "    def set_input(self, input):\n",
    "        input_A = input['A']\n",
    "        input_B = input['B']\n",
    "        if cuda:\n",
    "            input_A = input_A.cuda()\n",
    "            input_B = input_B.cuda()\n",
    "        self.input_A = input_A\n",
    "        self.input_B = input_B\n",
    "        self.image_paths = input['path_A']\n",
    "\n",
    "    def backward_G(self, real_A, real_B):\n",
    "        # Generatorに関連するlossと勾配計算処理\n",
    "        lambda_idt = 15.0\n",
    "        lambda_A = 20.0\n",
    "        lambda_B = 20.0\n",
    "\n",
    "        # G_A, G_Bは変換先ドメインの本物画像を入力したときはそのまま出力するべき\n",
    "        # netG_AはドメインAの画像からドメインBの画像を生成するGeneratorだが\n",
    "        # ドメインBの画像も入れることができる\n",
    "        # その場合は何も変換してほしくないという制約\n",
    "        # TODO: idt_Aの命名はよくない気がする idt_Bの方が適切では？\n",
    "        idt_A = self.netG_A(real_B)\n",
    "        loss_idt_A = self.criterionIdt(idt_A, real_B) * lambda_B * lambda_idt\n",
    "\n",
    "        idt_B = self.netG_B(real_A)\n",
    "        loss_idt_B = self.criterionIdt(idt_B, real_A) * lambda_A * lambda_idt\n",
    "\n",
    "        # GAN loss D_A(G_A(A))\n",
    "        # G_Aとしては生成した偽物画像が本物（True）とみなしてほしい\n",
    "        fake_B = self.netG_A(real_A)\n",
    "        pred_fake = self.netD_A(fake_B)\n",
    "        loss_G_A = self.criterionGAN(pred_fake, True)\n",
    "\n",
    "        # GAN loss D_B(G_B(B))\n",
    "        # G_Bとしては生成した偽物画像が本物（True）とみなしてほしい\n",
    "        fake_A = self.netG_B(real_B)\n",
    "        pred_fake = self.netD_B(fake_A)\n",
    "        loss_G_B = self.criterionGAN(pred_fake, True)\n",
    "        \n",
    "        # forward cycle loss\n",
    "        # real_A => fake_B => rec_Aが元のreal_Aに近いほどよい\n",
    "        rec_A = self.netG_B(fake_B)\n",
    "        loss_cycle_A = self.criterionCycle(rec_A, real_A) * lambda_A\n",
    "        \n",
    "        # backward cycle loss\n",
    "        # real_B => fake_A => rec_Bが元のreal_Bに近いほどよい\n",
    "        rec_B = self.netG_A(fake_A)\n",
    "        loss_cycle_B = self.criterionCycle(rec_B, real_B) * lambda_B\n",
    "        \n",
    "        # combined loss\n",
    "        loss_G = loss_G_A + loss_G_B + loss_cycle_A + loss_cycle_B + loss_idt_A + loss_idt_B\n",
    "        loss_G.backward()\n",
    "\n",
    "        # 次のDiscriminatorの更新でfake画像が必要なので一緒に返す\n",
    "        return loss_G_A.data[0], loss_G_B.data[0], loss_cycle_A.data[0], loss_cycle_B.data[0], loss_idt_A.data[0], loss_idt_B.data[0], fake_A.data, fake_B.data\n",
    "\n",
    "    def backward_D_A(self, real_B, fake_B):\n",
    "        # ドメインAから生成したfake_Bが本物か偽物か見分ける\n",
    "\n",
    "        # TODO: これは何をしている？\n",
    "        # fake_Bを直接使わずに過去に生成した偽画像から新しくランダムサンプリングしている？\n",
    "        fake_B = self.fake_B_pool.query(fake_B)\n",
    "\n",
    "        # 本物画像を入れたときは本物と認識するほうがよい\n",
    "        pred_real = self.netD_A(real_B)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "        # ドメインAから生成した偽物画像を入れたときは偽物と認識するほうがよい\n",
    "        # fake_Bを生成したGeneratorまで勾配が伝搬しないようにdetach()する\n",
    "        pred_fake = self.netD_A(fake_B.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "\n",
    "        # combined loss\n",
    "        loss_D_A = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_A.backward()\n",
    "        \n",
    "        return loss_D_A.data[0]\n",
    "\n",
    "    def backward_D_B(self, real_A, fake_A):\n",
    "        # ドメインBから生成したfake_Aが本物か偽物か見分ける\n",
    "\n",
    "        fake_A = self.fake_A_pool.query(fake_A)\n",
    "        \n",
    "        # 本物画像を入れたときは本物と認識するほうがよい\n",
    "        pred_real = self.netD_B(real_A)\n",
    "        loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "        # 偽物画像を入れたときは偽物と認識するほうがよい\n",
    "        pred_fake = self.netD_B(fake_A.detach())\n",
    "        loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "        \n",
    "        # combined loss\n",
    "        loss_D_B = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_B.backward()\n",
    "        \n",
    "        return loss_D_B.data[0]\n",
    "\n",
    "    def optimize(self):\n",
    "        real_A = Variable(self.input_A)\n",
    "        real_B = Variable(self.input_B)\n",
    "        \n",
    "        # update Generator (G_A and G_B)\n",
    "        self.optimizer_G.zero_grad()\n",
    "        loss_G_A, loss_G_B, loss_cycle_A, loss_cycle_B, loss_idt_A, loss_idt_B, fake_A, fake_B = self.backward_G(real_A, real_B)\n",
    "        self.optimizer_G.step()\n",
    "\n",
    "        # update D_A\n",
    "        self.optimizer_D_A.zero_grad()\n",
    "        loss_D_A = self.backward_D_A(real_B, fake_B)\n",
    "        self.optimizer_D_A.step()\n",
    "        \n",
    "        # update D_B\n",
    "        self.optimizer_D_B.zero_grad()\n",
    "        loss_D_B = self.backward_D_B(real_A, fake_A)\n",
    "        self.optimizer_D_B.step()\n",
    "\n",
    "        ret_loss = [loss_G_A, loss_D_A,\n",
    "                    loss_G_B, loss_D_B,\n",
    "                    loss_cycle_A, loss_cycle_B,\n",
    "                    loss_idt_A, loss_idt_B]\n",
    "\n",
    "        return np.array(ret_loss)\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        running_loss = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        for batch_idx, data in enumerate(data_loader):\n",
    "            self.set_input(data)\n",
    "            losses = self.optimize()\n",
    "            running_loss += losses\n",
    "        running_loss /= len(data_loader)\n",
    "        return running_loss\n",
    "    \n",
    "    def save_network(self, network, network_label, epoch_label):\n",
    "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        save_path = os.path.join(self.log_dir, save_filename)\n",
    "        # GPUで動いている場合はCPUに移してから保存\n",
    "        # これやっておけばCPUでモデルをロードしやすくなる?\n",
    "        #print(\"Test\")\n",
    "        torch.save(network.cpu().state_dict(), save_path)\n",
    "        # GPUに戻す\n",
    "        if torch.cuda.is_available():\n",
    "            #print(\"check\")\n",
    "            network.cuda()\n",
    "\n",
    "    def load_network(self, network, network_label, epoch_label):\n",
    "        load_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
    "        load_path = os.path.join(self.log_dir, load_filename)\n",
    "        network.load_state_dict(torch.load(load_path))\n",
    "\n",
    "    def save(self, label):\n",
    "        self.save_network(self.netG_A, 'G_A', label)\n",
    "        self.save_network(self.netD_A, 'D_A', label)\n",
    "        self.save_network(self.netG_B, 'G_B', label)\n",
    "        self.save_network(self.netD_B, 'D_B', label)\n",
    "    \n",
    "    def load(self, label):\n",
    "        self.load_network(self.netG_A, 'G_A', label)\n",
    "        self.load_network(self.netD_A, 'D_A', label)\n",
    "        self.load_network(self.netG_B, 'G_B', label)\n",
    "        self.load_network(self.netD_B, 'D_B', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# netG_AもnetG_Bも同じサイズの画像を生成することが確認できる\n",
    "model = CycleGAN()\n",
    "data = iter(train_loader).next()\n",
    "\n",
    "real_A = Variable(data['A'], volatile=True)\n",
    "print('real_A:', real_A.size())\n",
    "fake_B = model.netG_A(real_A)\n",
    "print('fake_B:', fake_B.size())\n",
    "recon_A = model.netG_B(fake_B)\n",
    "print('recon_A:', recon_A.size())\n",
    "\n",
    "real_B = Variable(data['B'], volatile=True)\n",
    "print('real_B:', real_B.size())\n",
    "fake_A = model.netG_B(real_B)\n",
    "print('fake_A:', fake_A.size())\n",
    "recon_B = model.netG_A(fake_A)\n",
    "print('recon_B:', recon_B.size())\n",
    "\n",
    "# Discriminatorが0 or 1のスカラーを返すのではなく、\n",
    "# 30x30のfeature mapを返す\n",
    "# 本物画像のときは30x30がすべて1、偽物画像のときは30x30がすべて0がターゲットとなる\n",
    "out = model.netD_A(real_A)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterionGAN = GANLoss()\n",
    "loss = criterionGAN(out, True)\n",
    "print(criterionGAN.real_label_var)\n",
    "loss = criterionGAN(out, False)\n",
    "print(criterionGAN.fake_label_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CycleGAN(log_dir)\n",
    "    \n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    losses = model.train(train_loader)\n",
    "\n",
    "    print('epoch %d, losses: %s' % (epoch + 1, losses))\n",
    "    writer.add_scalar('loss_G_A', losses[0], epoch)\n",
    "    writer.add_scalar('loss_D_A', losses[1], epoch)\n",
    "    writer.add_scalar('loss_G_B', losses[2], epoch)\n",
    "    writer.add_scalar('loss_D_B', losses[3], epoch)\n",
    "    writer.add_scalar('loss_cycle_A', losses[4], epoch)\n",
    "    writer.add_scalar('loss_cycle_B', losses[5], epoch)\n",
    "    writer.add_scalar('loss_idt_A', losses[6], epoch)\n",
    "    writer.add_scalar('loss_idt_B', losses[7], epoch)\n",
    "    \n",
    "    if epoch % save_epoch_freq == 0:\n",
    "        model.save('epoch%d' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### test_dataset = UnalignedDataset(is_train=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = iter(test_loader).next()\n",
    "print(batch['A'].shape)\n",
    "print(batch['B'].shape)\n",
    "print(batch['path_A'])\n",
    "print(batch['path_B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imshow(make_grid(batch['A'], nrow=2))\n",
    "#plt.axis('off')\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CycleGAN()\n",
    "model.log_dir = 'SJnim/' \n",
    "model.load('epoch1995')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"\")\n",
    "num_files = len(filenames)\n",
    "filename = []\n",
    "output = []\n",
    "for i in filenames:\n",
    "    fileload = np.load(\"\"+i)\n",
    "    img_torch = torch.from_numpy(fileload)\n",
    "    img_torch = img_torch.unsqueeze(0)\n",
    "    #img_torch = img_torch.unsqueeze(0)\n",
    "    filetorch = img_torch.type(torch.cuda.FloatTensor)\n",
    "    #print(filetorch.shape)\n",
    "    fake_A = model.netG_B(filetorch)\n",
    "    #fake_img = model.netG_B(filetorch)\n",
    "    out = fake_A.cpu().detach().numpy()\n",
    "    out = np.resize(out, [7, 128, 24])\n",
    "    #print(out.shape)\n",
    "    np.save(\"\"+i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"\")\n",
    "num_files = len(filenames)\n",
    "filename = []\n",
    "output = []\n",
    "for i in filenames:\n",
    "    fileload = np.load(\"\"+i)\n",
    "    img_torch = torch.from_numpy(fileload)\n",
    "    img_torch = img_torch.unsqueeze(0)\n",
    "    #img_torch = img_torch.unsqueeze(0)\n",
    "    filetorch = img_torch.type(torch.cuda.FloatTensor)\n",
    "    #print(filetorch.shape)\n",
    "    fake_A = model.netG_B(filetorch)\n",
    "    #fake_img = model.netG_B(filetorch)\n",
    "    out = fake_A.cpu().detach().numpy()\n",
    "    out = np.resize(out, [7, 128, 24])\n",
    "    #print(out.shape)\n",
    "    np.save(\"\"+i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(\"\")\n",
    "num_files = len(filenames)\n",
    "filename = []\n",
    "output = []\n",
    "for i in filenames:\n",
    "    fileload = np.load(\"\"+i)\n",
    "    img_torch = torch.from_numpy(fileload)\n",
    "    img_torch = img_torch.unsqueeze(0)\n",
    "    #img_torch = img_torch.unsqueeze(0)\n",
    "    filetorch = img_torch.type(torch.cuda.FloatTensor)\n",
    "    #print(filetorch.shape)\n",
    "    fake_A = model.netG_B(filetorch)\n",
    "    #fake_img = model.netG_B(filetorch)\n",
    "    out = fake_A.cpu().detach().numpy()\n",
    "    out = np.resize(out, [7, 128, 24])\n",
    "    #print(out.shape)\n",
    "    np.save(\"\"+i, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 40))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(make_grid(batch['B'], nrow=2))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(make_grid(fake_A.data, nrow=2))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
